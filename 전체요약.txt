conv2D 변수들
filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).
kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.
strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.
padding: one of "valid" or "same" (case-insensitive). "valid" means no padding. "same" results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.
data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height, width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be channels_last.
dilation_rate: an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1.
groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups.
activation: Activation function to use. If you don't specify anything, no activation is applied (see keras.activations).
use_bias: Boolean, whether the layer uses a bias vector.
kernel_initializer: Initializer for the kernel weights matrix (see keras.initializers).
bias_initializer: Initializer for the bias vector (see keras.initializers).
kernel_regularizer: Regularizer function applied to the kernel weights matrix (see keras.regularizers).
bias_regularizer: Regularizer function applied to the bias vector (see keras.regularizers).
activity_regularizer: Regularizer function applied to the output of the layer (its "activation") (see keras.regularizers).
kernel_constraint: Constraint function applied to the kernel matrix (see keras.constraints).
bias_constraint: Constraint function applied to the bias vector (see keras.constraints).

imshow
plt.imshow(x_train[0],'gray')
plt.show()

과적합 줄이기
# 1. 훈련 데이터를 늘린다
# 2. 규제 피처를 줄인다.
# 3. 정규화
# 4. DropOut

	데이터 구조		input_shape
Dense	(행, 열)  <- 2차원		(열, )  <- 1차원
LSTM	(행, 열, 몇) <- 3차원	(열, 몇)  <- 2차원
CNN	(행, h, w, f)   <- 4차원	(h, w, f)  <- 3차원

<LSTM>
4x(1+1+10)x10
첫번쨰 1은 인풋딤 두번째일은 바이어스 3번쨰는 10개가 돌아오는거
(행,열,몇개식 짜르는지)
(배치사이즈,타임스탭,인풋딤)
lstm에 인풋쉐이프 에서는  input_shape = (time_step, input_dim) 이렇게 넣어주거나
좀 훈련속도를 늘리기 위해 배치까지 정하려면 batch_input_shape = (batct_size,time_stemp,input_dim)
※※ LSTM층을 2개 연속 붙이려면 위에층에 return_sequences=True 라고 해줘야함 ※※

LSTM의 액티베이션 디폴트? 하이퍼볼리기 탄젠트
LSTM 게이트 : output_gate,input_gate,forget_gate,memory_cell?
GRU 파라미터 : 3 * n * (n + m + 1(bias) +1(cell state))

<train_test_split>
from sklearn.model_selection import train_test_split
train_test_split(x,y,test_size=0.2,stratify = y)

<정규화>
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)
x_val = scaler.transform(x_val)
의미 잘 생각해야함

<원핫 인코딩>
from sklearn.preprocessing import OneHotEncoder
onehot = OneHotEncoder()
onehot.fit(y.reshape(-1,1))
y = onehot.transform(y.reshape(-1.1)).toarray()
from tensorflow.keras.utils import to_categorical
y = to_categorical(y)

<r2,mean_squared_error>
from sklearn.metrics import r2_score
y_predict = model.predict(x_test)
r2 = r2_score(y_predict,y_test)

<early_stopping>
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss',patience = 20, mode = 'auto')


             <이진 vs 다중>
y onehot        x        o
activation     sig      softmax   
loss         binary     categorical
metrics        acc      acc
predict     np.where / argmax






