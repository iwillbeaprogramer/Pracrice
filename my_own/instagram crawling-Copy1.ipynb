{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "stopWord = ['ㅋ','ㅎ','^','!','ㅡ','-','_','ㅠ','ㅜ',';','ㅗ','ㄷ','\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "#    cleaned_text = re.sub('[a-zA-Z]' , '', text)\n",
    "#    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "#                          '', cleaned_text)\n",
    "#    return cleaned_text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def crawling(keyword,number):\n",
    "    error_count=0\n",
    "    count=0\n",
    "    base_url ='https://www.instagram.com/graphql/query/?query_hash=9b498c08113f1e09617a1703c22b2f32'\n",
    "    \n",
    "    \"\"\"\n",
    "    인스타그램 해당 키워드로 접속 기본 게시물 크롤링\n",
    "    \"\"\"\n",
    "    resp = requests.get('https://www.instagram.com/explore/tags/'+keyword+'/?__a=1')\n",
    "    edge = resp.json()[\"graphql\"][\"hashtag\"][\"edge_hashtag_to_media\"][\"edges\"]\n",
    "    has_next_page = resp.json()[\"graphql\"][\"hashtag\"][\"edge_hashtag_to_media\"]['page_info']['has_next_page']\n",
    "    end_cursor = resp.json()[\"graphql\"][\"hashtag\"][\"edge_hashtag_to_media\"]['page_info']['end_cursor']\n",
    "    n=1\n",
    "    SAVE_DIR='./'+keyword+'/'\n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.mkdir(SAVE_DIR)\n",
    "    for i in edge:\n",
    "        count+=1\n",
    "        try:\n",
    "            text = i[\"node\"][\"edge_media_to_caption\"]['edges'][0]['node']['text']\n",
    "            code = i[\"node\"]['shortcode']\n",
    "            img = i['node']['display_url']\n",
    "            dic = {'hashtag' : []}\n",
    "            text_list = clean_text(i[\"node\"][\"edge_media_to_caption\"]['edges'][0]['node']['text']).split('#')\n",
    "            for text in text_list:\n",
    "                if len(text)<10:\n",
    "                    if text not in stopWord:\n",
    "                        dic['hashtag'].append(clean_text(text).strip().replace('\\n',''))\n",
    "            df = pd.DataFrame(dic)\n",
    "            df.to_csv(keyword+'/'+keyword+str(n)+'.csv')\n",
    "            with urlopen(img) as f:\n",
    "                with open(keyword+'/'+keyword+str(n)+'.jpg','wb') as c:\n",
    "                    temp = f.read()\n",
    "                    c.write(temp)\n",
    "            n+=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    인스타그램 해당 키워드로 접속 TOP게시물 크롤링\n",
    "    \"\"\"\n",
    "    edges = resp.json()[\"graphql\"][\"hashtag\"][\"edge_hashtag_to_top_posts\"][\"edges\"]\n",
    "    for i in edges:\n",
    "        count+=1\n",
    "        try:\n",
    "            text = i[\"node\"][\"edge_media_to_caption\"]['edges'][0]['node']['text']\n",
    "            code = i[\"node\"]['shortcode']\n",
    "            img = i['node']['display_url']\n",
    "            dic = {'hashtag' : []}\n",
    "            text_list = clean_text(i[\"node\"][\"edge_media_to_caption\"]['edges'][0]['node']['text']).split('#')\n",
    "            for text in text_list:\n",
    "                if len(text)<10:\n",
    "                    if text not in stopWord:\n",
    "                        dic['hashtag'].append(clean_text(text).strip().replace('\\n',''))\n",
    "            df = pd.DataFrame(dic)\n",
    "            df.to_csv(keyword+'/'+keyword+str(n)+'.csv')\n",
    "            with urlopen(img) as f:\n",
    "                with open(keyword+'/'+keyword+str(n)+'.jpg','wb') as c:\n",
    "                    temp = f.read()\n",
    "                    c.write(temp)\n",
    "            n+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    다음페이지 받아옴\n",
    "    \"\"\"\n",
    "    while has_next_page:\n",
    "        for k in range(1,20):\n",
    "            params = {\n",
    "            \"tag_name\":keyword,\n",
    "            \"first\":k,\n",
    "            \"after\": end_cursor\n",
    "        }\n",
    "            try:\n",
    "                new_resp = requests.get(base_url,params=params)\n",
    "                if new_resp.status_code!=200:\n",
    "                    break\n",
    "                edge = new_resp.json()[\"data\"][\"hashtag\"][\"edge_hashtag_to_media\"][\"edges\"]\n",
    "                has_next_page = new_resp.json()[\"data\"][\"hashtag\"][\"edge_hashtag_to_media\"]['page_info']['has_next_page']\n",
    "                end_cursor = new_resp.json()[\"data\"][\"hashtag\"][\"edge_hashtag_to_media\"]['page_info']['end_cursor']\n",
    "                for i in edge:\n",
    "                    dic = {'hashtag' : []}\n",
    "                    count+=1\n",
    "                    text_list = clean_text(i[\"node\"][\"edge_media_to_caption\"]['edges'][0]['node']['text']).split('#')\n",
    "                    for text in text_list:\n",
    "                        if len(text)<10:\n",
    "                            if text not in stopWord:\n",
    "                                dic['hashtag'].append(clean_text(text))\n",
    "\n",
    "                    df = pd.DataFrame(dic)\n",
    "                    df.to_csv(keyword+'/'+keyword+str(n)+'.csv')\n",
    "                    code = i[\"node\"]['shortcode']\n",
    "                    img = i['node']['display_url']\n",
    "                    with urlopen(img) as f:\n",
    "                        with open(keyword+'/'+keyword+str(n)+'.jpg','wb') as c:\n",
    "                            temp = f.read()\n",
    "                            c.write(temp)\n",
    "                    n+=1\n",
    "                    if count>=number:\n",
    "                        return\n",
    "            except:\n",
    "                error_count+=1\n",
    "                print(error_count)\n",
    "                if error_count>=10:\n",
    "                    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "있긴있네\n",
      "1\n",
      "있긴있네\n"
     ]
    }
   ],
   "source": [
    "crawling('김태희',300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['83198561398', '53753박현규', '윤영선', '마준영123', '이영재elnelf', 'lfjnf']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf\tasdf\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링_해올_리스트 = ['윤아','태연','티파니','선미','한효주']\n",
    "크롤링_해올_리스트 = ['스타벅스','해커스','엽떡','크리스마스']\n",
    "for i in 크롤링_해올_리스트:\n",
    "    crawling(i,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get('http://www.naver.com')\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \"\"\"\n",
    "    다음페이지 받아옴\n",
    "    \"\"\"\n",
    "    while has_next_page:\n",
    "        for k in range(1,20):\n",
    "            params = {\n",
    "            \"tag_name\":keyword,\n",
    "            \"first\":k,\n",
    "            \"after\": end_cursor\n",
    "        }\n",
    "            try:\n",
    "                new_resp = requests.get(base_url,params=params)\n",
    "                if new_resp.status_code!=200:\n",
    "                    break\n",
    "                edge = new_resp.json()[\"data\"][\"hashtag\"][\"edge_hashtag_to_media\"][\"edges\"]\n",
    "                has_next_page = new_resp.json()[\"data\"][\"hashtag\"][\"edge_hashtag_to_media\"]['page_info']['has_next_page']\n",
    "                end_cursor = new_resp.json()[\"data\"][\"hashtag\"][\"edge_hashtag_to_media\"]['page_info']['end_cursor']\n",
    "                for i in edge:\n",
    "                    dic = {'hashtag' : []}\n",
    "                    count+=1\n",
    "                    text_list = clean_text(i[\"node\"][\"edge_media_to_caption\"]['edges'][0]['node']['text']).split('#')\n",
    "                    for text in text_list:\n",
    "                        if len(text)<10:\n",
    "                            if text not in stopWord:\n",
    "                                dic['hashtag'].append(clean_text(text))\n",
    "\n",
    "                    df = pd.DataFrame(dic)\n",
    "                    df.to_csv(keyword+'/'+keyword+str(n)+'.csv')\n",
    "                    code = i[\"node\"]['shortcode']\n",
    "                    img = i['node']['display_url']\n",
    "                    with urlopen(img) as f:\n",
    "                        with open(keyword+'/'+keyword+str(n)+'.jpg','wb') as c:\n",
    "                            temp = f.read()\n",
    "                            c.write(temp)\n",
    "                    n+=1\n",
    "                    if count>=n:\n",
    "                        break\n",
    "            except:\n",
    "                error_count+=1\n",
    "                if error_count>=10:\n",
    "                    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
