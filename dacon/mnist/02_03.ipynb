{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ooCpvRmT-MkR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dropout,Input,Activation,Dense\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GysUTZTp-MkX"
   },
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s4kX312e-MkY"
   },
   "outputs": [],
   "source": [
    "def modeling():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = inputs\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = _x\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048)(x)\n",
    "    x = Dense(10,activation='softmax')(x)\n",
    "    outputs=x\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cccyJfWO-MkY"
   },
   "source": [
    "# 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W73nGvZM-MkZ"
   },
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "es = EarlyStopping(monitor='val_loss',patience=160)\n",
    "reLR = ReduceLROnPlateau(patience=100,verbose=1,factor=0.5)\n",
    "kfold = StratifiedKFold(n_splits=40,random_state=42,shuffle=True)\n",
    "\n",
    "datagen = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "datagen2 = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWqLv-B1-MkZ"
   },
   "source": [
    "# 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "j6UbQKZk-MkZ",
    "outputId": "26984b86-65c4-4adf-c0dc-bcb09f17d346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 28, 28, 1) (2048,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 9, 0, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "y = df.values[:,0].astype('int32')\n",
    "x = df.values[:,2:].astype('float32')/255.0\n",
    "# print(x.shape,y.shape)               # (2048, 28, 28) (2048,)\n",
    "#onehot = OneHotEncoder()\n",
    "#y = onehot.fit_transform(y.reshape(-1,1)).toarray().astype('float32')\n",
    "x = x.reshape(-1,28,28,1)\n",
    "# x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.15)\n",
    "# x_train = x_train.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# x_val = x_val.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# print(x_train.shape,x_val.shape,y_train.shape,y_val.shape)\n",
    "print(x.shape,y.shape) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmMESLTT-Mkb",
    "outputId": "6e97efa1-d705-4b3d-9e71-4261745da5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 8s - loss: 9.4119 - accuracy: 0.0469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0930s vs `on_train_batch_end` time: 0.1886s). Check your callbacks.\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 4.8865 - accuracy: 0.1002 - val_loss: 93.5020 - val_accuracy: 0.0577\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - 20s 320ms/step - loss: 2.2838 - accuracy: 0.1693 - val_loss: 3.1435 - val_accuracy: 0.1154\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 2.1699 - accuracy: 0.2265 - val_loss: 2.3993 - val_accuracy: 0.0962\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.9449 - accuracy: 0.2941 - val_loss: 2.5402 - val_accuracy: 0.0962\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - 31s 495ms/step - loss: 1.6919 - accuracy: 0.4093 - val_loss: 2.6559 - val_accuracy: 0.1346\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 1.3994 - accuracy: 0.5125 - val_loss: 3.2992 - val_accuracy: 0.0769\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2265 - accuracy: 0.5942 - val_loss: 4.8697 - val_accuracy: 0.0962\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.9867 - accuracy: 0.6733 - val_loss: 27.5650 - val_accuracy: 0.1154\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.8530 - accuracy: 0.7199 - val_loss: 3.8159 - val_accuracy: 0.0769\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - 33s 529ms/step - loss: 0.6573 - accuracy: 0.7861 - val_loss: 3.3849 - val_accuracy: 0.1923\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - 20s 323ms/step - loss: 0.6134 - accuracy: 0.7986 - val_loss: 2.3315 - val_accuracy: 0.4615\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - 20s 322ms/step - loss: 0.5911 - accuracy: 0.8021 - val_loss: 0.8251 - val_accuracy: 0.6731\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - 20s 323ms/step - loss: 0.4774 - accuracy: 0.8447 - val_loss: 0.6666 - val_accuracy: 0.7692\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.4323 - accuracy: 0.8512 - val_loss: 0.7846 - val_accuracy: 0.7308\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.4026 - accuracy: 0.8652 - val_loss: 0.6838 - val_accuracy: 0.7692\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.3137 - accuracy: 0.8938 - val_loss: 0.7438 - val_accuracy: 0.7500\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.3562 - accuracy: 0.8818 - val_loss: 0.7407 - val_accuracy: 0.7692\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - 38s 596ms/step - loss: 0.3011 - accuracy: 0.9013 - val_loss: 0.7128 - val_accuracy: 0.8269\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - 20s 319ms/step - loss: 0.3172 - accuracy: 0.8883 - val_loss: 0.5038 - val_accuracy: 0.8654\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.2412 - accuracy: 0.9168 - val_loss: 0.6343 - val_accuracy: 0.8269\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2332 - accuracy: 0.9183 - val_loss: 14.1669 - val_accuracy: 0.4423\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.2295 - accuracy: 0.9228 - val_loss: 0.6560 - val_accuracy: 0.8269\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1826 - accuracy: 0.9369 - val_loss: 5.0868 - val_accuracy: 0.5385\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1581 - accuracy: 0.9439 - val_loss: 0.7547 - val_accuracy: 0.7885\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.2239 - accuracy: 0.9294 - val_loss: 3.5102 - val_accuracy: 0.6346\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - 34s 541ms/step - loss: 0.1781 - accuracy: 0.9389 - val_loss: 0.4481 - val_accuracy: 0.8846\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.1316 - accuracy: 0.9579 - val_loss: 0.6656 - val_accuracy: 0.8269\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.1980 - accuracy: 0.9359 - val_loss: 0.9010 - val_accuracy: 0.8462\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1774 - accuracy: 0.9449 - val_loss: 0.9327 - val_accuracy: 0.8462\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1592 - accuracy: 0.9454 - val_loss: 0.4407 - val_accuracy: 0.8846\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.1478 - accuracy: 0.9499 - val_loss: 0.5280 - val_accuracy: 0.8462\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0856 - accuracy: 0.9709 - val_loss: 0.9116 - val_accuracy: 0.8462\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0803 - accuracy: 0.9709 - val_loss: 0.6426 - val_accuracy: 0.7692\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.2205 - accuracy: 0.9369 - val_loss: 0.9002 - val_accuracy: 0.7500\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.3982 - accuracy: 0.8858 - val_loss: 1.0882 - val_accuracy: 0.7500\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1863 - accuracy: 0.9394 - val_loss: 0.5841 - val_accuracy: 0.8654\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - 38s 598ms/step - loss: 0.1525 - accuracy: 0.9539 - val_loss: 0.3118 - val_accuracy: 0.9038\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.1038 - accuracy: 0.9669 - val_loss: 0.4085 - val_accuracy: 0.8654\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0943 - accuracy: 0.9709 - val_loss: 0.5424 - val_accuracy: 0.8269\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0837 - accuracy: 0.9709 - val_loss: 0.7468 - val_accuracy: 0.8462\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0671 - accuracy: 0.9749 - val_loss: 0.7994 - val_accuracy: 0.7885\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0475 - accuracy: 0.9840 - val_loss: 0.6328 - val_accuracy: 0.8462\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1092 - accuracy: 0.9649 - val_loss: 1.1790 - val_accuracy: 0.7885\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0641 - accuracy: 0.9744 - val_loss: 1.1021 - val_accuracy: 0.8077\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0507 - accuracy: 0.9845 - val_loss: 0.6221 - val_accuracy: 0.8462\n",
      "Epoch 46/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0414 - accuracy: 0.9835 - val_loss: 1.0485 - val_accuracy: 0.8654\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0696 - accuracy: 0.9785 - val_loss: 1.1345 - val_accuracy: 0.7885\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1552 - accuracy: 0.9574 - val_loss: 1.6279 - val_accuracy: 0.7500\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1605 - accuracy: 0.9489 - val_loss: 2.6362 - val_accuracy: 0.5962\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1805 - accuracy: 0.9449 - val_loss: 0.7049 - val_accuracy: 0.7885\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1823 - accuracy: 0.9434 - val_loss: 0.8366 - val_accuracy: 0.8269\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1433 - accuracy: 0.9509 - val_loss: 0.7663 - val_accuracy: 0.8462\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0859 - accuracy: 0.9654 - val_loss: 4.0212 - val_accuracy: 0.6346\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1593 - accuracy: 0.9554 - val_loss: 0.7341 - val_accuracy: 0.8269\n",
      "Epoch 55/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1551 - accuracy: 0.9534 - val_loss: 0.2822 - val_accuracy: 0.8846\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0938 - accuracy: 0.9714 - val_loss: 0.5266 - val_accuracy: 0.8462\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0563 - accuracy: 0.9795 - val_loss: 0.7563 - val_accuracy: 0.8462\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1007 - accuracy: 0.9649 - val_loss: 0.6100 - val_accuracy: 0.8462\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1001 - accuracy: 0.9709 - val_loss: 0.6362 - val_accuracy: 0.8269\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1177 - accuracy: 0.9649 - val_loss: 0.4922 - val_accuracy: 0.8654\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0608 - accuracy: 0.9815 - val_loss: 0.5312 - val_accuracy: 0.8269\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1906 - accuracy: 0.9459 - val_loss: 1.7676 - val_accuracy: 0.7115\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1079 - accuracy: 0.9649 - val_loss: 1.4001 - val_accuracy: 0.8077\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0459 - accuracy: 0.9865 - val_loss: 2.2239 - val_accuracy: 0.6731\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0473 - accuracy: 0.9855 - val_loss: 0.7979 - val_accuracy: 0.8462\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0591 - accuracy: 0.9815 - val_loss: 0.2722 - val_accuracy: 0.8654\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0463 - accuracy: 0.9865 - val_loss: 1.2208 - val_accuracy: 0.8077\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0355 - accuracy: 0.9905 - val_loss: 0.9884 - val_accuracy: 0.8654\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0186 - accuracy: 0.9925 - val_loss: 0.8951 - val_accuracy: 0.8462\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1257 - accuracy: 0.9694 - val_loss: 0.5325 - val_accuracy: 0.8654\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0961 - accuracy: 0.9694 - val_loss: 1.0128 - val_accuracy: 0.7885\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1298 - accuracy: 0.9614 - val_loss: 0.7018 - val_accuracy: 0.8846\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1261 - accuracy: 0.9629 - val_loss: 0.8670 - val_accuracy: 0.8269\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1208 - accuracy: 0.9609 - val_loss: 0.7259 - val_accuracy: 0.8462\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0969 - accuracy: 0.9709 - val_loss: 2.4352 - val_accuracy: 0.7115\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1632 - accuracy: 0.9519 - val_loss: 0.6761 - val_accuracy: 0.8846\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1782 - accuracy: 0.9534 - val_loss: 1.3382 - val_accuracy: 0.8462\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1477 - accuracy: 0.9629 - val_loss: 0.7435 - val_accuracy: 0.8846\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0904 - accuracy: 0.9729 - val_loss: 1.1267 - val_accuracy: 0.8077\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0495 - accuracy: 0.9825 - val_loss: 0.9260 - val_accuracy: 0.8462\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0429 - accuracy: 0.9850 - val_loss: 0.9142 - val_accuracy: 0.8462\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0552 - accuracy: 0.9800 - val_loss: 1.1169 - val_accuracy: 0.7885\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0666 - accuracy: 0.9795 - val_loss: 1.7405 - val_accuracy: 0.7885\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0834 - accuracy: 0.9734 - val_loss: 1.0480 - val_accuracy: 0.8654\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0451 - accuracy: 0.9850 - val_loss: 0.6074 - val_accuracy: 0.8846\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0481 - accuracy: 0.9850 - val_loss: 1.1133 - val_accuracy: 0.8269\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0686 - accuracy: 0.9785 - val_loss: 1.1989 - val_accuracy: 0.8654\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1442 - accuracy: 0.9699 - val_loss: 0.8547 - val_accuracy: 0.8269\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1174 - accuracy: 0.9659 - val_loss: 0.8822 - val_accuracy: 0.8269\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0491 - accuracy: 0.9855 - val_loss: 0.5376 - val_accuracy: 0.8462\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0280 - accuracy: 0.9900 - val_loss: 0.8637 - val_accuracy: 0.8462\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 0.8577 - val_accuracy: 0.8462\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0283 - accuracy: 0.9905 - val_loss: 1.2503 - val_accuracy: 0.8654\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0949 - accuracy: 0.9744 - val_loss: 1.1622 - val_accuracy: 0.8269\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1032 - accuracy: 0.9684 - val_loss: 0.7500 - val_accuracy: 0.8462\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0868 - accuracy: 0.9755 - val_loss: 2.0300 - val_accuracy: 0.8269\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1006 - accuracy: 0.9765 - val_loss: 1.0120 - val_accuracy: 0.8462\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0272 - accuracy: 0.9910 - val_loss: 1.3040 - val_accuracy: 0.7885\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 1.1995 - val_accuracy: 0.8077\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0167 - accuracy: 0.9960 - val_loss: 0.6563 - val_accuracy: 0.8654\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 0.6819 - val_accuracy: 0.8846\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 1.2203 - val_accuracy: 0.7885\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0647 - accuracy: 0.9830 - val_loss: 1.0009 - val_accuracy: 0.8654\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0989 - accuracy: 0.9739 - val_loss: 0.9040 - val_accuracy: 0.8462\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0934 - accuracy: 0.9770 - val_loss: 0.9561 - val_accuracy: 0.8654\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.2017 - accuracy: 0.9514 - val_loss: 6.7829 - val_accuracy: 0.4808\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1214 - accuracy: 0.9689 - val_loss: 2.5411 - val_accuracy: 0.7308\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1022 - accuracy: 0.9699 - val_loss: 0.9628 - val_accuracy: 0.8462\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0834 - accuracy: 0.9749 - val_loss: 1.5050 - val_accuracy: 0.8077\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0716 - accuracy: 0.9795 - val_loss: 1.0672 - val_accuracy: 0.8654\n",
      "Epoch 111/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0912 - accuracy: 0.9684 - val_loss: 1.7656 - val_accuracy: 0.7692\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0599 - accuracy: 0.9815 - val_loss: 1.6213 - val_accuracy: 0.8077\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0365 - accuracy: 0.9870 - val_loss: 0.7358 - val_accuracy: 0.9038\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0617 - accuracy: 0.9845 - val_loss: 1.8194 - val_accuracy: 0.7500\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0578 - accuracy: 0.9825 - val_loss: 0.8397 - val_accuracy: 0.8462\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0414 - accuracy: 0.9885 - val_loss: 0.9759 - val_accuracy: 0.8654\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0604 - accuracy: 0.9820 - val_loss: 0.8468 - val_accuracy: 0.8462\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0606 - accuracy: 0.9805 - val_loss: 1.0514 - val_accuracy: 0.8462\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0309 - accuracy: 0.9890 - val_loss: 0.3314 - val_accuracy: 0.8846\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0247 - accuracy: 0.9930 - val_loss: 0.3767 - val_accuracy: 0.8846\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0081 - accuracy: 0.9960 - val_loss: 0.5830 - val_accuracy: 0.9038\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0080 - accuracy: 0.9965 - val_loss: 0.3497 - val_accuracy: 0.8846\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.9251 - val_accuracy: 0.8077\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.9770 - val_accuracy: 0.9038\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0613 - accuracy: 0.9840 - val_loss: 0.7745 - val_accuracy: 0.8846\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0868 - accuracy: 0.9820 - val_loss: 0.8854 - val_accuracy: 0.9038\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1488 - accuracy: 0.9664 - val_loss: 1.2281 - val_accuracy: 0.8654\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2082 - accuracy: 0.9529 - val_loss: 2.7418 - val_accuracy: 0.7692\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0864 - accuracy: 0.9755 - val_loss: 1.9324 - val_accuracy: 0.8077\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0817 - accuracy: 0.9785 - val_loss: 1.4497 - val_accuracy: 0.8077\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0380 - accuracy: 0.9900 - val_loss: 0.8347 - val_accuracy: 0.8269\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1077 - accuracy: 0.9800 - val_loss: 1.5712 - val_accuracy: 0.7692\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1367 - accuracy: 0.9699 - val_loss: 1.3891 - val_accuracy: 0.7885\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0696 - accuracy: 0.9835 - val_loss: 0.7793 - val_accuracy: 0.8654\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0153 - accuracy: 0.9930 - val_loss: 0.6767 - val_accuracy: 0.8846\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.8852 - val_accuracy: 0.8654\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.9038\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.7055 - val_accuracy: 0.9038\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0051 - accuracy: 0.9970 - val_loss: 0.8331 - val_accuracy: 0.9038\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.7096 - val_accuracy: 0.9038\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.5508 - val_accuracy: 0.8846\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.4685e-04 - accuracy: 1.0000 - val_loss: 0.7214 - val_accuracy: 0.9038\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.1679e-04 - accuracy: 0.9995 - val_loss: 0.8143 - val_accuracy: 0.9038\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.5464 - val_accuracy: 0.8846\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.6687 - val_accuracy: 0.8846\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.6065 - val_accuracy: 0.8846\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.4862e-04 - accuracy: 1.0000 - val_loss: 0.6435 - val_accuracy: 0.8654\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.4853 - val_accuracy: 0.8846\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0054 - accuracy: 0.9980 - val_loss: 1.4795 - val_accuracy: 0.8462\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1275 - accuracy: 0.9684 - val_loss: 1.0537 - val_accuracy: 0.8269\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2476 - accuracy: 0.9539 - val_loss: 3.0264 - val_accuracy: 0.7692\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1179 - accuracy: 0.9704 - val_loss: 1.4709 - val_accuracy: 0.8077\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1722 - accuracy: 0.9644 - val_loss: 3.3943 - val_accuracy: 0.7500\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0868 - accuracy: 0.9755 - val_loss: 1.6796 - val_accuracy: 0.8269\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0688 - accuracy: 0.9865 - val_loss: 1.1826 - val_accuracy: 0.8077\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0363 - accuracy: 0.9915 - val_loss: 0.5352 - val_accuracy: 0.8846\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0203 - accuracy: 0.9920 - val_loss: 0.5749 - val_accuracy: 0.8462\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0331 - accuracy: 0.9910 - val_loss: 0.7518 - val_accuracy: 0.9038\n",
      "Epoch 159/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0131 - accuracy: 0.9950 - val_loss: 0.5484 - val_accuracy: 0.9038\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - 33s 521ms/step - loss: 0.0338 - accuracy: 0.9920 - val_loss: 0.5785 - val_accuracy: 0.9231\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.0300 - accuracy: 0.9930 - val_loss: 0.7313 - val_accuracy: 0.8846\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0252 - accuracy: 0.9940 - val_loss: 1.2245 - val_accuracy: 0.8269\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0224 - accuracy: 0.9920 - val_loss: 0.7171 - val_accuracy: 0.8654\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.9985 - val_accuracy: 0.8269\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.6733 - val_accuracy: 0.8846\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9815\n",
      "Epoch 00166: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0714 - accuracy: 0.9815 - val_loss: 0.4276 - val_accuracy: 0.9038\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - 33s 522ms/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.3016 - val_accuracy: 0.9615\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 0.5742 - val_accuracy: 0.9038\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0023 - accuracy: 0.9990 - val_loss: 0.4657 - val_accuracy: 0.9231\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0029 - accuracy: 0.9985 - val_loss: 0.6223 - val_accuracy: 0.8846\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.3347 - val_accuracy: 0.9423\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 8.8143e-04 - accuracy: 1.0000 - val_loss: 0.4856 - val_accuracy: 0.9423\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.8477e-04 - accuracy: 1.0000 - val_loss: 0.3221 - val_accuracy: 0.9423\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.3182e-04 - accuracy: 1.0000 - val_loss: 0.5306 - val_accuracy: 0.9231\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.8393e-04 - accuracy: 1.0000 - val_loss: 0.3757 - val_accuracy: 0.9231\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 8.4502e-05 - accuracy: 1.0000 - val_loss: 0.2685 - val_accuracy: 0.9615\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.9573e-04 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.9423\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.9004e-04 - accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9423\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 6.3368e-05 - accuracy: 1.0000 - val_loss: 0.3005 - val_accuracy: 0.9615\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.0526e-04 - accuracy: 1.0000 - val_loss: 0.5291 - val_accuracy: 0.9231\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 4.0899e-05 - accuracy: 1.0000 - val_loss: 0.4600 - val_accuracy: 0.9615\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 6.5238e-05 - accuracy: 1.0000 - val_loss: 0.5968 - val_accuracy: 0.9231\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 5.3005e-05 - accuracy: 1.0000 - val_loss: 0.5833 - val_accuracy: 0.9231\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 7.6620e-05 - accuracy: 1.0000 - val_loss: 0.6518 - val_accuracy: 0.8846\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 5.6731e-05 - accuracy: 1.0000 - val_loss: 0.3279 - val_accuracy: 0.9038\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.5487e-04 - accuracy: 1.0000 - val_loss: 0.6431 - val_accuracy: 0.9038\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.4539e-04 - accuracy: 1.0000 - val_loss: 0.5705 - val_accuracy: 0.8654\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.0805e-04 - accuracy: 1.0000 - val_loss: 0.2405 - val_accuracy: 0.9615\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.9034e-05 - accuracy: 1.0000 - val_loss: 0.4359 - val_accuracy: 0.9423\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.3003e-05 - accuracy: 1.0000 - val_loss: 0.5664 - val_accuracy: 0.9038\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.7459e-05 - accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 0.9423\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 4.6899e-05 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 0.9038\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2127e-04 - accuracy: 1.0000 - val_loss: 0.3366 - val_accuracy: 0.9615\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.3974e-05 - accuracy: 1.0000 - val_loss: 0.4250 - val_accuracy: 0.9038\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.3437e-05 - accuracy: 1.0000 - val_loss: 0.2728 - val_accuracy: 0.9615\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.0257e-05 - accuracy: 1.0000 - val_loss: 0.4141 - val_accuracy: 0.9231\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.7434e-05 - accuracy: 1.0000 - val_loss: 0.4916 - val_accuracy: 0.9231\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 3.6657e-05 - accuracy: 1.0000 - val_loss: 0.3729 - val_accuracy: 0.9423\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.6323e-05 - accuracy: 1.0000 - val_loss: 0.3809 - val_accuracy: 0.9231\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.8525e-05 - accuracy: 1.0000 - val_loss: 0.4684 - val_accuracy: 0.9231\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.3032e-05 - accuracy: 1.0000 - val_loss: 0.4765 - val_accuracy: 0.8846\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.7899e-05 - accuracy: 1.0000 - val_loss: 0.3801 - val_accuracy: 0.9423\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.7131e-05 - accuracy: 1.0000 - val_loss: 0.4184 - val_accuracy: 0.9231\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.5473e-05 - accuracy: 1.0000 - val_loss: 0.5938 - val_accuracy: 0.8846\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.7638e-05 - accuracy: 1.0000 - val_loss: 0.6711 - val_accuracy: 0.9231\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.8196e-05 - accuracy: 1.0000 - val_loss: 0.5126 - val_accuracy: 0.9231\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.1299e-05 - accuracy: 1.0000 - val_loss: 0.3190 - val_accuracy: 0.9423\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9356e-05 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9615\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.6420e-05 - accuracy: 1.0000 - val_loss: 0.4111 - val_accuracy: 0.9231\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.4825e-05 - accuracy: 1.0000 - val_loss: 0.7261 - val_accuracy: 0.9038\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1742e-05 - accuracy: 1.0000 - val_loss: 0.2844 - val_accuracy: 0.9231\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.9508e-05 - accuracy: 1.0000 - val_loss: 0.4503 - val_accuracy: 0.9231\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.3370e-05 - accuracy: 1.0000 - val_loss: 0.5315 - val_accuracy: 0.8846\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.0088e-05 - accuracy: 1.0000 - val_loss: 0.4819 - val_accuracy: 0.9231\n",
      "Epoch 215/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.1764e-05 - accuracy: 1.0000 - val_loss: 0.5007 - val_accuracy: 0.9038\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2371e-05 - accuracy: 1.0000 - val_loss: 0.6928 - val_accuracy: 0.9038\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.5590e-05 - accuracy: 1.0000 - val_loss: 0.4858 - val_accuracy: 0.9038\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.4059e-05 - accuracy: 1.0000 - val_loss: 0.4098 - val_accuracy: 0.8846\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.5188e-05 - accuracy: 1.0000 - val_loss: 0.6166 - val_accuracy: 0.8846\n",
      "Epoch 220/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2586e-05 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 0.9038\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.5647e-05 - accuracy: 1.0000 - val_loss: 0.3145 - val_accuracy: 0.9038\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1685e-05 - accuracy: 1.0000 - val_loss: 0.4579 - val_accuracy: 0.9231\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.0805e-05 - accuracy: 1.0000 - val_loss: 0.5823 - val_accuracy: 0.9038\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.4283e-06 - accuracy: 1.0000 - val_loss: 0.5733 - val_accuracy: 0.9038\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.9761e-05 - accuracy: 1.0000 - val_loss: 0.3831 - val_accuracy: 0.9231\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.3093e-05 - accuracy: 1.0000 - val_loss: 0.5669 - val_accuracy: 0.9038\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.8091e-05 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.9231\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1484e-05 - accuracy: 1.0000 - val_loss: 0.4774 - val_accuracy: 0.9231\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.7678e-06 - accuracy: 1.0000 - val_loss: 0.3602 - val_accuracy: 0.9423\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.0338e-06 - accuracy: 1.0000 - val_loss: 0.5849 - val_accuracy: 0.8846\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.7942e-06 - accuracy: 1.0000 - val_loss: 0.5843 - val_accuracy: 0.8846\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 6.5732e-06 - accuracy: 1.0000 - val_loss: 0.5649 - val_accuracy: 0.9038\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 7.3881e-06 - accuracy: 1.0000 - val_loss: 0.4238 - val_accuracy: 0.9231\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.2431e-06 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 0.9038\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 6.9293e-06 - accuracy: 1.0000 - val_loss: 0.3265 - val_accuracy: 0.9423\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.5933e-06 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 0.9423\n",
      "Epoch 237/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.1831e-06 - accuracy: 1.0000 - val_loss: 0.6959 - val_accuracy: 0.9231\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.0218e-06 - accuracy: 1.0000 - val_loss: 0.4054 - val_accuracy: 0.9423\n",
      "Epoch 239/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 7.1508e-06 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9615\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1307e-05 - accuracy: 1.0000 - val_loss: 0.3079 - val_accuracy: 0.9423\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9379e-05 - accuracy: 1.0000 - val_loss: 0.4220 - val_accuracy: 0.9423\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.4456e-05 - accuracy: 1.0000 - val_loss: 0.6730 - val_accuracy: 0.9038\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2759e-05 - accuracy: 1.0000 - val_loss: 0.5954 - val_accuracy: 0.8846\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.6759e-06 - accuracy: 1.00 - 18s 283ms/step - loss: 6.6759e-06 - accuracy: 1.0000 - val_loss: 0.7244 - val_accuracy: 0.9038\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 7.8467e-06 - accuracy: 1.0000 - val_loss: 0.4906 - val_accuracy: 0.8846\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 5.0110e-06 - accuracy: 1.0000 - val_loss: 0.5268 - val_accuracy: 0.8654\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.7830e-06 - accuracy: 1.0000 - val_loss: 0.7146 - val_accuracy: 0.9038\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 7.0698e-06 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.9231\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2027e-05 - accuracy: 1.0000 - val_loss: 0.3100 - val_accuracy: 0.9423\n",
      "Epoch 250/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.7334e-06 - accuracy: 1.0000 - val_loss: 0.6302 - val_accuracy: 0.9038\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 5.0930e-06 - accuracy: 1.0000 - val_loss: 0.2778 - val_accuracy: 0.9423\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.6175e-05 - accuracy: 1.0000 - val_loss: 0.4105 - val_accuracy: 0.9038\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.6661e-05 - accuracy: 1.0000 - val_loss: 0.7558 - val_accuracy: 0.8846\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.1521e-05 - accuracy: 1.0000 - val_loss: 0.6543 - val_accuracy: 0.9231\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.9146e-06 - accuracy: 1.0000 - val_loss: 0.5590 - val_accuracy: 0.9231\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.7513e-06 - accuracy: 1.0000 - val_loss: 0.8704 - val_accuracy: 0.8846\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.0968e-06 - accuracy: 1.0000 - val_loss: 0.5690 - val_accuracy: 0.8846\n",
      "Epoch 258/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.4333e-06 - accuracy: 1.0000 - val_loss: 0.9334 - val_accuracy: 0.8846\n",
      "Epoch 259/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.9805e-06 - accuracy: 1.0000 - val_loss: 0.5656 - val_accuracy: 0.9038\n",
      "Epoch 260/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.6216e-06 - accuracy: 1.0000 - val_loss: 0.7839 - val_accuracy: 0.8846\n",
      "Epoch 261/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.8333e-06 - accuracy: 1.0000 - val_loss: 0.3844 - val_accuracy: 0.9231\n",
      "Epoch 262/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.6943e-06 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 0.9423\n",
      "Epoch 263/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 6.8112e-06 - accuracy: 1.0000 - val_loss: 0.4860 - val_accuracy: 0.9231\n",
      "Epoch 264/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.9418e-06 - accuracy: 1.0000 - val_loss: 0.4861 - val_accuracy: 0.9038\n",
      "Epoch 265/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.6360e-06 - accuracy: 1.0000 - val_loss: 0.3876 - val_accuracy: 0.9423\n",
      "Epoch 266/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.1141e-06 - accuracy: 1.0000 - val_loss: 0.4124 - val_accuracy: 0.9423\n",
      "Epoch 267/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.6777e-06 - accuracy: 1.0000 - val_loss: 0.5542 - val_accuracy: 0.9231\n",
      "Epoch 268/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.4697e-06 - accuracy: 1.0000 - val_loss: 0.6235 - val_accuracy: 0.9038\n",
      "Epoch 269/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.3642e-06 - accuracy: 1.0000 - val_loss: 0.5517 - val_accuracy: 0.9038\n",
      "Epoch 270/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.3328e-06 - accuracy: 1.0000 - val_loss: 0.5428 - val_accuracy: 0.9231\n",
      "Epoch 271/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.9664e-06 - accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 0.9231\n",
      "Epoch 272/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.9010e-06 - accuracy: 1.0000 - val_loss: 0.4911 - val_accuracy: 0.8846\n",
      "Epoch 273/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.7339e-06 - accuracy: 1.0000 - val_loss: 0.4252 - val_accuracy: 0.9423\n",
      "Epoch 274/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 283ms/step - loss: 1.8803e-06 - accuracy: 1.0000 - val_loss: 0.6105 - val_accuracy: 0.9231\n",
      "Epoch 275/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.1935e-06 - accuracy: 1.0000 - val_loss: 0.3874 - val_accuracy: 0.9231\n",
      "Epoch 276/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9238e-06 - accuracy: 1.0000 - val_loss: 0.5444 - val_accuracy: 0.9231\n",
      "Epoch 277/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.8856e-06 - accuracy: 1.0000 - val_loss: 0.4104 - val_accuracy: 0.9231\n",
      "Epoch 278/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.9984e-06 - accuracy: 1.0000\n",
      "Epoch 00278: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.9984e-06 - accuracy: 1.0000 - val_loss: 0.5735 - val_accuracy: 0.8654\n",
      "Epoch 279/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.5595e-06 - accuracy: 1.0000 - val_loss: 0.6688 - val_accuracy: 0.9038\n",
      "Epoch 280/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.7291e-06 - accuracy: 1.0000 - val_loss: 0.4208 - val_accuracy: 0.9038\n",
      "Epoch 281/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.1072e-06 - accuracy: 1.0000 - val_loss: 0.6683 - val_accuracy: 0.8846\n",
      "Epoch 282/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.5050e-06 - accuracy: 1.0000 - val_loss: 0.5483 - val_accuracy: 0.9038\n",
      "Epoch 283/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.7455e-06 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 0.9423\n",
      "Epoch 284/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.5983e-06 - accuracy: 1.0000 - val_loss: 0.5019 - val_accuracy: 0.9231\n",
      "Epoch 285/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.2872e-06 - accuracy: 1.0000 - val_loss: 0.4495 - val_accuracy: 0.9423\n",
      "Epoch 286/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.0514e-06 - accuracy: 1.0000 - val_loss: 0.4542 - val_accuracy: 0.9423\n",
      "Epoch 287/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.6643e-06 - accuracy: 1.0000 - val_loss: 0.4147 - val_accuracy: 0.9231\n",
      "Epoch 288/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.6070e-06 - accuracy: 1.0000 - val_loss: 0.2330 - val_accuracy: 0.9423\n",
      "Epoch 289/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9478e-06 - accuracy: 1.0000 - val_loss: 0.6147 - val_accuracy: 0.8846\n",
      "Epoch 290/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.4078e-06 - accuracy: 1.0000 - val_loss: 0.7026 - val_accuracy: 0.9038\n",
      "Epoch 291/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.5239e-06 - accuracy: 1.0000 - val_loss: 0.6240 - val_accuracy: 0.9231\n",
      "Epoch 292/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.7973e-06 - accuracy: 1.0000 - val_loss: 0.6713 - val_accuracy: 0.9038\n",
      "Epoch 293/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.6297e-06 - accuracy: 1.0000 - val_loss: 0.2958 - val_accuracy: 0.9231\n",
      "Epoch 294/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.5096e-06 - accuracy: 1.0000 - val_loss: 0.4933 - val_accuracy: 0.9038\n",
      "Epoch 295/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.7442e-06 - accuracy: 1.0000 - val_loss: 0.6148 - val_accuracy: 0.8846\n",
      "Epoch 296/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 6.3949e-06 - accuracy: 1.0000 - val_loss: 0.3844 - val_accuracy: 0.9231\n",
      "Epoch 297/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9356e-06 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.9038\n",
      "Epoch 298/2000\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 1.5197e-06 - accuracy: 1.0000 - val_loss: 0.5950 - val_accuracy: 0.9038\n",
      "Epoch 299/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.0056e-06 - accuracy: 1.0000 - val_loss: 0.5641 - val_accuracy: 0.9231\n",
      "Epoch 300/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.8682e-06 - accuracy: 1.0000 - val_loss: 0.5821 - val_accuracy: 0.9038\n",
      "Epoch 301/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.8181e-06 - accuracy: 1.0000 - val_loss: 0.6291 - val_accuracy: 0.9038\n",
      "Epoch 302/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.7211e-06 - accuracy: 1.0000 - val_loss: 0.7159 - val_accuracy: 0.9038\n",
      "Epoch 303/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1416e-06 - accuracy: 1.0000 - val_loss: 0.5796 - val_accuracy: 0.9038\n",
      "Epoch 304/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 9.8516e-07 - accuracy: 1.0000 - val_loss: 0.5414 - val_accuracy: 0.9038\n",
      "Epoch 305/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2142e-06 - accuracy: 1.0000 - val_loss: 0.4072 - val_accuracy: 0.9038\n",
      "Epoch 306/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2229e-06 - accuracy: 1.0000 - val_loss: 0.7757 - val_accuracy: 0.8654\n",
      "Epoch 307/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2543e-06 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.9038\n",
      "Epoch 308/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.6526e-07 - accuracy: 1.0000 - val_loss: 0.4872 - val_accuracy: 0.9038\n",
      "Epoch 309/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.4362e-06 - accuracy: 1.0000 - val_loss: 0.6103 - val_accuracy: 0.9038\n",
      "Epoch 310/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.7784e-07 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9615\n",
      "Epoch 311/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.0636e-06 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.9231\n",
      "Epoch 312/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2722e-06 - accuracy: 1.0000 - val_loss: 0.6550 - val_accuracy: 0.9231\n",
      "Epoch 313/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.0860e-06 - accuracy: 1.0000 - val_loss: 0.7336 - val_accuracy: 0.9038\n",
      "Epoch 314/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1543e-06 - accuracy: 1.0000 - val_loss: 0.8303 - val_accuracy: 0.9038\n",
      "Epoch 315/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1306e-06 - accuracy: 1.0000 - val_loss: 0.3670 - val_accuracy: 0.9038\n",
      "Epoch 316/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.0217e-06 - accuracy: 1.0000 - val_loss: 0.3988 - val_accuracy: 0.9423\n",
      "Epoch 317/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2519e-06 - accuracy: 1.0000 - val_loss: 0.4374 - val_accuracy: 0.9231\n",
      "Epoch 318/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.3973e-06 - accuracy: 1.0000 - val_loss: 0.6442 - val_accuracy: 0.9038\n",
      "Epoch 319/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.6936e-07 - accuracy: 1.0000 - val_loss: 0.6351 - val_accuracy: 0.9231\n",
      "Epoch 320/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.3099e-06 - accuracy: 1.0000 - val_loss: 0.3280 - val_accuracy: 0.9423\n",
      "Epoch 321/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.4177e-06 - accuracy: 1.0000 - val_loss: 0.4527 - val_accuracy: 0.9231\n",
      "Epoch 322/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.6767e-07 - accuracy: 1.0000 - val_loss: 0.6208 - val_accuracy: 0.9038\n",
      "Epoch 323/2000\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 7.9897e-07 - accuracy: 1.0000 - val_loss: 0.7357 - val_accuracy: 0.9038\n",
      "Epoch 324/2000\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 9.6457e-07 - accuracy: 1.0000 - val_loss: 0.5843 - val_accuracy: 0.9231\n",
      "Epoch 325/2000\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 7.8366e-07 - accuracy: 1.0000 - val_loss: 0.2458 - val_accuracy: 0.9615\n",
      "Epoch 326/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 7.5475e-07 - accuracy: 1.0000 - val_loss: 0.7417 - val_accuracy: 0.9231\n",
      "Epoch 327/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 289ms/step - loss: 6.0398e-07 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 0.9423\n",
      "Epoch 328/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 8.0348e-07 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.8846\n",
      "Epoch 329/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.6136e-07 - accuracy: 1.0000 - val_loss: 0.3783 - val_accuracy: 0.9231\n",
      "Epoch 330/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 7.2921e-07 - accuracy: 1.0000 - val_loss: 0.7297 - val_accuracy: 0.9038\n",
      "Epoch 331/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 7.6980e-07 - accuracy: 1.0000 - val_loss: 0.2265 - val_accuracy: 0.9423\n",
      "Epoch 332/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.0688e-06 - accuracy: 1.0000 - val_loss: 0.7846 - val_accuracy: 0.9038\n",
      "Epoch 333/2000\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 4.2524e-06 - accuracy: 1.0000 - val_loss: 0.7272 - val_accuracy: 0.9038\n",
      "Epoch 334/2000\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 1.1469e-06 - accuracy: 1.0000 - val_loss: 0.7187 - val_accuracy: 0.8654\n",
      "Epoch 335/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1202e-06 - accuracy: 1.0000 - val_loss: 0.7546 - val_accuracy: 0.9038\n",
      "Epoch 336/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.9376e-07 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.8846\n",
      "Epoch 337/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 8.3178e-07 - accuracy: 1.0000 - val_loss: 0.3152 - val_accuracy: 0.9423\n",
      "Epoch 338/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.0843e-06 - accuracy: 1.0000 - val_loss: 0.4417 - val_accuracy: 0.9038\n",
      "2  번째 학습을 완료했습니다.\n",
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 8s - loss: 15.2784 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0979s vs `on_train_batch_end` time: 0.1906s). Check your callbacks.\n",
      "63/63 [==============================] - 21s 326ms/step - loss: 4.3503 - accuracy: 0.1057 - val_loss: 5203.5005 - val_accuracy: 0.0962\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.3097 - accuracy: 0.1328 - val_loss: 466.1134 - val_accuracy: 0.0962\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.2074 - accuracy: 0.1829 - val_loss: 20.7384 - val_accuracy: 0.0962\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.0915 - accuracy: 0.2375 - val_loss: 2.9564 - val_accuracy: 0.0962\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.7141 - accuracy: 0.3963 - val_loss: 3.1051 - val_accuracy: 0.0769\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.4684 - accuracy: 0.4885 - val_loss: 3.2790 - val_accuracy: 0.0962\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.2105 - accuracy: 0.6012 - val_loss: 2.7300 - val_accuracy: 0.0962\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - 32s 505ms/step - loss: 1.0063 - accuracy: 0.6498 - val_loss: 3.2288 - val_accuracy: 0.1154\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.8208 - accuracy: 0.7229 - val_loss: 4.1493 - val_accuracy: 0.0769\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - 35s 557ms/step - loss: 0.8080 - accuracy: 0.7260 - val_loss: 4.8935 - val_accuracy: 0.1538\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - 20s 317ms/step - loss: 0.7406 - accuracy: 0.7540 - val_loss: 2.4388 - val_accuracy: 0.3269\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - 20s 318ms/step - loss: 0.6023 - accuracy: 0.7976 - val_loss: 1.1024 - val_accuracy: 0.7115\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.5089 - accuracy: 0.8257 - val_loss: 1.0431 - val_accuracy: 0.6731\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.4564 - accuracy: 0.8542 - val_loss: 0.8194 - val_accuracy: 0.6923\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - 31s 498ms/step - loss: 0.4606 - accuracy: 0.8432 - val_loss: 0.7023 - val_accuracy: 0.7885\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.4333 - accuracy: 0.8537 - val_loss: 0.7509 - val_accuracy: 0.7885\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - 38s 604ms/step - loss: 0.4230 - accuracy: 0.8542 - val_loss: 0.6475 - val_accuracy: 0.8077\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - 20s 319ms/step - loss: 0.3264 - accuracy: 0.8878 - val_loss: 0.5423 - val_accuracy: 0.8269\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.2911 - accuracy: 0.9028 - val_loss: 0.4760 - val_accuracy: 0.8269\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.3397 - accuracy: 0.8878 - val_loss: 0.7806 - val_accuracy: 0.7692\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - 34s 546ms/step - loss: 0.3059 - accuracy: 0.8933 - val_loss: 0.4064 - val_accuracy: 0.9038\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.2795 - accuracy: 0.9008 - val_loss: 0.6431 - val_accuracy: 0.8462\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.2432 - accuracy: 0.9193 - val_loss: 0.4600 - val_accuracy: 0.8654\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1785 - accuracy: 0.9394 - val_loss: 0.5169 - val_accuracy: 0.8269\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2466 - accuracy: 0.9173 - val_loss: 0.9646 - val_accuracy: 0.8077\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1652 - accuracy: 0.9434 - val_loss: 0.7137 - val_accuracy: 0.8269\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1927 - accuracy: 0.9339 - val_loss: 0.8734 - val_accuracy: 0.7692\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.2530 - accuracy: 0.9148 - val_loss: 0.5425 - val_accuracy: 0.7885\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.2571 - accuracy: 0.9148 - val_loss: 0.6026 - val_accuracy: 0.8269\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1675 - accuracy: 0.9419 - val_loss: 0.5621 - val_accuracy: 0.8077\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1275 - accuracy: 0.9479 - val_loss: 0.5057 - val_accuracy: 0.8654\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1359 - accuracy: 0.9489 - val_loss: 0.9969 - val_accuracy: 0.8077\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1254 - accuracy: 0.9604 - val_loss: 0.6085 - val_accuracy: 0.8654\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1537 - accuracy: 0.9484 - val_loss: 0.9675 - val_accuracy: 0.7692\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1627 - accuracy: 0.9434 - val_loss: 1.0136 - val_accuracy: 0.8077\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1260 - accuracy: 0.9564 - val_loss: 0.7352 - val_accuracy: 0.8269\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1208 - accuracy: 0.9579 - val_loss: 0.8787 - val_accuracy: 0.8077\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1439 - accuracy: 0.9524 - val_loss: 1.4269 - val_accuracy: 0.6923\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1329 - accuracy: 0.9594 - val_loss: 2.3917 - val_accuracy: 0.5769\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1124 - accuracy: 0.9619 - val_loss: 0.9108 - val_accuracy: 0.7500\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0997 - accuracy: 0.9659 - val_loss: 0.7336 - val_accuracy: 0.8077\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1052 - accuracy: 0.9624 - val_loss: 1.2076 - val_accuracy: 0.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1571 - accuracy: 0.9504 - val_loss: 1.4902 - val_accuracy: 0.7500\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1535 - accuracy: 0.9449 - val_loss: 1.8998 - val_accuracy: 0.6923\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.2110 - accuracy: 0.9269 - val_loss: 0.6903 - val_accuracy: 0.8462\n",
      "Epoch 46/2000\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.1214 - accuracy: 0.9569 - val_loss: 1.1724 - val_accuracy: 0.8077\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.1072 - accuracy: 0.9674 - val_loss: 1.0429 - val_accuracy: 0.8269\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1837 - accuracy: 0.9444 - val_loss: 0.9374 - val_accuracy: 0.8269\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1836 - accuracy: 0.9414 - val_loss: 0.8385 - val_accuracy: 0.8462\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1242 - accuracy: 0.9609 - val_loss: 0.8533 - val_accuracy: 0.7692\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1196 - accuracy: 0.9639 - val_loss: 1.1016 - val_accuracy: 0.7885\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0708 - accuracy: 0.9749 - val_loss: 0.6594 - val_accuracy: 0.8654\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0542 - accuracy: 0.9840 - val_loss: 0.6983 - val_accuracy: 0.8077\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0677 - accuracy: 0.9795 - val_loss: 0.8130 - val_accuracy: 0.8462\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1526 - accuracy: 0.9549 - val_loss: 1.2725 - val_accuracy: 0.8077\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0750 - accuracy: 0.9739 - val_loss: 0.6804 - val_accuracy: 0.8462\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 0.5004 - val_accuracy: 0.8269\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0379 - accuracy: 0.9870 - val_loss: 0.5494 - val_accuracy: 0.8846\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0475 - accuracy: 0.9820 - val_loss: 1.0725 - val_accuracy: 0.8654\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1468 - accuracy: 0.9614 - val_loss: 1.1829 - val_accuracy: 0.7692\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1564 - accuracy: 0.9514 - val_loss: 1.2314 - val_accuracy: 0.7885\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1403 - accuracy: 0.9564 - val_loss: 0.7822 - val_accuracy: 0.8654\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1522 - accuracy: 0.9519 - val_loss: 1.2159 - val_accuracy: 0.8077\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1371 - accuracy: 0.9554 - val_loss: 0.9890 - val_accuracy: 0.8654\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0653 - accuracy: 0.9765 - val_loss: 0.8916 - val_accuracy: 0.8269\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0594 - accuracy: 0.9795 - val_loss: 1.1039 - val_accuracy: 0.7885\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1076 - accuracy: 0.9669 - val_loss: 1.0520 - val_accuracy: 0.8654\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0818 - accuracy: 0.9744 - val_loss: 0.6975 - val_accuracy: 0.8654\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - 39s 617ms/step - loss: 0.0698 - accuracy: 0.9825 - val_loss: 0.5363 - val_accuracy: 0.9231\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0636 - accuracy: 0.9790 - val_loss: 0.7894 - val_accuracy: 0.9038\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0467 - accuracy: 0.9835 - val_loss: 0.7849 - val_accuracy: 0.8654\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0517 - accuracy: 0.9815 - val_loss: 0.8292 - val_accuracy: 0.8269\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0981 - accuracy: 0.9679 - val_loss: 1.0225 - val_accuracy: 0.8846\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0629 - accuracy: 0.9785 - val_loss: 1.3600 - val_accuracy: 0.8269\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1018 - accuracy: 0.9704 - val_loss: 1.1444 - val_accuracy: 0.7692\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1306 - accuracy: 0.9644 - val_loss: 1.0924 - val_accuracy: 0.7885\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1125 - accuracy: 0.9659 - val_loss: 0.8505 - val_accuracy: 0.8462\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1252 - accuracy: 0.9619 - val_loss: 0.5906 - val_accuracy: 0.9038\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0681 - accuracy: 0.9770 - val_loss: 0.9900 - val_accuracy: 0.8269\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1242 - accuracy: 0.9649 - val_loss: 0.7920 - val_accuracy: 0.8269\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0477 - accuracy: 0.9835 - val_loss: 1.6156 - val_accuracy: 0.7885\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0200 - accuracy: 0.9945 - val_loss: 1.0188 - val_accuracy: 0.8654\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0731 - accuracy: 0.9800 - val_loss: 1.6042 - val_accuracy: 0.8077\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0928 - accuracy: 0.9724 - val_loss: 0.3281 - val_accuracy: 0.9231\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0583 - accuracy: 0.9795 - val_loss: 1.4901 - val_accuracy: 0.8077\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1355 - accuracy: 0.9639 - val_loss: 0.9806 - val_accuracy: 0.8269\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0865 - accuracy: 0.9729 - val_loss: 0.8195 - val_accuracy: 0.8269\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0887 - accuracy: 0.9795 - val_loss: 1.2192 - val_accuracy: 0.8077\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0481 - accuracy: 0.9855 - val_loss: 1.3571 - val_accuracy: 0.9038\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0500 - accuracy: 0.9855 - val_loss: 1.0527 - val_accuracy: 0.8462\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0207 - accuracy: 0.9940 - val_loss: 1.0139 - val_accuracy: 0.8846\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0179 - accuracy: 0.9945 - val_loss: 0.9151 - val_accuracy: 0.8846\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 1.0550 - val_accuracy: 0.8269\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.7220 - val_accuracy: 0.9038\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0281 - accuracy: 0.9915 - val_loss: 0.7243 - val_accuracy: 0.8654\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0949 - accuracy: 0.9805 - val_loss: 0.9903 - val_accuracy: 0.8654\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1259 - accuracy: 0.9659 - val_loss: 1.1335 - val_accuracy: 0.8269\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1040 - accuracy: 0.9699 - val_loss: 0.9517 - val_accuracy: 0.8654\n",
      "Epoch 99/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1296 - accuracy: 0.9614 - val_loss: 1.5041 - val_accuracy: 0.7308\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0669 - accuracy: 0.9775 - val_loss: 0.7864 - val_accuracy: 0.8462\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1101 - accuracy: 0.9724 - val_loss: 1.0683 - val_accuracy: 0.8269\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0662 - accuracy: 0.9770 - val_loss: 1.0443 - val_accuracy: 0.8077\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1133 - accuracy: 0.9689 - val_loss: 1.6468 - val_accuracy: 0.6923\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0611 - accuracy: 0.9850 - val_loss: 1.5968 - val_accuracy: 0.8462\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0957 - accuracy: 0.9744 - val_loss: 0.7450 - val_accuracy: 0.8846\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0415 - accuracy: 0.9860 - val_loss: 1.0464 - val_accuracy: 0.8654\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0813 - accuracy: 0.9765 - val_loss: 1.4801 - val_accuracy: 0.7692\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0661 - accuracy: 0.9775 - val_loss: 1.4041 - val_accuracy: 0.8462\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1067 - accuracy: 0.9714 - val_loss: 1.8894 - val_accuracy: 0.7692\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0330 - accuracy: 0.9895 - val_loss: 1.0986 - val_accuracy: 0.8654\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0725 - accuracy: 0.9820 - val_loss: 1.3794 - val_accuracy: 0.8846\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1136 - accuracy: 0.9684 - val_loss: 1.3954 - val_accuracy: 0.8462\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0967 - accuracy: 0.9699 - val_loss: 1.0054 - val_accuracy: 0.8462\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0720 - accuracy: 0.9775 - val_loss: 1.0177 - val_accuracy: 0.8846\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0415 - accuracy: 0.9870 - val_loss: 0.7923 - val_accuracy: 0.8654\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0440 - accuracy: 0.9885 - val_loss: 0.8978 - val_accuracy: 0.8077\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0495 - accuracy: 0.9875 - val_loss: 1.4049 - val_accuracy: 0.8077\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0201 - accuracy: 0.9945 - val_loss: 0.6792 - val_accuracy: 0.9038\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0244 - accuracy: 0.9940 - val_loss: 0.7794 - val_accuracy: 0.9038\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0219 - accuracy: 0.9925 - val_loss: 0.8500 - val_accuracy: 0.8462\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0255 - accuracy: 0.9935 - val_loss: 0.8712 - val_accuracy: 0.8462\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0272 - accuracy: 0.9900 - val_loss: 0.7091 - val_accuracy: 0.8846\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0234 - accuracy: 0.9920 - val_loss: 0.8733 - val_accuracy: 0.8846\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0227 - accuracy: 0.9920 - val_loss: 1.0200 - val_accuracy: 0.8462\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0675 - accuracy: 0.9825 - val_loss: 0.7901 - val_accuracy: 0.8846\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0961 - accuracy: 0.9755 - val_loss: 0.5102 - val_accuracy: 0.9231\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1283 - accuracy: 0.9629 - val_loss: 2.4242 - val_accuracy: 0.8077\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1410 - accuracy: 0.9664 - val_loss: 0.6394 - val_accuracy: 0.9038\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0736 - accuracy: 0.9830 - val_loss: 0.7699 - val_accuracy: 0.8269\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0336 - accuracy: 0.9920 - val_loss: 0.7317 - val_accuracy: 0.8846\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 0.7915 - val_accuracy: 0.9038\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0160 - accuracy: 0.9960 - val_loss: 0.9184 - val_accuracy: 0.8654\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0492 - accuracy: 0.9875 - val_loss: 0.9148 - val_accuracy: 0.8654\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1453 - accuracy: 0.9659 - val_loss: 2.1105 - val_accuracy: 0.7692\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2573 - accuracy: 0.9464 - val_loss: 3.4033 - val_accuracy: 0.6154\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1316 - accuracy: 0.9699 - val_loss: 2.5288 - val_accuracy: 0.8077\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1002 - accuracy: 0.9765 - val_loss: 1.8148 - val_accuracy: 0.8269\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1038 - accuracy: 0.9770 - val_loss: 1.1148 - val_accuracy: 0.8846\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0482 - accuracy: 0.9865 - val_loss: 1.1019 - val_accuracy: 0.8846\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0326 - accuracy: 0.9885 - val_loss: 1.0827 - val_accuracy: 0.8846\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0141 - accuracy: 0.9970 - val_loss: 0.8596 - val_accuracy: 0.8654\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0237 - accuracy: 0.9950 - val_loss: 1.2032 - val_accuracy: 0.8462\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 1.0598 - val_accuracy: 0.8846\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0139 - accuracy: 0.9970 - val_loss: 6.8601 - val_accuracy: 0.6154\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0262 - accuracy: 0.9935 - val_loss: 1.4946 - val_accuracy: 0.7885\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0319 - accuracy: 0.9900 - val_loss: 1.0394 - val_accuracy: 0.8462\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 1.2171 - val_accuracy: 0.8654\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - 36s 573ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.7936 - val_accuracy: 0.9423\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.8200 - val_accuracy: 0.8462\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.8010 - val_accuracy: 0.8654\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0029 - accuracy: 0.9980 - val_loss: 0.7015 - val_accuracy: 0.9038\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0153 - accuracy: 0.9980 - val_loss: 1.0618 - val_accuracy: 0.8462\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0371 - accuracy: 0.9895 - val_loss: 1.7407 - val_accuracy: 0.8269\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0167 - accuracy: 0.9940 - val_loss: 1.9703 - val_accuracy: 0.8269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 1.3160 - val_accuracy: 0.8654\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0288 - accuracy: 0.9940 - val_loss: 1.4056 - val_accuracy: 0.8462\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0109 - accuracy: 0.9960 - val_loss: 0.9535 - val_accuracy: 0.8846\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0318 - accuracy: 0.9925 - val_loss: 0.9846 - val_accuracy: 0.9038\n",
      "Epoch 159/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0318 - accuracy: 0.9935 - val_loss: 0.9859 - val_accuracy: 0.8846\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 0.9137 - val_accuracy: 0.9038\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0123 - accuracy: 0.9950 - val_loss: 1.0266 - val_accuracy: 0.8846\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.2012 - accuracy: 0.9594 - val_loss: 3.3065 - val_accuracy: 0.5769\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1756 - accuracy: 0.9564 - val_loss: 1.2587 - val_accuracy: 0.8654\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1766 - accuracy: 0.9614 - val_loss: 6.4860 - val_accuracy: 0.6538\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0399 - accuracy: 0.9895 - val_loss: 1.1445 - val_accuracy: 0.8462\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0971 - accuracy: 0.9775 - val_loss: 0.9216 - val_accuracy: 0.9038\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0754 - accuracy: 0.9830 - val_loss: 1.5174 - val_accuracy: 0.8077\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0468 - accuracy: 0.9855 - val_loss: 1.0125 - val_accuracy: 0.8269\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0063 - accuracy: 0.9975 - val_loss: 1.1873 - val_accuracy: 0.9038\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0081 - accuracy: 0.9960 - val_loss: 1.0998 - val_accuracy: 0.8462\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.8466 - val_accuracy: 0.8269\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.9869 - val_accuracy: 0.8654\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0079 - accuracy: 0.9970 - val_loss: 0.9420 - val_accuracy: 0.8077\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0063 - accuracy: 0.9975 - val_loss: 1.0709 - val_accuracy: 0.8462\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0165 - accuracy: 0.9940 - val_loss: 0.5736 - val_accuracy: 0.8077\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0557 - accuracy: 0.9870 - val_loss: 1.9206 - val_accuracy: 0.7885\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0638 - accuracy: 0.9830 - val_loss: 1.8600 - val_accuracy: 0.7692\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1486 - accuracy: 0.9634 - val_loss: 0.9635 - val_accuracy: 0.8462\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0420 - accuracy: 0.9925 - val_loss: 0.9912 - val_accuracy: 0.8846\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.9580 - val_accuracy: 0.8846\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0471 - accuracy: 0.9860 - val_loss: 1.4277 - val_accuracy: 0.8846\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0334 - accuracy: 0.9910 - val_loss: 1.2801 - val_accuracy: 0.8462\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0200 - accuracy: 0.9970 - val_loss: 1.4760 - val_accuracy: 0.8846\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9960\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0104 - accuracy: 0.9960 - val_loss: 1.6610 - val_accuracy: 0.8654\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0021 - accuracy: 0.9990 - val_loss: 1.0642 - val_accuracy: 0.8846\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 7.8178e-04 - accuracy: 1.0000 - val_loss: 1.2073 - val_accuracy: 0.8846\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.9088 - val_accuracy: 0.9038\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 6.7211e-04 - accuracy: 1.0000 - val_loss: 1.0261 - val_accuracy: 0.8462\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.7393e-04 - accuracy: 1.0000 - val_loss: 1.0564 - val_accuracy: 0.8654\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.6208e-04 - accuracy: 1.0000 - val_loss: 1.2522 - val_accuracy: 0.8462\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.0403e-04 - accuracy: 1.0000 - val_loss: 1.1453 - val_accuracy: 0.8654\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.8226e-04 - accuracy: 1.0000 - val_loss: 1.2060 - val_accuracy: 0.8269\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.9689e-05 - accuracy: 1.0000 - val_loss: 1.3142 - val_accuracy: 0.8654\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.2040e-05 - accuracy: 1.0000 - val_loss: 1.0729 - val_accuracy: 0.8846\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.8079e-05 - accuracy: 1.0000 - val_loss: 1.1699 - val_accuracy: 0.8269\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.7713e-05 - accuracy: 1.0000 - val_loss: 1.1681 - val_accuracy: 0.8654\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 6.9948e-05 - accuracy: 1.0000 - val_loss: 1.1935 - val_accuracy: 0.8654\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 7.2872e-05 - accuracy: 1.0000 - val_loss: 1.3037 - val_accuracy: 0.8846\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1605e-04 - accuracy: 1.0000 - val_loss: 1.2736 - val_accuracy: 0.8462\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.7822e-05 - accuracy: 1.0000 - val_loss: 1.0927 - val_accuracy: 0.8846\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.5444e-05 - accuracy: 1.0000 - val_loss: 1.2050 - val_accuracy: 0.8846\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.9536e-05 - accuracy: 1.0000 - val_loss: 1.3457 - val_accuracy: 0.8654\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.5433e-05 - accuracy: 1.0000 - val_loss: 1.2896 - val_accuracy: 0.8462\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 5.2928e-05 - accuracy: 1.0000 - val_loss: 1.0525 - val_accuracy: 0.8654\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 3.3789e-05 - accuracy: 1.0000 - val_loss: 1.2249 - val_accuracy: 0.8654\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.0042e-05 - accuracy: 1.0000 - val_loss: 1.1688 - val_accuracy: 0.8654\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.0944e-05 - accuracy: 1.0000 - val_loss: 1.0309 - val_accuracy: 0.8654\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 6.6819e-05 - accuracy: 1.0000 - val_loss: 1.2096 - val_accuracy: 0.8462\n",
      "Epoch 209/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 283ms/step - loss: 2.2952e-04 - accuracy: 1.0000 - val_loss: 1.1623 - val_accuracy: 0.8462\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9559e-04 - accuracy: 1.0000 - val_loss: 1.4051 - val_accuracy: 0.8462\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 8.2798e-05 - accuracy: 1.0000 - val_loss: 1.2936 - val_accuracy: 0.8462\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.2554e-05 - accuracy: 1.0000 - val_loss: 1.0978 - val_accuracy: 0.8846\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.2047e-05 - accuracy: 1.0000 - val_loss: 1.2757 - val_accuracy: 0.8462\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.5512e-05 - accuracy: 1.0000 - val_loss: 1.1816 - val_accuracy: 0.8654\n",
      "Epoch 215/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.3003e-05 - accuracy: 1.0000 - val_loss: 1.2162 - val_accuracy: 0.8846\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 3.3354e-05 - accuracy: 1.0000 - val_loss: 1.3079 - val_accuracy: 0.8654\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 3.5472e-05 - accuracy: 1.0000 - val_loss: 1.2168 - val_accuracy: 0.8462\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.4270e-05 - accuracy: 1.0000 - val_loss: 1.2285 - val_accuracy: 0.8654\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.4574e-05 - accuracy: 1.0000 - val_loss: 1.2322 - val_accuracy: 0.8654\n",
      "Epoch 220/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.1606e-05 - accuracy: 1.0000 - val_loss: 1.3063 - val_accuracy: 0.8654\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.4584e-05 - accuracy: 1.0000 - val_loss: 0.9116 - val_accuracy: 0.8654\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.0236e-05 - accuracy: 1.0000 - val_loss: 1.2099 - val_accuracy: 0.8846\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.6105e-05 - accuracy: 1.0000 - val_loss: 1.3464 - val_accuracy: 0.8462\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 2.2993e-05 - accuracy: 1.0000 - val_loss: 1.1498 - val_accuracy: 0.8654\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.0465e-05 - accuracy: 1.0000 - val_loss: 1.3850 - val_accuracy: 0.8462\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9117e-05 - accuracy: 1.0000 - val_loss: 1.1187 - val_accuracy: 0.8462\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 2.4212e-05 - accuracy: 1.0000 - val_loss: 1.3499 - val_accuracy: 0.8462\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 3.0989e-05 - accuracy: 1.0000 - val_loss: 0.9982 - val_accuracy: 0.8654\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 2.1662e-05 - accuracy: 1.0000 - val_loss: 1.3660 - val_accuracy: 0.8462\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.4125e-05 - accuracy: 1.0000 - val_loss: 1.4301 - val_accuracy: 0.8462\n",
      "Epoch 231/2000\n",
      "18/63 [=======>......................] - ETA: 12s - loss: 1.4184e-05 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "index=1\n",
    "result = 0\n",
    "for train_index,val_index in kfold.split(x,y):\n",
    "    if index!=1:\n",
    "        modelpath = './models/02_03_imger_best_index_{}.h5'.format(index)\n",
    "        cp = ModelCheckpoint(monitor = 'val_accuracy',filepath=modelpath,save_best_only=True)\n",
    "\n",
    "\n",
    "        x_train = x[train_index]\n",
    "        x_val = x[val_index]\n",
    "        y_train = y[train_index]\n",
    "        y_val = y[val_index]\n",
    "\n",
    "        onehot = OneHotEncoder()\n",
    "        y_train = onehot.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "        y_val = onehot.fit_transform(y_val.reshape(-1,1)).toarray().astype('float32')\n",
    "\n",
    "        train_generator = datagen.flow(x_train,y_train,batch_size=32)\n",
    "        val_generator = datagen.flow(x_val,y_val)\n",
    "        model = modeling()\n",
    "        model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n",
    "        model.fit_generator(train_generator,validation_data = val_generator,epochs=epochs,callbacks=[cp,es,reLR])\n",
    "\n",
    "        model = load_model(modelpath)\n",
    "        df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "        x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred,axis=-1)\n",
    "        df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "        df_sub['digit'] = y_pred\n",
    "        df_sub.to_csv('kfold_{}.csv'.format(index))\n",
    "\n",
    "        print(index, \" 번째 학습을 완료했습니다.\")\n",
    "    index+=1\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "0.905\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onOYBeVy-Mkb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KswTfNSi-Mkc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "0.87\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx1Z4nhC-Mkc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "44FNCmNG-Mkd"
   },
   "outputs": [],
   "source": [
    "model = load_model('./models/02_03_imger_best_index_1.h5')\n",
    "df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzM0rw6U-Mke",
    "outputId": "0312f408-37af-44e5-b673-febb0961614c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "binary_model = []\n",
    "for i in range(0,10):\n",
    "    print(i)\n",
    "    model = load_model('./binary_models/{}_binary.h5'.format(i))\n",
    "    binary_model.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QwofhPdW-Mke"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "def ordering(array):\n",
    "    temp = array.copy()\n",
    "    result = []\n",
    "    for i in range(len(temp)):\n",
    "        sol = np.argmax(temp)\n",
    "        result.append(sol)\n",
    "        temp[sol]=0\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fI2mNTi-Mkf",
    "outputId": "f253edd1-103b-41f5-8832-f5ac667f5a38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6569414e-08, 2.2260668e-08, 4.6740688e-06, ..., 2.5446711e-10,\n",
       "        5.9703314e-07, 5.4656005e-11],\n",
       "       [2.9817595e-16, 2.0647546e-12, 4.4424861e-16, ..., 2.6480063e-10,\n",
       "        8.9294266e-11, 1.0000000e+00],\n",
       "       [2.4707546e-05, 1.3310514e-01, 8.9886552e-03, ..., 9.1972132e-04,\n",
       "        1.2284001e-02, 2.9377347e-06],\n",
       "       ...,\n",
       "       [8.6063210e-09, 7.5784501e-10, 2.6377617e-10, ..., 3.1178827e-13,\n",
       "        1.2005766e-07, 6.1626551e-15],\n",
       "       [1.0054513e-03, 2.9064235e-01, 1.6038346e-05, ..., 1.7447629e-06,\n",
       "        8.1512779e-03, 4.5916289e-03],\n",
       "       [9.9825722e-01, 6.9838888e-16, 2.4161539e-10, ..., 1.8471900e-15,\n",
       "        6.6174334e-13, 1.4705076e-15]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy1domP0-Mkf",
    "outputId": "db3b1dcb-6d43-4fc7-9938-34990934eb64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, ..., 6, 5, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_notyet = np.argmax(y_pred,axis=-1)\n",
    "y_notyet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1xQykxG-Mkf",
    "outputId": "946d6597-cef3-4297-f226-1a1349f5005d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.88120088e-36],\n",
       "       [1.13777095e-36]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model[3].predict(x_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULE3KyHS-Mkg",
    "outputId": "41ead2b9-7d2e-44a9-a6ba-e8c4645752cd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cc4ab4411252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Jhbsfx-Mkg"
   },
   "outputs": [],
   "source": [
    "k=729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPmflg5c-Mkg",
    "outputId": "05282ba4-d754-4522-aa33-73d47b49b612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래모델 :  9 \n",
      "원래모델확률 :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] \n",
      "바이너리 :  1.6463632e-24\n",
      "0 0.0\n",
      "1 5.6854813e-21\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 3.455557e-05\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 1.2415284e-22\n",
      "9 1.6463632e-24\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "for i in [k]:\n",
    "    temp_result = y_notyet[i]\n",
    "    binary_result = binary_model[temp_result].predict(np.array([x_test[i].reshape(28,28,1)]))\n",
    "    a=np.round(y_pred[i],3)\n",
    "    print(\"원래모델 : \",temp_result,'\\n원래모델확률 : ',a,\"\\n바이너리 : \",binary_result[0][0])\n",
    "    print(\"0\",binary_model[0].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"1\",binary_model[1].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"2\",binary_model[2].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"3\",binary_model[3].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"4\",binary_model[4].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"5\",binary_model[5].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"6\",binary_model[6].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"7\",binary_model[7].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"8\",binary_model[8].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"9\",binary_model[9].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEgcogTL-Mkg",
    "outputId": "b1e9adb4-4593-4bda-ae06-bb49fd8bf6ac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-LIIBisa-Mkh",
    "outputId": "aa584c54-1643-48af-eb35-d853835f2d2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8, 8, ..., 6, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred,axis=-1)\n",
    "df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "df_sub['digit'] = y_pred\n",
    "df_sub.to_csv('test_4.csv')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT0ZAdKJ-Mkh",
    "outputId": "4f04bdd5-32cc-4226-9a6a-19a6aa77a588"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 8, ..., 6, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TjhvZQW-Mkh",
    "outputId": "4b83ad9f-6588-4c0e-c152-b4310a2c4f32"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUzElEQVR4nO3da4yc1XkH8P9/9mp2fbfXbGxzM04TenOSjQuCtlSokSFSDKpSxVUjIqE4UkMVqvSCaKXwoapo2gTlQ0RlihWnSklQA4JIpMWlSCiKcL12Ddh1AQPG+IKNsc36gr2Xefphh2oD+z7PMGdm3qHn/5NWuztnznnPvDvPzuw+73MOzQwi8v9fpewJiEh7KNhFMqFgF8mEgl0kEwp2kUx0t/Ngvey3fg449wgyA14zG5nRBxkgJWuROnbQ38uohOclYey6xvfGTugLAEyce1ljh+M3HgfncRbjdmHWwZOCneQ6AN8B0AXgH83sHu/+/RzA1T3riu9gVfd4NjlZPJfuxN9bDN7kBHNr6dhBf5ucKO7a1ZU29tRU0L3xaLdqWsBEj80mxosbK37fpLFTx4/iwDlv26aeKJ6SO6qDZBeA7wK4EcBVADaQvKrR8USktVL+Zl8LYJ+ZvWJm4wB+CGB9c6YlIs2WEuzLAbw+4/uDtdt+AcmNJEdJjk7Y+YTDiUiKlGCf7Y+19/0xYWabzGzEzEZ62J9wOBFJkRLsBwGsnPH9CgCH06YjIq2SEuzbAawmeTnJXgBfAPBYc6YlIs3WcL7KzCZJ3g7g3zCdettsZntSJuOl1lrNS18B8FNUVT89FeVs2d3jtqfMLUyddbf4uipnbgyygmEKKnpsPb3FfaPUWZiyTLuwI5q7e2hvbtXieSUlp83scQCPp4whIu2hy2VFMqFgF8mEgl0kEwp2kUwo2EUyoWAXyURb69lh5uY3U8pUoxy9l3OtDeD37y7ObYaFmkEePsr5xnN38tEMcviJpZphmao55bfR9QXB2GH5bkJZcnRtQ2rpcMqxzXs6OXX2emUXyYSCXSQTCnaRTCjYRTKhYBfJhIJdJBPtTb0BbionLscsTtVEabswvZXQP0wZVoKxg7RhWA7ppJiiMtLwcUcpzeixO6m/sHQ3YBONlxanPu6U1Nr0AZyfWZCSdDklrnplF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTLQ3z066pYFu6V4Tju0Jc9lO/5TrAwDEO36GO6U65zQ1Tx4Iy1Ar3jUAQflswnLL0wMUzy112fLUawTcLZ8Td5gtHLahXiLyoaNgF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTba5nt6D2uvG8a5iL9vKaQJjbdCUuFR1eA1Bt/HdytAz1vr/5VMNjA8CVd+1w2718dHT9QbSddHheWyh1KemUPL17/YHzPE8KdpL7AZwGMAVg0sxGUsYTkdZpxiv775jZ8SaMIyItpL/ZRTKRGuwG4AmSO0hunO0OJDeSHCU5OmEXEg8nIo1KfRt/rZkdJjkEYCvJ/zGzp2fewcw2AdgEAPMqi8Jt0USkNZJe2c3scO3zMQCPAFjbjEmJSPM1HOwkB0jOffdrAJ8BsLtZExOR5kp5G78MwCOczhF3A/hnM/vXlMmkrI8e5aLj9c0T3uRE1wdEOdXg2NE1BPv+tjjj2XXe79sV/BulMun3r669ym3nz58tbAvPS3BtRHQNQVIePtqqOqqHj67bcH7m8foFDmeX6oaD3cxeAfDrjfYXkfZS6k0kEwp2kUwo2EUyoWAXyYSCXSQTbS5xZZBmipZzdvoGZaboSkvTJC25HKSQXv6mfy3S1CI/RdX9VnGqZuB1tysm5vppnkq0c/GUk+sB0L38I4VtduaM27d69h23PUrdeam5OO3nP66kkuhAUlrPW6G6semIyIeNgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTHy4lpL2cqNRSWLClsyAn2c/u+7X3L6vf9bPs/cv8PPNX/7YM277P2z/7cK2sS7/R2yVoIw0SDe/duOg2z73tYHCtqGfvur2tVNv+wdPEG6jHeTZw62wo/Jb7/kYPBfDawAK6JVdJBMKdpFMKNhFMqFgF8mEgl0kEwp2kUwo2EUy0d48u/n5ySg32bLldwEAfp6+smRxYVu1yz/2pZcec9s/tfiA237NwEtu+/391xa2WZBn95YeBoBqj5+Hn+r3288uLz43Y1df6vYd+EnafqEp20VbNW2L7/C6Dk/K0uKqZxcRBbtIJhTsIplQsItkQsEukgkFu0gmFOwimWhzPXuaqN7d7dvr51Ur8+a67UdvvKSwbewK/9gblvl58lvm7XTbV3T7tdM3XPlCYdu/H/U32u05Hfy+p59vvjDk55MHPzZW2HZkwQK376pHg1x3xFmvP3WfgOR69pRrALx1HVLy7CQ3kzxGcveM2xaR3ErypdrnhdE4IlKuet7Gfw/AuvfcdieAJ81sNYAna9+LSAcLg93MngZw4j03rwewpfb1FgA3N3daItJsjf6DbpmZHQGA2uehojuS3EhylOToBC40eDgRSdXy/8ab2SYzGzGzkR70tfpwIlKg0WA/SnIYAGqf/bIuESldo8H+GIBba1/fCuDR5kxHRFolzLOTfBDA9QCWkDwI4BsA7gHwEMnbABwA8PlmTCbcb9vJs0f1x11z/ezghV8q3kcc8OuyJy/yi8KfeuOjbvtQT3EuGgB+td/fZP21M4sK2/qO+7/P57/qz31ijl+rP36Zn2++fOFbhW27+ua7fVOF6yM4wv3bo/5BHt/fYz1YZCCody8SBruZbShouqGhI4pIKXS5rEgmFOwimVCwi2RCwS6SCQW7SCbaX+KakHLwygq7lhVesQsAeONzfh3qO8uCFNOC4rlVxv2+x58edtvvXfhZt706FKRxThanmAbP+10jE4PR9sF+87OvrShsW/RfwZLJCSXNABre2ni6b/DAoi2+w+3HnZRlxQ9Ldylp5yHrlV0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTCjYRTLR3jw7/RyhVaPfPcVJxOqKpW7PMI8+L8irOvnLntP+2MPP+MtxdZ/28+jH1wy67Us3by9si7YOjvLBx/5uxG2vHPNXH1q6s/i8zvvRf7p9w4Wkq8G2yF4uPMijpywFPT1A8Fz2rjcJ+rrls9qyWUQU7CKZULCLZELBLpIJBbtIJhTsIplQsItkos317HRziKwE9exOHv7cygG37/h8f+yqv0suek8VH3v+K/7YfTv2ue024S/HvGTUL0o3L9/s5XOBMKd75Z8843eP8tFenj+l3rweTi492pI5FOXCg+sb3GsEvHr1BHplF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTLQ3z27m1gFHtdXsKZ6uRbnJqDkqZ+8pvsOp1UHO9XNXue0956Jtk/3xT/5ycdvkHP+Brb5jm9se2XfPJ932VX/qjB/kqt310evg5rpTasYR5+mjLcS9/uEaBN6xveXo3VEBkNxM8hjJ3TNuu5vkIZK7ah83ReOISLnqeRv/PQDrZrn9XjNbU/t4vLnTEpFmC4PdzJ4GcKINcxGRFkr5B93tJJ+rvc1fWHQnkhtJjpIcnYC/FpuItE6jwX4fgFUA1gA4AuBbRXc0s01mNmJmIz3wFycUkdZpKNjN7KiZTZlZFcD9ANY2d1oi0mwNBTvJmXsQ3wJgd9F9RaQzhHl2kg8CuB7AEpIHAXwDwPUk12B6ler9AL7SjMm4e1YDqDj5xcp4kCi3tJzt1MrimvK//o2H3b5HJxa47bvPfsRtf/O8v278yUPF/Xv2XeT2PfTn17jtZy/3fyYDQ6fd9he/++nCtkt/4v/MBp4/7LaHnHy1jft5dFs+5I9d8V8nK/sPue1Tb485B/fPixsnTtcw2M1swyw3PxD1E5HOostlRTKhYBfJhIJdJBMKdpFMKNhFMtHmpaThlxaaX9pXHS8uj+076adSKpNz3PbJXr/MdHjp24Vtv9nvp1n65xxx208N7nHbX5mc77b/1dmbC9vGzvlLbFf8zBp6FvjLWP/eql1u+8sXF2+lvW3s427focEVbntUllyZKL5D3yl/y+WxS/rddgteJpeMByd27ExxW5Ql9pbg1pbNIqJgF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQT7c+zp2wv7PTtftHPdc97+Uq3/e0r/d97f3DJ9sK2oS6/jLQrWLZ4YfCw36z6ue43T8wrbOMCPxk9udDPB396hX9e/2zxDre9srj4sX/pOv+B7+hd7Y896Sek6Ty07nf8p37XO24z+t4OkvwM5uYtix6U37Lb2V98ovi4emUXyYSCXSQTCnaRTCjYRTKhYBfJhIJdJBMKdpFMtD/PHuXSPU7u0s6edbsOHvTzyac+6p+K1b1vFLYdmDzn9j1R7XXbt79zudu+ad91bnv1reLxgxWPUbnIPy9DfU7dNYA+OjlfAGeqxVt+jY37NeO9Y8GWzkHJOJ1LOvpP+HnywcP+2gpz3ggS8YePuc1uLj3aTtrZ9txbhlqv7CKZULCLZELBLpIJBbtIJhTsIplQsItkQsEukon25tlJsMvJs3vrYcNfVj6qAb7ogLNFLoAFLyxy2/9o2x8Wtk2N+bnmiw74p7nvpJ/zrfhLnGN4rPi8nV/o9z055NddP3fC305649T1bvurY4sL2848NOz2vWLr6247qkFNebX4vNg5P0/ubqkMwILn6lSw7TKd7cejPLu7/XjKuvEkV5J8iuRekntIfq12+yKSW0m+VPscPK1EpEz1vI2fBPB1M/s4gKsBfJXkVQDuBPCkma0G8GTtexHpUGGwm9kRM9tZ+/o0gL0AlgNYD2BL7W5bANzcojmKSBN8oL/ZSV4G4BMAtgFYZmZHgOlfCCSHCvpsBLARAPrhr9UmIq1T93/jSQ4C+DGAO8zM/+/FDGa2ycxGzGykh37hg4i0Tl3BTrIH04H+AzN7uHbzUZLDtfZhAH6Zj4iUKnwbT5IAHgCw18y+PaPpMQC3Arin9vnR+HDmptds0q9Z9NIVUd+pvfvc9sV7/JLGxQ8UpzuO/vE1bt/h/zjhtvN8cRkoANigv930uZWDhW3VHj+1Vjnul98eOrnMbT885bdf/PPiXNDSnz7r9p0MzkuUqnWXXI76VqJ9k/1S7ej56KXXbMp/Lrrpa2e/53r+Zr8WwBcBPE9yV+22uzAd5A+RvA3AAQCfr2MsESlJGOxm9jMU/7q4obnTEZFW0eWyIplQsItkQsEukgkFu0gmFOwimWhviasB5pUlBtvcen3dkkHUkfcMeLnNi+8bdftWvaV/AXf5XwAY23C12z7/X3YWts0J8slDUU7Xy1Unis5LeGyv1BP+kst+rhppyzkjfj56ef5wbg3SK7tIJhTsIplQsItkQsEukgkFu0gmFOwimVCwi2SizUtJB3XCwfa/Xm7TW2YaQLhVdFS/7OXp2ePXhKfmdOf9aLvb7mXpo8cV5bKj2mpUozy98xQLri8IlxaPlpJOEOXRw7mH4ydc9+Fdj6Itm0VEwS6SCQW7SCYU7CKZULCLZELBLpIJBbtIJtpfz+7lbS3IPQa58hThWt3emvUT/nbRUR4+FOSb/a7+73NGpzQljw7/vIY139HWxdXWnffwvARSrgEIz4t74OImvbKLZELBLpIJBbtIJhTsIplQsItkQsEukgkFu0gm6tmffSWA7wO4GEAVwCYz+w7JuwF8GcCbtbveZWaPp0wmJe8a5rqjfHCQjw7ruhNEcw+vL2jlGuTRWv7RefF+ZlEuOjWP7pyX+NjR40pbJwDOzyWspfeuP3AeVj3Z+0kAXzeznSTnAthBcmut7V4z+/s6xhCRktWzP/sRAEdqX58muRfA8lZPTESa6wP9zU7yMgCfALCtdtPtJJ8juZnkwoI+G0mOkhydwIW02YpIw+oOdpKDAH4M4A4zGwNwH4BVANZg+pX/W7P1M7NNZjZiZiM96EufsYg0pK5gJ9mD6UD/gZk9DABmdtTMpsysCuB+AGtbN00RSRUGO0kCeADAXjP79ozbh2fc7RYAu5s/PRFplnr+G38tgC8CeJ7krtptdwHYQHINpv/Zvx/AV+o6YkIqxis7TCm1nB4gKgVtXXltKCpxjUpBvaGTt00Olnv2znt0zqOfaZSybHDJZSBO68VLTQfnxVtKOkrrecuDO4et57/xPwMw2+hJOXURaS9dQSeSCQW7SCYU7CKZULCLZELBLpIJBbtIJtq7lDSQtiyym7Jt8dbE7uBBeWyYy068RsArxwyuD4iuH0jdstkrz41+Zsl5+IRttiPJz6eWnZfivnplF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTNCCut6mHox8E8BrM25aAuB42ybwwXTq3Dp1XoDm1qhmzu1SM1s6W0Nbg/19BydHzWyktAk4OnVunTovQHNrVLvmprfxIplQsItkouxg31Ty8T2dOrdOnReguTWqLXMr9W92EWmfsl/ZRaRNFOwimSgl2EmuI/kCyX0k7yxjDkVI7if5PMldJEdLnstmksdI7p5x2yKSW0m+VPs86x57Jc3tbpKHauduF8mbSprbSpJPkdxLcg/Jr9VuL/XcOfNqy3lr+9/sJLsAvAjgdwEcBLAdwAYz+++2TqQAyf0ARsys9AswSP4WgDMAvm9mv1K77ZsATpjZPbVflAvN7C86ZG53AzhT9jbetd2KhmduMw7gZgBfQonnzpnX76MN562MV/a1APaZ2StmNg7ghwDWlzCPjmdmTwM48Z6b1wPYUvt6C6afLG1XMLeOYGZHzGxn7evTAN7dZrzUc+fMqy3KCPblAF6f8f1BdNZ+7wbgCZI7SG4sezKzWGZmR4DpJw+AoZLn817hNt7t9J5txjvm3DWy/XmqMoJ9tkWyOin/d62ZfRLAjQC+Wnu7KvWpaxvvdpllm/GO0Oj256nKCPaDAFbO+H4FgMMlzGNWZna49vkYgEfQeVtRH313B93a52Mlz+f/dNI23rNtM44OOHdlbn9eRrBvB7Ca5OUkewF8AcBjJczjfUgO1P5xApIDAD6DztuK+jEAt9a+vhXAoyXO5Rd0yjbeRduMo+RzV/r252bW9g8AN2H6P/IvA/jLMuZQMK8rADxb+9hT9twAPIjpt3UTmH5HdBuAxQCeBPBS7fOiDprbPwF4HsBzmA6s4ZLmdh2m/zR8DsCu2sdNZZ87Z15tOW+6XFYkE7qCTiQTCnaRTCjYRTKhYBfJhIJdJBMKdpFMKNhFMvG/kr0K5rmdSfAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=5\n",
    "df = pd.read_csv(\"preprocessing_150.csv\",index_col=[0])\n",
    "df2 = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "plt.imshow(df.values[k,2:].reshape(28,28).astype('float32'))\n",
    "plt.show()\n",
    "df2 = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "plt.imshow(df2.values[k,2:].reshape(28,28).astype('float32'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHKAVNiD-Mkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7nEeGDn-Mkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc_-Flz2-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wImlSOL--Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj2wn75b-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7YkDa_J-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6a0jyE7-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jYY0jU-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4NiWL-c-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzhsBOPW-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDPP4Lp2-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wsmnbLS-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4vmV_gu-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxBZ_-66-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBoKPs_E-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwaS-Ang-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5_Acvtq-Mkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7pmh6Ej-Mkk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
