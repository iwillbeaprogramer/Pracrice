{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ooCpvRmT-MkR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dropout,Input,Activation,Dense,GlobalAveragePooling2D,Add,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GysUTZTp-MkX"
   },
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembles(index):\n",
    "    result = []\n",
    "    for i in range(1,index+1):\n",
    "        a = pd.read_csv(\"./AI_models/loss_kfold_{}.csv\".format(i),index_col=0)\n",
    "        b = pd.read_csv(\"./AI_models/accuracy_kfold_{}.csv\".format(i),index_col=0)\n",
    "        result.append(a.values)\n",
    "        result.append(b.values)\n",
    "    result = np.concatenate(result,axis=1)\n",
    "    ### 합쳐놓음\n",
    "    \n",
    "    mode_list=[]\n",
    "    for j in range(len(result)):\n",
    "        count_list=[0,0,0,0,0,0,0,0,0,0]\n",
    "        k = list(result[j])\n",
    "        for n in k:\n",
    "            count_list[n]+=1\n",
    "        mode = count_list.index(max(count_list))\n",
    "        mode_list.append(mode)\n",
    "    return np.array(mode_list).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s4kX312e-MkY"
   },
   "outputs": [],
   "source": [
    "def modeling():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = inputs\n",
    "    _x = Conv2D(64,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = _x\n",
    "    _x = Conv2D(256,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(256,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(256,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(256,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = Dense(10,activation='softmax')(x)\n",
    "    outputs=x\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cccyJfWO-MkY"
   },
   "source": [
    "# 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W73nGvZM-MkZ"
   },
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "es = EarlyStopping(monitor='val_loss',patience=100)\n",
    "reLR = ReduceLROnPlateau(patience=40,verbose=1,factor=0.5)\n",
    "kfold = StratifiedKFold(n_splits=10,shuffle=True)\n",
    "datagen = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "datagen2 = ImageDataGenerator()\n",
    "optimizer = Adam(lr=0.002,epsilon=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWqLv-B1-MkZ"
   },
   "source": [
    "# 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j6UbQKZk-MkZ",
    "outputId": "26984b86-65c4-4adf-c0dc-bcb09f17d346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 28, 28, 1) (2048,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 9, 0, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "y = df.values[:,0].astype('int32')\n",
    "x = df.values[:,2:].astype('float32')/255.0\n",
    "# print(x.shape,y.shape)               # (2048, 28, 28) (2048,)\n",
    "#onehot = OneHotEncoder()\n",
    "#y = onehot.fit_transform(y.reshape(-1,1)).toarray().astype('float32')\n",
    "x = x.reshape(-1,28,28,1)\n",
    "# x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.15)\n",
    "# x_train = x_train.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# x_val = x_val.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# print(x_train.shape,x_val.shape,y_train.shape,y_val.shape)\n",
    "print(x.shape,y.shape) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "LmMESLTT-Mkb",
    "outputId": "6e97efa1-d705-4b3d-9e71-4261745da5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 2/58 [>.............................] - ETA: 9s - loss: 36.3661 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1276s vs `on_train_batch_end` time: 0.2212s). Check your callbacks.\n",
      "58/58 [==============================] - ETA: 0s - loss: 16.6359 - accuracy: 0.1058\n",
      "Epoch 00001: val_loss improved from inf to 13455.08008, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.09268, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 25s 426ms/step - loss: 16.6359 - accuracy: 0.1058 - val_loss: 13455.0801 - val_accuracy: 0.0927\n",
      "Epoch 2/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.5425 - accuracy: 0.1042\n",
      "Epoch 00002: val_loss improved from 13455.08008 to 14.67144, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.09268 to 0.09756, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 25s 427ms/step - loss: 2.5425 - accuracy: 0.1042 - val_loss: 14.6714 - val_accuracy: 0.0976\n",
      "Epoch 3/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.4465 - accuracy: 0.1302\n",
      "Epoch 00003: val_loss improved from 14.67144 to 2.76059, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.09756 to 0.10244, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 25s 423ms/step - loss: 2.4465 - accuracy: 0.1302 - val_loss: 2.7606 - val_accuracy: 0.1024\n",
      "Epoch 4/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.3567 - accuracy: 0.1579\n",
      "Epoch 00004: val_loss improved from 2.76059 to 2.67435, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.10244 to 0.10732, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 25s 429ms/step - loss: 2.3567 - accuracy: 0.1579 - val_loss: 2.6744 - val_accuracy: 0.1073\n",
      "Epoch 5/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.1911 - accuracy: 0.2203\n",
      "Epoch 00005: val_loss did not improve from 2.67435\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.10732\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.1911 - accuracy: 0.2203 - val_loss: 2.8796 - val_accuracy: 0.1024\n",
      "Epoch 6/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.2117 - accuracy: 0.2263\n",
      "Epoch 00006: val_loss did not improve from 2.67435\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.10732 to 0.12683, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 36s 624ms/step - loss: 2.2117 - accuracy: 0.2263 - val_loss: 3.1357 - val_accuracy: 0.1268\n",
      "Epoch 7/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0889 - accuracy: 0.2740\n",
      "Epoch 00007: val_loss did not improve from 2.67435\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.12683 to 0.15122, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 23s 388ms/step - loss: 2.0889 - accuracy: 0.2740 - val_loss: 2.8409 - val_accuracy: 0.1512\n",
      "Epoch 8/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.9703 - accuracy: 0.3299\n",
      "Epoch 00008: val_loss did not improve from 2.67435\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.15122\n",
      "58/58 [==============================] - 20s 353ms/step - loss: 1.9703 - accuracy: 0.3299 - val_loss: 2.8102 - val_accuracy: 0.1366\n",
      "Epoch 9/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.8190 - accuracy: 0.3798\n",
      "Epoch 00009: val_loss did not improve from 2.67435\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.15122 to 0.20488, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 35s 600ms/step - loss: 1.8190 - accuracy: 0.3798 - val_loss: 2.6900 - val_accuracy: 0.2049\n",
      "Epoch 10/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5776 - accuracy: 0.4693\n",
      "Epoch 00010: val_loss improved from 2.67435 to 2.08740, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.20488 to 0.29268, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 54s 923ms/step - loss: 1.5776 - accuracy: 0.4693 - val_loss: 2.0874 - val_accuracy: 0.2927\n",
      "Epoch 11/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3704 - accuracy: 0.5345\n",
      "Epoch 00011: val_loss improved from 2.08740 to 1.83349, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.29268 to 0.40000, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 25s 426ms/step - loss: 1.3704 - accuracy: 0.5345 - val_loss: 1.8335 - val_accuracy: 0.4000\n",
      "Epoch 12/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0830 - accuracy: 0.6408\n",
      "Epoch 00012: val_loss improved from 1.83349 to 1.56094, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.40000 to 0.52195, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 39s 679ms/step - loss: 1.0830 - accuracy: 0.6408 - val_loss: 1.5609 - val_accuracy: 0.5220\n",
      "Epoch 13/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.9586 - accuracy: 0.6755\n",
      "Epoch 00013: val_loss did not improve from 1.56094\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.52195\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.9586 - accuracy: 0.6755 - val_loss: 3.6787 - val_accuracy: 0.3415\n",
      "Epoch 14/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.8486 - accuracy: 0.7211\n",
      "Epoch 00014: val_loss improved from 1.56094 to 0.94231, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.52195 to 0.63902, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 38s 650ms/step - loss: 0.8486 - accuracy: 0.7211 - val_loss: 0.9423 - val_accuracy: 0.6390\n",
      "Epoch 15/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.8009 - accuracy: 0.7417\n",
      "Epoch 00015: val_loss did not improve from 0.94231\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.63902\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.8009 - accuracy: 0.7417 - val_loss: 1.7532 - val_accuracy: 0.5902\n",
      "Epoch 16/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.6464 - accuracy: 0.7889\n",
      "Epoch 00016: val_loss did not improve from 0.94231\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.63902\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.6464 - accuracy: 0.7889 - val_loss: 1.9831 - val_accuracy: 0.5756\n",
      "Epoch 17/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5812 - accuracy: 0.8101\n",
      "Epoch 00017: val_loss did not improve from 0.94231\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.63902\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.5812 - accuracy: 0.8101 - val_loss: 2.1013 - val_accuracy: 0.5659\n",
      "Epoch 18/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4383 - accuracy: 0.8573\n",
      "Epoch 00018: val_loss did not improve from 0.94231\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.63902 to 0.68780, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 36s 620ms/step - loss: 0.4383 - accuracy: 0.8573 - val_loss: 1.0189 - val_accuracy: 0.6878\n",
      "Epoch 19/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3978 - accuracy: 0.8638\n",
      "Epoch 00019: val_loss did not improve from 0.94231\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.68780 to 0.70732, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 23s 389ms/step - loss: 0.3978 - accuracy: 0.8638 - val_loss: 0.9970 - val_accuracy: 0.7073\n",
      "Epoch 20/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.8844\n",
      "Epoch 00020: val_loss did not improve from 0.94231\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.70732\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.3532 - accuracy: 0.8844 - val_loss: 2.4334 - val_accuracy: 0.5805\n",
      "Epoch 21/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.9007\n",
      "Epoch 00021: val_loss did not improve from 0.94231\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.70732 to 0.75122, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 35s 604ms/step - loss: 0.2998 - accuracy: 0.9007 - val_loss: 1.0341 - val_accuracy: 0.7512\n",
      "Epoch 22/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.9045\n",
      "Epoch 00022: val_loss improved from 0.94231 to 0.84511, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.75122 to 0.80488, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 52s 890ms/step - loss: 0.2886 - accuracy: 0.9045 - val_loss: 0.8451 - val_accuracy: 0.8049\n",
      "Epoch 23/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.9067\n",
      "Epoch 00023: val_loss did not improve from 0.84511\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2678 - accuracy: 0.9067 - val_loss: 1.9417 - val_accuracy: 0.5317\n",
      "Epoch 24/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.9208\n",
      "Epoch 00024: val_loss did not improve from 0.84511\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.2338 - accuracy: 0.9208 - val_loss: 1.0766 - val_accuracy: 0.7366\n",
      "Epoch 25/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.9343\n",
      "Epoch 00025: val_loss did not improve from 0.84511\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2071 - accuracy: 0.9343 - val_loss: 1.4895 - val_accuracy: 0.6732\n",
      "Epoch 26/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9159\n",
      "Epoch 00026: val_loss did not improve from 0.84511\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2462 - accuracy: 0.9159 - val_loss: 1.0804 - val_accuracy: 0.7805\n",
      "Epoch 27/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9284\n",
      "Epoch 00027: val_loss did not improve from 0.84511\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 21s 354ms/step - loss: 0.1993 - accuracy: 0.9284 - val_loss: 1.0008 - val_accuracy: 0.7317\n",
      "Epoch 28/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9414\n",
      "Epoch 00028: val_loss did not improve from 0.84511\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 353ms/step - loss: 0.1665 - accuracy: 0.9414 - val_loss: 1.2519 - val_accuracy: 0.6976\n",
      "Epoch 29/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9463\n",
      "Epoch 00029: val_loss did not improve from 0.84511\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.1395 - accuracy: 0.9463 - val_loss: 2.6211 - val_accuracy: 0.5024\n",
      "Epoch 30/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9430\n",
      "Epoch 00030: val_loss did not improve from 0.84511\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1707 - accuracy: 0.9430 - val_loss: 1.8066 - val_accuracy: 0.6683\n",
      "Epoch 31/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9425\n",
      "Epoch 00031: val_loss improved from 0.84511 to 0.82352, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 36s 624ms/step - loss: 0.1659 - accuracy: 0.9425 - val_loss: 0.8235 - val_accuracy: 0.7902\n",
      "Epoch 32/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9560\n",
      "Epoch 00032: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1208 - accuracy: 0.9560 - val_loss: 1.0274 - val_accuracy: 0.7122\n",
      "Epoch 33/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9409\n",
      "Epoch 00033: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1664 - accuracy: 0.9409 - val_loss: 1.0947 - val_accuracy: 0.8049\n",
      "Epoch 34/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9501\n",
      "Epoch 00034: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1467 - accuracy: 0.9501 - val_loss: 1.2787 - val_accuracy: 0.7756\n",
      "Epoch 35/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9490\n",
      "Epoch 00035: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1675 - accuracy: 0.9490 - val_loss: 1.3373 - val_accuracy: 0.8049\n",
      "Epoch 36/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9571\n",
      "Epoch 00036: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1336 - accuracy: 0.9571 - val_loss: 1.0610 - val_accuracy: 0.7951\n",
      "Epoch 37/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9615\n",
      "Epoch 00037: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1086 - accuracy: 0.9615 - val_loss: 1.9707 - val_accuracy: 0.6927\n",
      "Epoch 38/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9609\n",
      "Epoch 00038: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1190 - accuracy: 0.9609 - val_loss: 1.2456 - val_accuracy: 0.7317\n",
      "Epoch 39/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9734\n",
      "Epoch 00039: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0914 - accuracy: 0.9734 - val_loss: 2.1399 - val_accuracy: 0.6000\n",
      "Epoch 40/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9528\n",
      "Epoch 00040: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.1454 - accuracy: 0.9528 - val_loss: 4.4981 - val_accuracy: 0.4683\n",
      "Epoch 41/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9571\n",
      "Epoch 00041: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1466 - accuracy: 0.9571 - val_loss: 1.9883 - val_accuracy: 0.6488\n",
      "Epoch 42/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.9523\n",
      "Epoch 00042: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1681 - accuracy: 0.9523 - val_loss: 2.5262 - val_accuracy: 0.5707\n",
      "Epoch 43/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9604\n",
      "Epoch 00043: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1166 - accuracy: 0.9604 - val_loss: 1.4650 - val_accuracy: 0.6683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9685\n",
      "Epoch 00044: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0965 - accuracy: 0.9685 - val_loss: 1.1148 - val_accuracy: 0.7707\n",
      "Epoch 45/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9664\n",
      "Epoch 00045: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1025 - accuracy: 0.9664 - val_loss: 0.8783 - val_accuracy: 0.7951\n",
      "Epoch 46/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.9230\n",
      "Epoch 00046: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2538 - accuracy: 0.9230 - val_loss: 2.7090 - val_accuracy: 0.5951\n",
      "Epoch 47/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1539 - accuracy: 0.9517\n",
      "Epoch 00047: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1539 - accuracy: 0.9517 - val_loss: 2.8742 - val_accuracy: 0.6146\n",
      "Epoch 48/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9371\n",
      "Epoch 00048: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2159 - accuracy: 0.9371 - val_loss: 3.6974 - val_accuracy: 0.5366\n",
      "Epoch 49/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9479\n",
      "Epoch 00049: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1850 - accuracy: 0.9479 - val_loss: 3.2554 - val_accuracy: 0.5073\n",
      "Epoch 50/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.9523\n",
      "Epoch 00050: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1463 - accuracy: 0.9523 - val_loss: 1.8307 - val_accuracy: 0.6780\n",
      "Epoch 51/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9485\n",
      "Epoch 00051: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1813 - accuracy: 0.9485 - val_loss: 3.4867 - val_accuracy: 0.5707\n",
      "Epoch 52/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9609\n",
      "Epoch 00052: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.80488\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1275 - accuracy: 0.9609 - val_loss: 3.1319 - val_accuracy: 0.6000\n",
      "Epoch 53/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9588\n",
      "Epoch 00053: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00053: val_accuracy improved from 0.80488 to 0.80976, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 35s 611ms/step - loss: 0.1372 - accuracy: 0.9588 - val_loss: 1.4487 - val_accuracy: 0.8098\n",
      "Epoch 54/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9826\n",
      "Epoch 00054: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.80976\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0604 - accuracy: 0.9826 - val_loss: 1.6518 - val_accuracy: 0.7659\n",
      "Epoch 55/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9479\n",
      "Epoch 00055: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.80976\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2002 - accuracy: 0.9479 - val_loss: 1.5491 - val_accuracy: 0.7707\n",
      "Epoch 56/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9680\n",
      "Epoch 00056: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.80976\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1013 - accuracy: 0.9680 - val_loss: 1.3035 - val_accuracy: 0.7951\n",
      "Epoch 57/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9544\n",
      "Epoch 00057: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.80976\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1626 - accuracy: 0.9544 - val_loss: 1.9977 - val_accuracy: 0.7122\n",
      "Epoch 58/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9653\n",
      "Epoch 00058: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00058: val_accuracy improved from 0.80976 to 0.84878, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 35s 601ms/step - loss: 0.1124 - accuracy: 0.9653 - val_loss: 1.0788 - val_accuracy: 0.8488\n",
      "Epoch 59/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9740\n",
      "Epoch 00059: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0897 - accuracy: 0.9740 - val_loss: 1.0291 - val_accuracy: 0.8293\n",
      "Epoch 60/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9788\n",
      "Epoch 00060: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0650 - accuracy: 0.9788 - val_loss: 1.4021 - val_accuracy: 0.8098\n",
      "Epoch 61/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9794\n",
      "Epoch 00061: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0688 - accuracy: 0.9794 - val_loss: 1.3620 - val_accuracy: 0.7805\n",
      "Epoch 62/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9745\n",
      "Epoch 00062: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.0784 - accuracy: 0.9745 - val_loss: 1.1577 - val_accuracy: 0.8293\n",
      "Epoch 63/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9859\n",
      "Epoch 00063: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0408 - accuracy: 0.9859 - val_loss: 1.0873 - val_accuracy: 0.8488\n",
      "Epoch 64/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9799\n",
      "Epoch 00064: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0658 - accuracy: 0.9799 - val_loss: 1.5128 - val_accuracy: 0.8146\n",
      "Epoch 65/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.9577\n",
      "Epoch 00065: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1607 - accuracy: 0.9577 - val_loss: 3.5625 - val_accuracy: 0.5317\n",
      "Epoch 66/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.9441\n",
      "Epoch 00066: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.2507 - accuracy: 0.9441 - val_loss: 1.1320 - val_accuracy: 0.8098\n",
      "Epoch 67/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.9289\n",
      "Epoch 00067: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.3126 - accuracy: 0.9289 - val_loss: 2.1686 - val_accuracy: 0.7561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.9338\n",
      "Epoch 00068: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.2912 - accuracy: 0.9338 - val_loss: 15.6375 - val_accuracy: 0.4829\n",
      "Epoch 69/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.9159\n",
      "Epoch 00069: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.4156 - accuracy: 0.9159 - val_loss: 34.9300 - val_accuracy: 0.1317\n",
      "Epoch 70/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9479\n",
      "Epoch 00070: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1876 - accuracy: 0.9479 - val_loss: 7.6125 - val_accuracy: 0.4049\n",
      "Epoch 71/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9707\n",
      "Epoch 00071: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "\n",
      "Epoch 00071: val_accuracy improved from 0.84878 to 0.85366, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 35s 610ms/step - loss: 0.0945 - accuracy: 0.9707 - val_loss: 1.0513 - val_accuracy: 0.8537\n",
      "Epoch 72/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9908\n",
      "Epoch 00072: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.85366\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0272 - accuracy: 0.9908 - val_loss: 1.0192 - val_accuracy: 0.8537\n",
      "Epoch 73/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9984\n",
      "Epoch 00073: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.85366\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.0091 - accuracy: 0.9984 - val_loss: 1.1037 - val_accuracy: 0.8537\n",
      "Epoch 74/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9989\n",
      "Epoch 00074: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.85366\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0022 - accuracy: 0.9989 - val_loss: 1.0468 - val_accuracy: 0.8390\n",
      "Epoch 75/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9689e-04 - accuracy: 1.0000\n",
      "Epoch 00075: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.85366\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 6.9689e-04 - accuracy: 1.0000 - val_loss: 1.0820 - val_accuracy: 0.8439\n",
      "Epoch 76/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 00076: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.85366\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 1.0339 - val_accuracy: 0.8537\n",
      "Epoch 77/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9989\n",
      "Epoch 00077: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.85366\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.8243 - val_accuracy: 0.8488\n",
      "Epoch 78/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00078: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00078: val_accuracy improved from 0.85366 to 0.86341, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 36s 624ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.9583 - val_accuracy: 0.8634\n",
      "Epoch 79/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.7708e-04 - accuracy: 1.0000\n",
      "Epoch 00079: val_loss did not improve from 0.82352\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.86341\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 4.7708e-04 - accuracy: 1.0000 - val_loss: 0.8955 - val_accuracy: 0.8634\n",
      "Epoch 80/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.1440e-04 - accuracy: 1.0000\n",
      "Epoch 00080: val_loss improved from 0.82352 to 0.82083, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00080: val_accuracy improved from 0.86341 to 0.86829, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 51s 883ms/step - loss: 5.1440e-04 - accuracy: 1.0000 - val_loss: 0.8208 - val_accuracy: 0.8683\n",
      "Epoch 81/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0441e-04 - accuracy: 1.0000\n",
      "Epoch 00081: val_loss did not improve from 0.82083\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.0441e-04 - accuracy: 1.0000 - val_loss: 1.0183 - val_accuracy: 0.8585\n",
      "Epoch 82/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.3114e-04 - accuracy: 1.0000\n",
      "Epoch 00082: val_loss did not improve from 0.82083\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 2.3114e-04 - accuracy: 1.0000 - val_loss: 1.0688 - val_accuracy: 0.8634\n",
      "Epoch 83/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5657e-04 - accuracy: 1.0000\n",
      "Epoch 00083: val_loss did not improve from 0.82083\n",
      "\n",
      "Epoch 00083: val_accuracy improved from 0.86829 to 0.87805, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 35s 606ms/step - loss: 1.5657e-04 - accuracy: 1.0000 - val_loss: 0.9766 - val_accuracy: 0.8780\n",
      "Epoch 84/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0659e-04 - accuracy: 1.0000\n",
      "Epoch 00084: val_loss did not improve from 0.82083\n",
      "\n",
      "Epoch 00084: val_accuracy improved from 0.87805 to 0.88293, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 22s 388ms/step - loss: 2.0659e-04 - accuracy: 1.0000 - val_loss: 0.8666 - val_accuracy: 0.8829\n",
      "Epoch 85/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.6717e-04 - accuracy: 1.0000\n",
      "Epoch 00085: val_loss did not improve from 0.82083\n",
      "\n",
      "Epoch 00085: val_accuracy improved from 0.88293 to 0.88780, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 22s 388ms/step - loss: 1.6717e-04 - accuracy: 1.0000 - val_loss: 0.8691 - val_accuracy: 0.8878\n",
      "Epoch 86/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0052e-04 - accuracy: 1.0000\n",
      "Epoch 00086: val_loss did not improve from 0.82083\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.0052e-04 - accuracy: 1.0000 - val_loss: 0.8650 - val_accuracy: 0.8780\n",
      "Epoch 87/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.4656e-04 - accuracy: 1.0000\n",
      "Epoch 00087: val_loss did not improve from 0.82083\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.4656e-04 - accuracy: 1.0000 - val_loss: 1.0629 - val_accuracy: 0.8537\n",
      "Epoch 88/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7809e-04 - accuracy: 1.0000\n",
      "Epoch 00088: val_loss improved from 0.82083 to 0.80143, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 36s 621ms/step - loss: 1.7809e-04 - accuracy: 1.0000 - val_loss: 0.8014 - val_accuracy: 0.8780\n",
      "Epoch 89/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1353e-04 - accuracy: 1.0000\n",
      "Epoch 00089: val_loss did not improve from 0.80143\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.1353e-04 - accuracy: 1.0000 - val_loss: 0.8872 - val_accuracy: 0.8732\n",
      "Epoch 90/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0151e-04 - accuracy: 1.0000\n",
      "Epoch 00090: val_loss did not improve from 0.80143\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.0151e-04 - accuracy: 1.0000 - val_loss: 0.8029 - val_accuracy: 0.8829\n",
      "Epoch 91/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1019e-04 - accuracy: 1.0000\n",
      "Epoch 00091: val_loss improved from 0.80143 to 0.79793, saving model to ./home_models_0205\\02_04_AI_val_loss_index_5.h5\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 35s 607ms/step - loss: 1.1019e-04 - accuracy: 1.0000 - val_loss: 0.7979 - val_accuracy: 0.8780\n",
      "Epoch 92/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1389e-04 - accuracy: 1.0000\n",
      "Epoch 00092: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.1389e-04 - accuracy: 1.0000 - val_loss: 0.9604 - val_accuracy: 0.8683\n",
      "Epoch 93/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3349e-04 - accuracy: 1.0000\n",
      "Epoch 00093: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.3349e-04 - accuracy: 1.0000 - val_loss: 0.9293 - val_accuracy: 0.8634\n",
      "Epoch 94/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5386e-04 - accuracy: 1.0000\n",
      "Epoch 00094: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.5386e-04 - accuracy: 1.0000 - val_loss: 0.8699 - val_accuracy: 0.8829\n",
      "Epoch 95/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1018e-04 - accuracy: 1.0000\n",
      "Epoch 00095: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.1018e-04 - accuracy: 1.0000 - val_loss: 1.0659 - val_accuracy: 0.8732\n",
      "Epoch 96/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 9.6912e-05 - accuracy: 1.0000\n",
      "Epoch 00096: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 9.6912e-05 - accuracy: 1.0000 - val_loss: 0.9549 - val_accuracy: 0.8732\n",
      "Epoch 97/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 8.1118e-05 - accuracy: 1.0000\n",
      "Epoch 00097: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 8.1118e-05 - accuracy: 1.0000 - val_loss: 1.0297 - val_accuracy: 0.8585\n",
      "Epoch 98/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0424e-04 - accuracy: 1.0000\n",
      "Epoch 00098: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.0424e-04 - accuracy: 1.0000 - val_loss: 0.9799 - val_accuracy: 0.8780\n",
      "Epoch 99/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.5601e-05 - accuracy: 1.0000\n",
      "Epoch 00099: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 7.5601e-05 - accuracy: 1.0000 - val_loss: 0.9287 - val_accuracy: 0.8634\n",
      "Epoch 100/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 8.9427e-05 - accuracy: 1.0000\n",
      "Epoch 00100: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 8.9427e-05 - accuracy: 1.0000 - val_loss: 0.9054 - val_accuracy: 0.8829\n",
      "Epoch 101/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 8.0417e-05 - accuracy: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 8.0417e-05 - accuracy: 1.0000 - val_loss: 0.9831 - val_accuracy: 0.8780\n",
      "Epoch 102/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.7166e-05 - accuracy: 1.0000\n",
      "Epoch 00102: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 7.7166e-05 - accuracy: 1.0000 - val_loss: 0.9938 - val_accuracy: 0.8683\n",
      "Epoch 103/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0063e-05 - accuracy: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 7.0063e-05 - accuracy: 1.0000 - val_loss: 0.8160 - val_accuracy: 0.8829\n",
      "Epoch 104/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8685e-05 - accuracy: 1.0000\n",
      "Epoch 00104: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 6.8685e-05 - accuracy: 1.0000 - val_loss: 0.9508 - val_accuracy: 0.8683\n",
      "Epoch 105/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.9170e-05 - accuracy: 1.0000\n",
      "Epoch 00105: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 5.9170e-05 - accuracy: 1.0000 - val_loss: 0.9402 - val_accuracy: 0.8878\n",
      "Epoch 106/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.6742e-05 - accuracy: 1.0000\n",
      "Epoch 00106: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 4.6742e-05 - accuracy: 1.0000 - val_loss: 0.9354 - val_accuracy: 0.8732\n",
      "Epoch 107/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.7085e-05 - accuracy: 1.0000\n",
      "Epoch 00107: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 5.7085e-05 - accuracy: 1.0000 - val_loss: 1.0056 - val_accuracy: 0.8634\n",
      "Epoch 108/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.8978e-05 - accuracy: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 7.8978e-05 - accuracy: 1.0000 - val_loss: 0.9814 - val_accuracy: 0.8683\n",
      "Epoch 109/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.2598e-05 - accuracy: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 6.2598e-05 - accuracy: 1.0000 - val_loss: 1.1228 - val_accuracy: 0.8634\n",
      "Epoch 110/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.8521e-05 - accuracy: 1.0000\n",
      "Epoch 00110: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 3.8521e-05 - accuracy: 1.0000 - val_loss: 0.9064 - val_accuracy: 0.8780\n",
      "Epoch 111/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.0830e-05 - accuracy: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 5.0830e-05 - accuracy: 1.0000 - val_loss: 0.9732 - val_accuracy: 0.8634\n",
      "Epoch 112/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.0054e-05 - accuracy: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 5.0054e-05 - accuracy: 1.0000 - val_loss: 1.1037 - val_accuracy: 0.8634\n",
      "Epoch 113/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.1833e-05 - accuracy: 1.0000\n",
      "Epoch 00113: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 5.1833e-05 - accuracy: 1.0000 - val_loss: 0.9918 - val_accuracy: 0.8683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.8611e-05 - accuracy: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 4.8611e-05 - accuracy: 1.0000 - val_loss: 0.9719 - val_accuracy: 0.8634\n",
      "Epoch 115/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.8255e-05 - accuracy: 1.0000\n",
      "Epoch 00115: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.8255e-05 - accuracy: 1.0000 - val_loss: 0.9278 - val_accuracy: 0.8732\n",
      "Epoch 116/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.4514e-05 - accuracy: 1.0000\n",
      "Epoch 00116: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 4.4514e-05 - accuracy: 1.0000 - val_loss: 1.0092 - val_accuracy: 0.8537\n",
      "Epoch 117/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.9092e-05 - accuracy: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 7.9092e-05 - accuracy: 1.0000 - val_loss: 0.9913 - val_accuracy: 0.8488\n",
      "Epoch 118/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.9264e-05 - accuracy: 1.0000\n",
      "Epoch 00118: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.9264e-05 - accuracy: 1.0000 - val_loss: 0.9677 - val_accuracy: 0.8780\n",
      "Epoch 119/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.2808e-05 - accuracy: 1.0000\n",
      "Epoch 00119: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.2808e-05 - accuracy: 1.0000 - val_loss: 0.8926 - val_accuracy: 0.8829\n",
      "Epoch 120/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.4140e-05 - accuracy: 1.0000\n",
      "Epoch 00120: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 5.4140e-05 - accuracy: 1.0000 - val_loss: 1.0552 - val_accuracy: 0.8585\n",
      "Epoch 121/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.4014e-05 - accuracy: 1.0000\n",
      "Epoch 00121: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 5.4014e-05 - accuracy: 1.0000 - val_loss: 0.9532 - val_accuracy: 0.8732\n",
      "Epoch 122/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.7638e-05 - accuracy: 1.0000\n",
      "Epoch 00122: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 4.7638e-05 - accuracy: 1.0000 - val_loss: 0.9441 - val_accuracy: 0.8732\n",
      "Epoch 123/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.8121e-05 - accuracy: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.8121e-05 - accuracy: 1.0000 - val_loss: 0.8806 - val_accuracy: 0.8780\n",
      "Epoch 124/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.2929e-05 - accuracy: 1.0000\n",
      "Epoch 00124: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 4.2929e-05 - accuracy: 1.0000 - val_loss: 0.9419 - val_accuracy: 0.8683\n",
      "Epoch 125/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9028e-05 - accuracy: 1.0000\n",
      "Epoch 00125: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.9028e-05 - accuracy: 1.0000 - val_loss: 1.0091 - val_accuracy: 0.8683\n",
      "Epoch 126/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9501e-05 - accuracy: 1.0000\n",
      "Epoch 00126: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.9501e-05 - accuracy: 1.0000 - val_loss: 0.9121 - val_accuracy: 0.8634\n",
      "Epoch 127/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.4280e-05 - accuracy: 1.0000\n",
      "Epoch 00127: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 3.4280e-05 - accuracy: 1.0000 - val_loss: 0.9655 - val_accuracy: 0.8732\n",
      "Epoch 128/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.1310e-05 - accuracy: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.1310e-05 - accuracy: 1.0000 - val_loss: 1.0157 - val_accuracy: 0.8732\n",
      "Epoch 129/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.6042e-05 - accuracy: 1.0000\n",
      "Epoch 00129: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.6042e-05 - accuracy: 1.0000 - val_loss: 1.0668 - val_accuracy: 0.8780\n",
      "Epoch 130/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.3781e-05 - accuracy: 1.0000\n",
      "Epoch 00130: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.3781e-05 - accuracy: 1.0000 - val_loss: 0.8376 - val_accuracy: 0.8780\n",
      "Epoch 131/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.5431e-05 - accuracy: 1.0000\n",
      "Epoch 00131: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.5431e-05 - accuracy: 1.0000 - val_loss: 0.9220 - val_accuracy: 0.8780\n",
      "Epoch 132/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0735e-05 - accuracy: 1.0000\n",
      "Epoch 00132: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.0735e-05 - accuracy: 1.0000 - val_loss: 0.8929 - val_accuracy: 0.8683\n",
      "Epoch 133/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.1150e-05 - accuracy: 1.0000\n",
      "Epoch 00133: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 4.1150e-05 - accuracy: 1.0000 - val_loss: 0.8833 - val_accuracy: 0.8732\n",
      "Epoch 134/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.9688e-04 - accuracy: 1.0000\n",
      "Epoch 00134: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.9688e-04 - accuracy: 1.0000 - val_loss: 0.9780 - val_accuracy: 0.8683\n",
      "Epoch 135/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.6899e-05 - accuracy: 1.0000\n",
      "Epoch 00135: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.6899e-05 - accuracy: 1.0000 - val_loss: 1.0430 - val_accuracy: 0.8537\n",
      "Epoch 136/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.6529e-05 - accuracy: 1.0000\n",
      "Epoch 00136: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 4.6529e-05 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.8585\n",
      "Epoch 137/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.0659e-05 - accuracy: 1.0000\n",
      "Epoch 00137: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 3.0659e-05 - accuracy: 1.0000 - val_loss: 1.0046 - val_accuracy: 0.8683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.4107e-05 - accuracy: 1.0000\n",
      "Epoch 00138: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 2.4107e-05 - accuracy: 1.0000 - val_loss: 0.9528 - val_accuracy: 0.8732\n",
      "Epoch 139/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9096e-05 - accuracy: 1.0000\n",
      "Epoch 00139: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.9096e-05 - accuracy: 1.0000 - val_loss: 0.9485 - val_accuracy: 0.8780\n",
      "Epoch 140/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.8407e-05 - accuracy: 1.0000\n",
      "Epoch 00140: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 3.8407e-05 - accuracy: 1.0000 - val_loss: 0.9884 - val_accuracy: 0.8732\n",
      "Epoch 141/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.6266e-05 - accuracy: 1.0000\n",
      "Epoch 00141: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.6266e-05 - accuracy: 1.0000 - val_loss: 1.0293 - val_accuracy: 0.8585\n",
      "Epoch 142/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.7948e-05 - accuracy: 1.0000\n",
      "Epoch 00142: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.7948e-05 - accuracy: 1.0000 - val_loss: 1.0722 - val_accuracy: 0.8537\n",
      "Epoch 143/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7936e-05 - accuracy: 1.0000\n",
      "Epoch 00143: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.7936e-05 - accuracy: 1.0000 - val_loss: 0.8765 - val_accuracy: 0.8732\n",
      "Epoch 144/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0490e-05 - accuracy: 1.0000\n",
      "Epoch 00144: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.0490e-05 - accuracy: 1.0000 - val_loss: 1.0005 - val_accuracy: 0.8683\n",
      "Epoch 145/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.6933e-05 - accuracy: 1.0000\n",
      "Epoch 00145: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.6933e-05 - accuracy: 1.0000 - val_loss: 0.9182 - val_accuracy: 0.8780\n",
      "Epoch 146/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.3612e-05 - accuracy: 1.0000\n",
      "Epoch 00146: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00146: val_accuracy improved from 0.88780 to 0.89268, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 36s 628ms/step - loss: 2.3612e-05 - accuracy: 1.0000 - val_loss: 0.8558 - val_accuracy: 0.8927\n",
      "Epoch 147/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5701e-05 - accuracy: 1.0000\n",
      "Epoch 00147: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.5701e-05 - accuracy: 1.0000 - val_loss: 1.0690 - val_accuracy: 0.8488\n",
      "Epoch 148/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7397e-05 - accuracy: 1.0000\n",
      "Epoch 00148: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.7397e-05 - accuracy: 1.0000 - val_loss: 0.9871 - val_accuracy: 0.8732\n",
      "Epoch 149/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5106e-05 - accuracy: 1.0000\n",
      "Epoch 00149: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.5106e-05 - accuracy: 1.0000 - val_loss: 1.0210 - val_accuracy: 0.8780\n",
      "Epoch 150/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.8706e-05 - accuracy: 1.0000\n",
      "Epoch 00150: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.8706e-05 - accuracy: 1.0000 - val_loss: 0.9316 - val_accuracy: 0.8927\n",
      "Epoch 151/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.9191e-05 - accuracy: 1.0000\n",
      "Epoch 00151: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.9191e-05 - accuracy: 1.0000 - val_loss: 1.0156 - val_accuracy: 0.8732\n",
      "Epoch 152/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.1832e-05 - accuracy: 1.0000\n",
      "Epoch 00152: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 3.1832e-05 - accuracy: 1.0000 - val_loss: 1.0447 - val_accuracy: 0.8829\n",
      "Epoch 153/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5531e-05 - accuracy: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.5531e-05 - accuracy: 1.0000 - val_loss: 0.8213 - val_accuracy: 0.8878\n",
      "Epoch 154/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.6862e-05 - accuracy: 1.0000\n",
      "Epoch 00154: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.6862e-05 - accuracy: 1.0000 - val_loss: 0.9402 - val_accuracy: 0.8585\n",
      "Epoch 155/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0521e-05 - accuracy: 1.0000\n",
      "Epoch 00155: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.0521e-05 - accuracy: 1.0000 - val_loss: 0.8363 - val_accuracy: 0.8829\n",
      "Epoch 156/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.2127e-05 - accuracy: 1.0000\n",
      "Epoch 00156: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.2127e-05 - accuracy: 1.0000 - val_loss: 0.9895 - val_accuracy: 0.8732\n",
      "Epoch 157/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7907e-05 - accuracy: 1.0000\n",
      "Epoch 00157: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.7907e-05 - accuracy: 1.0000 - val_loss: 1.0663 - val_accuracy: 0.8732\n",
      "Epoch 158/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.2454e-05 - accuracy: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.2454e-05 - accuracy: 1.0000 - val_loss: 1.0010 - val_accuracy: 0.8878\n",
      "Epoch 159/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3836e-05 - accuracy: 1.0000\n",
      "Epoch 00159: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.3836e-05 - accuracy: 1.0000 - val_loss: 1.0085 - val_accuracy: 0.8780\n",
      "Epoch 160/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.6775e-05 - accuracy: 1.0000\n",
      "Epoch 00160: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.6775e-05 - accuracy: 1.0000 - val_loss: 0.9324 - val_accuracy: 0.8878\n",
      "Epoch 161/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1876e-05 - accuracy: 1.0000\n",
      "Epoch 00161: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.1876e-05 - accuracy: 1.0000 - val_loss: 0.9636 - val_accuracy: 0.8878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3597e-05 - accuracy: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.3597e-05 - accuracy: 1.0000 - val_loss: 0.9023 - val_accuracy: 0.8780\n",
      "Epoch 163/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3960e-05 - accuracy: 1.0000\n",
      "Epoch 00163: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.3960e-05 - accuracy: 1.0000 - val_loss: 1.0239 - val_accuracy: 0.8829\n",
      "Epoch 164/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.4001e-05 - accuracy: 1.0000\n",
      "Epoch 00164: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.4001e-05 - accuracy: 1.0000 - val_loss: 1.0064 - val_accuracy: 0.8780\n",
      "Epoch 165/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0555e-05 - accuracy: 1.0000\n",
      "Epoch 00165: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.0555e-05 - accuracy: 1.0000 - val_loss: 0.9209 - val_accuracy: 0.8732\n",
      "Epoch 166/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2102e-05 - accuracy: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 1.2102e-05 - accuracy: 1.0000 - val_loss: 0.9626 - val_accuracy: 0.8683\n",
      "Epoch 167/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7952e-05 - accuracy: 1.0000\n",
      "Epoch 00167: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.7952e-05 - accuracy: 1.0000 - val_loss: 0.9026 - val_accuracy: 0.8878\n",
      "Epoch 168/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.6505e-05 - accuracy: 1.0000\n",
      "Epoch 00168: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.6505e-05 - accuracy: 1.0000 - val_loss: 1.0161 - val_accuracy: 0.8780\n",
      "Epoch 169/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2977e-05 - accuracy: 1.0000\n",
      "Epoch 00169: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.2977e-05 - accuracy: 1.0000 - val_loss: 1.0350 - val_accuracy: 0.8585\n",
      "Epoch 170/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2400e-05 - accuracy: 1.0000\n",
      "Epoch 00170: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.2400e-05 - accuracy: 1.0000 - val_loss: 0.9973 - val_accuracy: 0.8585\n",
      "Epoch 171/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2789e-05 - accuracy: 1.0000\n",
      "Epoch 00171: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.2789e-05 - accuracy: 1.0000 - val_loss: 1.0376 - val_accuracy: 0.8780\n",
      "Epoch 172/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2066e-05 - accuracy: 1.0000\n",
      "Epoch 00172: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 1.2066e-05 - accuracy: 1.0000 - val_loss: 1.1007 - val_accuracy: 0.8732\n",
      "Epoch 173/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0729e-05 - accuracy: 1.0000\n",
      "Epoch 00173: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.89268\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.0729e-05 - accuracy: 1.0000 - val_loss: 1.0085 - val_accuracy: 0.8878\n",
      "Epoch 174/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 8.6710e-06 - accuracy: 1.0000\n",
      "Epoch 00174: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00174: val_accuracy improved from 0.89268 to 0.89756, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_5.h5\n",
      "58/58 [==============================] - 35s 606ms/step - loss: 8.6710e-06 - accuracy: 1.0000 - val_loss: 0.8923 - val_accuracy: 0.8976\n",
      "Epoch 175/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1850e-05 - accuracy: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.1850e-05 - accuracy: 1.0000 - val_loss: 1.1047 - val_accuracy: 0.8683\n",
      "Epoch 176/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0468e-05 - accuracy: 1.0000\n",
      "Epoch 00176: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.0468e-05 - accuracy: 1.0000 - val_loss: 0.8588 - val_accuracy: 0.8780\n",
      "Epoch 177/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0014e-05 - accuracy: 1.0000\n",
      "Epoch 00177: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 1.0014e-05 - accuracy: 1.0000 - val_loss: 1.0260 - val_accuracy: 0.8732\n",
      "Epoch 178/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0416e-05 - accuracy: 1.0000\n",
      "Epoch 00178: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.0416e-05 - accuracy: 1.0000 - val_loss: 0.9926 - val_accuracy: 0.8732\n",
      "Epoch 179/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 9.8021e-06 - accuracy: 1.0000\n",
      "Epoch 00179: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 9.8021e-06 - accuracy: 1.0000 - val_loss: 1.0502 - val_accuracy: 0.8780\n",
      "Epoch 180/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.4113e-06 - accuracy: 1.0000\n",
      "Epoch 00180: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 7.4113e-06 - accuracy: 1.0000 - val_loss: 1.0068 - val_accuracy: 0.8732\n",
      "Epoch 181/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1954e-05 - accuracy: 1.0000\n",
      "Epoch 00181: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.1954e-05 - accuracy: 1.0000 - val_loss: 1.0595 - val_accuracy: 0.8878\n",
      "Epoch 182/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0083e-05 - accuracy: 1.0000\n",
      "Epoch 00182: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.0083e-05 - accuracy: 1.0000 - val_loss: 1.0684 - val_accuracy: 0.8780\n",
      "Epoch 183/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2143e-05 - accuracy: 1.0000\n",
      "Epoch 00183: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.2143e-05 - accuracy: 1.0000 - val_loss: 0.8182 - val_accuracy: 0.8976\n",
      "Epoch 184/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 8.8844e-06 - accuracy: 1.0000\n",
      "Epoch 00184: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 8.8844e-06 - accuracy: 1.0000 - val_loss: 0.9597 - val_accuracy: 0.8829\n",
      "Epoch 185/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0723e-05 - accuracy: 1.0000\n",
      "Epoch 00185: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.0723e-05 - accuracy: 1.0000 - val_loss: 0.9998 - val_accuracy: 0.8634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 8.6534e-06 - accuracy: 1.0000\n",
      "Epoch 00186: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 8.6534e-06 - accuracy: 1.0000 - val_loss: 0.9182 - val_accuracy: 0.8927\n",
      "Epoch 187/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1020e-05 - accuracy: 1.0000\n",
      "Epoch 00187: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.1020e-05 - accuracy: 1.0000 - val_loss: 1.0118 - val_accuracy: 0.8829\n",
      "Epoch 188/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1792e-05 - accuracy: 1.0000\n",
      "Epoch 00188: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.1792e-05 - accuracy: 1.0000 - val_loss: 0.9346 - val_accuracy: 0.8683\n",
      "Epoch 189/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1307e-06 - accuracy: 1.0000\n",
      "Epoch 00189: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 7.1307e-06 - accuracy: 1.0000 - val_loss: 1.1331 - val_accuracy: 0.8829\n",
      "Epoch 190/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 9.4375e-06 - accuracy: 1.0000\n",
      "Epoch 00190: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 9.4375e-06 - accuracy: 1.0000 - val_loss: 0.9544 - val_accuracy: 0.8829\n",
      "Epoch 191/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 8.2339e-06 - accuracy: 1.0000\n",
      "Epoch 00191: val_loss did not improve from 0.79793\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.89756\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 8.2339e-06 - accuracy: 1.0000 - val_loss: 1.1413 - val_accuracy: 0.8780\n",
      "5  번째 학습을 완료했습니다.\n",
      "Epoch 1/2000\n",
      " 2/58 [>.............................] - ETA: 9s - loss: 34.3525 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1259s vs `on_train_batch_end` time: 0.2080s). Check your callbacks.\n",
      "58/58 [==============================] - ETA: 0s - loss: 17.1807 - accuracy: 0.0890\n",
      "Epoch 00001: val_loss improved from inf to 2586.12988, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.12195, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 24s 409ms/step - loss: 17.1807 - accuracy: 0.0890 - val_loss: 2586.1299 - val_accuracy: 0.1220\n",
      "Epoch 2/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.4556 - accuracy: 0.1101\n",
      "Epoch 00002: val_loss improved from 2586.12988 to 99.36571, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.12195\n",
      "58/58 [==============================] - 21s 362ms/step - loss: 2.4556 - accuracy: 0.1101 - val_loss: 99.3657 - val_accuracy: 0.1220\n",
      "Epoch 3/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.5389 - accuracy: 0.1226\n",
      "Epoch 00003: val_loss improved from 99.36571 to 9.44267, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.12195\n",
      "58/58 [==============================] - 21s 369ms/step - loss: 2.5389 - accuracy: 0.1226 - val_loss: 9.4427 - val_accuracy: 0.0829\n",
      "Epoch 4/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.3160 - accuracy: 0.1709\n",
      "Epoch 00004: val_loss improved from 9.44267 to 3.28067, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.12195\n",
      "58/58 [==============================] - 22s 373ms/step - loss: 2.3160 - accuracy: 0.1709 - val_loss: 3.2807 - val_accuracy: 0.0976\n",
      "Epoch 5/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.3347 - accuracy: 0.1975\n",
      "Epoch 00005: val_loss improved from 3.28067 to 3.05620, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.12195\n",
      "58/58 [==============================] - 22s 372ms/step - loss: 2.3347 - accuracy: 0.1975 - val_loss: 3.0562 - val_accuracy: 0.1171\n",
      "Epoch 6/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.2115 - accuracy: 0.2246\n",
      "Epoch 00006: val_loss improved from 3.05620 to 2.83686, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.12195 to 0.12683, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 39s 664ms/step - loss: 2.2115 - accuracy: 0.2246 - val_loss: 2.8369 - val_accuracy: 0.1268\n",
      "Epoch 7/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.2027 - accuracy: 0.2328\n",
      "Epoch 00007: val_loss did not improve from 2.83686\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.12683\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.2027 - accuracy: 0.2328 - val_loss: 12.8667 - val_accuracy: 0.0976\n",
      "Epoch 8/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0556 - accuracy: 0.2751\n",
      "Epoch 00008: val_loss did not improve from 2.83686\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.12683\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 2.0556 - accuracy: 0.2751 - val_loss: 2.8947 - val_accuracy: 0.1171\n",
      "Epoch 9/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.8825 - accuracy: 0.3527\n",
      "Epoch 00009: val_loss did not improve from 2.83686\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.12683\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.8825 - accuracy: 0.3527 - val_loss: 3.5257 - val_accuracy: 0.0976\n",
      "Epoch 10/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5748 - accuracy: 0.4531\n",
      "Epoch 00010: val_loss improved from 2.83686 to 2.13890, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.12683 to 0.23902, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 51s 880ms/step - loss: 1.5748 - accuracy: 0.4531 - val_loss: 2.1389 - val_accuracy: 0.2390\n",
      "Epoch 11/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.4069 - accuracy: 0.5193\n",
      "Epoch 00011: val_loss did not improve from 2.13890\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.23902 to 0.29756, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 22s 387ms/step - loss: 1.4069 - accuracy: 0.5193 - val_loss: 2.2913 - val_accuracy: 0.2976\n",
      "Epoch 12/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3715 - accuracy: 0.5404\n",
      "Epoch 00012: val_loss did not improve from 2.13890\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.29756\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.3715 - accuracy: 0.5404 - val_loss: 2.1440 - val_accuracy: 0.2976\n",
      "Epoch 13/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2248 - accuracy: 0.5817\n",
      "Epoch 00013: val_loss improved from 2.13890 to 1.61540, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.29756 to 0.44878, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 53s 908ms/step - loss: 1.2248 - accuracy: 0.5817 - val_loss: 1.6154 - val_accuracy: 0.4488\n",
      "Epoch 14/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0051 - accuracy: 0.6641\n",
      "Epoch 00014: val_loss did not improve from 1.61540\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.44878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.0051 - accuracy: 0.6641 - val_loss: 1.9793 - val_accuracy: 0.4390\n",
      "Epoch 15/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0210 - accuracy: 0.6549\n",
      "Epoch 00015: val_loss did not improve from 1.61540\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.44878\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 1.0210 - accuracy: 0.6549 - val_loss: 4.7191 - val_accuracy: 0.2732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.8913 - accuracy: 0.7037\n",
      "Epoch 00016: val_loss improved from 1.61540 to 1.29420, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.44878 to 0.59512, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 51s 879ms/step - loss: 0.8913 - accuracy: 0.7037 - val_loss: 1.2942 - val_accuracy: 0.5951\n",
      "Epoch 17/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.7550 - accuracy: 0.7406\n",
      "Epoch 00017: val_loss improved from 1.29420 to 1.11239, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.59512\n",
      "58/58 [==============================] - 22s 386ms/step - loss: 0.7550 - accuracy: 0.7406 - val_loss: 1.1124 - val_accuracy: 0.5902\n",
      "Epoch 18/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.6408 - accuracy: 0.7873\n",
      "Epoch 00018: val_loss improved from 1.11239 to 1.06585, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.59512 to 0.71220, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 25s 424ms/step - loss: 0.6408 - accuracy: 0.7873 - val_loss: 1.0658 - val_accuracy: 0.7122\n",
      "Epoch 19/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.7954\n",
      "Epoch 00019: val_loss did not improve from 1.06585\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.71220\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.6084 - accuracy: 0.7954 - val_loss: 1.9103 - val_accuracy: 0.5756\n",
      "Epoch 20/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.8291\n",
      "Epoch 00020: val_loss did not improve from 1.06585\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.71220\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.5031 - accuracy: 0.8291 - val_loss: 1.2567 - val_accuracy: 0.6000\n",
      "Epoch 21/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.8144\n",
      "Epoch 00021: val_loss did not improve from 1.06585\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.71220\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.5836 - accuracy: 0.8144 - val_loss: 1.1460 - val_accuracy: 0.7122\n",
      "Epoch 22/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.8209\n",
      "Epoch 00022: val_loss did not improve from 1.06585\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.71220\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.5114 - accuracy: 0.8209 - val_loss: 1.1903 - val_accuracy: 0.6732\n",
      "Epoch 23/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.8464\n",
      "Epoch 00023: val_loss did not improve from 1.06585\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.71220\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.4643 - accuracy: 0.8464 - val_loss: 1.6552 - val_accuracy: 0.6049\n",
      "Epoch 24/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4134 - accuracy: 0.8709\n",
      "Epoch 00024: val_loss improved from 1.06585 to 0.82821, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.71220 to 0.73171, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 51s 882ms/step - loss: 0.4134 - accuracy: 0.8709 - val_loss: 0.8282 - val_accuracy: 0.7317\n",
      "Epoch 25/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3149 - accuracy: 0.8899\n",
      "Epoch 00025: val_loss did not improve from 0.82821\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.73171\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.3149 - accuracy: 0.8899 - val_loss: 1.1607 - val_accuracy: 0.7073\n",
      "Epoch 26/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.9067\n",
      "Epoch 00026: val_loss did not improve from 0.82821\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.73171\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.2875 - accuracy: 0.9067 - val_loss: 1.0861 - val_accuracy: 0.6927\n",
      "Epoch 27/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.8985\n",
      "Epoch 00027: val_loss did not improve from 0.82821\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.73171 to 0.74634, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 22s 382ms/step - loss: 0.3025 - accuracy: 0.8985 - val_loss: 0.9733 - val_accuracy: 0.7463\n",
      "Epoch 28/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3647 - accuracy: 0.8747\n",
      "Epoch 00028: val_loss did not improve from 0.82821\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.74634\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.3647 - accuracy: 0.8747 - val_loss: 1.0023 - val_accuracy: 0.7220\n",
      "Epoch 29/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9078\n",
      "Epoch 00029: val_loss did not improve from 0.82821\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.74634\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2692 - accuracy: 0.9078 - val_loss: 1.3599 - val_accuracy: 0.7463\n",
      "Epoch 30/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.9110\n",
      "Epoch 00030: val_loss did not improve from 0.82821\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.74634\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2631 - accuracy: 0.9110 - val_loss: 2.1465 - val_accuracy: 0.5756\n",
      "Epoch 31/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9154\n",
      "Epoch 00031: val_loss improved from 0.82821 to 0.79347, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.74634 to 0.76585, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 53s 906ms/step - loss: 0.2648 - accuracy: 0.9154 - val_loss: 0.7935 - val_accuracy: 0.7659\n",
      "Epoch 32/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9251\n",
      "Epoch 00032: val_loss did not improve from 0.79347\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.76585\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.2284 - accuracy: 0.9251 - val_loss: 0.8878 - val_accuracy: 0.7610\n",
      "Epoch 33/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2150 - accuracy: 0.9267\n",
      "Epoch 00033: val_loss did not improve from 0.79347\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.76585\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2150 - accuracy: 0.9267 - val_loss: 1.7116 - val_accuracy: 0.6732\n",
      "Epoch 34/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9316\n",
      "Epoch 00034: val_loss did not improve from 0.79347\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.76585\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.2188 - accuracy: 0.9316 - val_loss: 1.0692 - val_accuracy: 0.7171\n",
      "Epoch 35/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9208\n",
      "Epoch 00035: val_loss did not improve from 0.79347\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.76585\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.2506 - accuracy: 0.9208 - val_loss: 1.4130 - val_accuracy: 0.6439\n",
      "Epoch 36/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.9159\n",
      "Epoch 00036: val_loss did not improve from 0.79347\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.76585\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2544 - accuracy: 0.9159 - val_loss: 1.3618 - val_accuracy: 0.6732\n",
      "Epoch 37/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.9050\n",
      "Epoch 00037: val_loss did not improve from 0.79347\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.76585\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.3165 - accuracy: 0.9050 - val_loss: 1.4092 - val_accuracy: 0.7073\n",
      "Epoch 38/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3534 - accuracy: 0.8909\n",
      "Epoch 00038: val_loss did not improve from 0.79347\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.76585\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.3534 - accuracy: 0.8909 - val_loss: 1.4020 - val_accuracy: 0.6878\n",
      "Epoch 39/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9230\n",
      "Epoch 00039: val_loss did not improve from 0.79347\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.76585\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2609 - accuracy: 0.9230 - val_loss: 1.2983 - val_accuracy: 0.7220\n",
      "Epoch 40/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9550\n",
      "Epoch 00040: val_loss improved from 0.79347 to 0.75474, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00040: val_accuracy improved from 0.76585 to 0.80976, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 53s 907ms/step - loss: 0.1505 - accuracy: 0.9550 - val_loss: 0.7547 - val_accuracy: 0.8098\n",
      "Epoch 41/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9626\n",
      "Epoch 00041: val_loss improved from 0.75474 to 0.55892, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00041: val_accuracy improved from 0.80976 to 0.84878, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 25s 425ms/step - loss: 0.0977 - accuracy: 0.9626 - val_loss: 0.5589 - val_accuracy: 0.8488\n",
      "Epoch 42/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9810\n",
      "Epoch 00042: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0624 - accuracy: 0.9810 - val_loss: 0.8892 - val_accuracy: 0.8000\n",
      "Epoch 43/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9593\n",
      "Epoch 00043: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1292 - accuracy: 0.9593 - val_loss: 0.7270 - val_accuracy: 0.8098\n",
      "Epoch 44/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9512\n",
      "Epoch 00044: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1604 - accuracy: 0.9512 - val_loss: 1.1295 - val_accuracy: 0.7756\n",
      "Epoch 45/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9257\n",
      "Epoch 00045: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2560 - accuracy: 0.9257 - val_loss: 1.2387 - val_accuracy: 0.7659\n",
      "Epoch 46/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.9181\n",
      "Epoch 00046: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.2902 - accuracy: 0.9181 - val_loss: 0.8265 - val_accuracy: 0.7610\n",
      "Epoch 47/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.9067\n",
      "Epoch 00047: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.3084 - accuracy: 0.9067 - val_loss: 2.5078 - val_accuracy: 0.6585\n",
      "Epoch 48/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.9088\n",
      "Epoch 00048: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.3371 - accuracy: 0.9088 - val_loss: 2.9639 - val_accuracy: 0.5902\n",
      "Epoch 49/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.9240\n",
      "Epoch 00049: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2502 - accuracy: 0.9240 - val_loss: 0.9448 - val_accuracy: 0.7805\n",
      "Epoch 50/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9501\n",
      "Epoch 00050: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1479 - accuracy: 0.9501 - val_loss: 0.7263 - val_accuracy: 0.8293\n",
      "Epoch 51/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1492 - accuracy: 0.9495\n",
      "Epoch 00051: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1492 - accuracy: 0.9495 - val_loss: 0.8877 - val_accuracy: 0.7805\n",
      "Epoch 52/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9523\n",
      "Epoch 00052: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.1576 - accuracy: 0.9523 - val_loss: 1.2481 - val_accuracy: 0.7463\n",
      "Epoch 53/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9512\n",
      "Epoch 00053: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1692 - accuracy: 0.9512 - val_loss: 0.7368 - val_accuracy: 0.8244\n",
      "Epoch 54/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9691\n",
      "Epoch 00054: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1109 - accuracy: 0.9691 - val_loss: 0.7511 - val_accuracy: 0.8244\n",
      "Epoch 55/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9750\n",
      "Epoch 00055: val_loss did not improve from 0.55892\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.84878\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0793 - accuracy: 0.9750 - val_loss: 0.8310 - val_accuracy: 0.8341\n",
      "Epoch 56/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9783\n",
      "Epoch 00056: val_loss improved from 0.55892 to 0.53624, saving model to ./home_models_0205\\02_04_AI_val_loss_index_7.h5\n",
      "\n",
      "Epoch 00056: val_accuracy improved from 0.84878 to 0.86829, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 53s 911ms/step - loss: 0.0728 - accuracy: 0.9783 - val_loss: 0.5362 - val_accuracy: 0.8683\n",
      "Epoch 57/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9729\n",
      "Epoch 00057: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1093 - accuracy: 0.9729 - val_loss: 1.3599 - val_accuracy: 0.7610\n",
      "Epoch 58/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9588\n",
      "Epoch 00058: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1423 - accuracy: 0.9588 - val_loss: 1.1727 - val_accuracy: 0.7756\n",
      "Epoch 59/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9674\n",
      "Epoch 00059: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1300 - accuracy: 0.9674 - val_loss: 1.3982 - val_accuracy: 0.7659\n",
      "Epoch 60/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9826\n",
      "Epoch 00060: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0589 - accuracy: 0.9826 - val_loss: 1.1060 - val_accuracy: 0.7951\n",
      "Epoch 61/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9740\n",
      "Epoch 00061: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0938 - accuracy: 0.9740 - val_loss: 0.9686 - val_accuracy: 0.8293\n",
      "Epoch 62/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9712\n",
      "Epoch 00062: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0890 - accuracy: 0.9712 - val_loss: 1.2175 - val_accuracy: 0.8000\n",
      "Epoch 63/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9674\n",
      "Epoch 00063: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1406 - accuracy: 0.9674 - val_loss: 4.5977 - val_accuracy: 0.5171\n",
      "Epoch 64/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9468\n",
      "Epoch 00064: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.2000 - accuracy: 0.9468 - val_loss: 1.0236 - val_accuracy: 0.8000\n",
      "Epoch 65/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.9414\n",
      "Epoch 00065: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.2192 - accuracy: 0.9414 - val_loss: 2.3692 - val_accuracy: 0.7220\n",
      "Epoch 66/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9457\n",
      "Epoch 00066: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.2020 - accuracy: 0.9457 - val_loss: 1.7972 - val_accuracy: 0.7220\n",
      "Epoch 67/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3153 - accuracy: 0.9284\n",
      "Epoch 00067: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.3153 - accuracy: 0.9284 - val_loss: 1.4302 - val_accuracy: 0.7317\n",
      "Epoch 68/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9349\n",
      "Epoch 00068: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2645 - accuracy: 0.9349 - val_loss: 1.2086 - val_accuracy: 0.7756\n",
      "Epoch 69/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9544\n",
      "Epoch 00069: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1846 - accuracy: 0.9544 - val_loss: 1.5004 - val_accuracy: 0.7951\n",
      "Epoch 70/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9664\n",
      "Epoch 00070: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1153 - accuracy: 0.9664 - val_loss: 1.6404 - val_accuracy: 0.7805\n",
      "Epoch 71/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9571\n",
      "Epoch 00071: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1542 - accuracy: 0.9571 - val_loss: 1.6329 - val_accuracy: 0.7659\n",
      "Epoch 72/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9512\n",
      "Epoch 00072: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1959 - accuracy: 0.9512 - val_loss: 2.4426 - val_accuracy: 0.6341\n",
      "Epoch 73/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3164 - accuracy: 0.9327\n",
      "Epoch 00073: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.3164 - accuracy: 0.9327 - val_loss: 2.6770 - val_accuracy: 0.6732\n",
      "Epoch 74/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9631\n",
      "Epoch 00074: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.1304 - accuracy: 0.9631 - val_loss: 1.2821 - val_accuracy: 0.8146\n",
      "Epoch 75/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9598\n",
      "Epoch 00075: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1659 - accuracy: 0.9598 - val_loss: 5.2834 - val_accuracy: 0.6537\n",
      "Epoch 76/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9571\n",
      "Epoch 00076: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1788 - accuracy: 0.9571 - val_loss: 1.3074 - val_accuracy: 0.7854\n",
      "Epoch 77/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9740\n",
      "Epoch 00077: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0964 - accuracy: 0.9740 - val_loss: 1.5100 - val_accuracy: 0.8000\n",
      "Epoch 78/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9783\n",
      "Epoch 00078: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0825 - accuracy: 0.9783 - val_loss: 2.2288 - val_accuracy: 0.7366\n",
      "Epoch 79/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1510 - accuracy: 0.9669\n",
      "Epoch 00079: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.1510 - accuracy: 0.9669 - val_loss: 0.9483 - val_accuracy: 0.8146\n",
      "Epoch 80/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9745\n",
      "Epoch 00080: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0918 - accuracy: 0.9745 - val_loss: 1.7150 - val_accuracy: 0.7659\n",
      "Epoch 81/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9756\n",
      "Epoch 00081: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0641 - accuracy: 0.9756 - val_loss: 1.7515 - val_accuracy: 0.7854\n",
      "Epoch 82/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9886\n",
      "Epoch 00082: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0345 - accuracy: 0.9886 - val_loss: 1.0378 - val_accuracy: 0.8488\n",
      "Epoch 83/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9864\n",
      "Epoch 00083: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0439 - accuracy: 0.9864 - val_loss: 1.2617 - val_accuracy: 0.8049\n",
      "Epoch 84/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9718\n",
      "Epoch 00084: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0967 - accuracy: 0.9718 - val_loss: 1.0836 - val_accuracy: 0.8439\n",
      "Epoch 85/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9392\n",
      "Epoch 00085: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.2841 - accuracy: 0.9392 - val_loss: 2.8032 - val_accuracy: 0.6829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.9387\n",
      "Epoch 00086: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2695 - accuracy: 0.9387 - val_loss: 1.9316 - val_accuracy: 0.7659\n",
      "Epoch 87/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9582\n",
      "Epoch 00087: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1856 - accuracy: 0.9582 - val_loss: 1.7587 - val_accuracy: 0.7707\n",
      "Epoch 88/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9636\n",
      "Epoch 00088: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.1309 - accuracy: 0.9636 - val_loss: 1.2754 - val_accuracy: 0.8195\n",
      "Epoch 89/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9658\n",
      "Epoch 00089: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1571 - accuracy: 0.9658 - val_loss: 2.6466 - val_accuracy: 0.7366\n",
      "Epoch 90/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9479\n",
      "Epoch 00090: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2409 - accuracy: 0.9479 - val_loss: 2.5733 - val_accuracy: 0.7268\n",
      "Epoch 91/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.9392\n",
      "Epoch 00091: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.2769 - accuracy: 0.9392 - val_loss: 1.8137 - val_accuracy: 0.7902\n",
      "Epoch 92/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9533\n",
      "Epoch 00092: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.2125 - accuracy: 0.9533 - val_loss: 2.0281 - val_accuracy: 0.7415\n",
      "Epoch 93/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9783\n",
      "Epoch 00093: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0661 - accuracy: 0.9783 - val_loss: 1.8762 - val_accuracy: 0.7951\n",
      "Epoch 94/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9805\n",
      "Epoch 00094: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0892 - accuracy: 0.9805 - val_loss: 1.4451 - val_accuracy: 0.7854\n",
      "Epoch 95/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9848\n",
      "Epoch 00095: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0600 - accuracy: 0.9848 - val_loss: 1.9378 - val_accuracy: 0.7707\n",
      "Epoch 96/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9745\n",
      "Epoch 00096: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.1099 - accuracy: 0.9745 - val_loss: 1.9121 - val_accuracy: 0.7366\n",
      "Epoch 97/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9853\n",
      "Epoch 00097: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 0.0604 - accuracy: 0.9853 - val_loss: 1.0339 - val_accuracy: 0.8390\n",
      "Epoch 98/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9967\n",
      "Epoch 00098: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.86829\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0066 - accuracy: 0.9967 - val_loss: 0.6826 - val_accuracy: 0.8683\n",
      "Epoch 99/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9984\n",
      "Epoch 00099: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00099: val_accuracy improved from 0.86829 to 0.87805, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 36s 614ms/step - loss: 0.0030 - accuracy: 0.9984 - val_loss: 0.6687 - val_accuracy: 0.8780\n",
      "Epoch 100/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 00100: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00100: val_accuracy improved from 0.87805 to 0.88293, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 23s 390ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.7432 - val_accuracy: 0.8829\n",
      "Epoch 101/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.7853e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.88293\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 6.7853e-04 - accuracy: 1.0000 - val_loss: 0.7191 - val_accuracy: 0.8634\n",
      "Epoch 102/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0798e-04 - accuracy: 1.0000\n",
      "Epoch 00102: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.88293\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 1.0798e-04 - accuracy: 1.0000 - val_loss: 0.7094 - val_accuracy: 0.8732\n",
      "Epoch 103/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9856e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.88293\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.9856e-04 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.8683\n",
      "Epoch 104/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.5288e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00104: val_accuracy improved from 0.88293 to 0.88780, saving model to ./home_models_0205\\02_04_AI_val_accuracy_index_7.h5\n",
      "58/58 [==============================] - 35s 610ms/step - loss: 2.5288e-04 - accuracy: 1.0000 - val_loss: 0.6584 - val_accuracy: 0.8878\n",
      "Epoch 105/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.2366e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.2366e-04 - accuracy: 1.0000 - val_loss: 0.7533 - val_accuracy: 0.8732\n",
      "Epoch 106/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1912e-04 - accuracy: 0.9995\n",
      "Epoch 00106: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 7.1912e-04 - accuracy: 0.9995 - val_loss: 0.8794 - val_accuracy: 0.8683\n",
      "Epoch 107/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 00107: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.9106 - val_accuracy: 0.8780\n",
      "Epoch 108/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3034e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 7.3034e-04 - accuracy: 1.0000 - val_loss: 0.7784 - val_accuracy: 0.8732\n",
      "Epoch 109/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9995\n",
      "Epoch 00109: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 0.0010 - accuracy: 0.9995 - val_loss: 0.7008 - val_accuracy: 0.8732\n",
      "Epoch 110/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 9.3446e-04 - accuracy: 0.9995\n",
      "Epoch 00110: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 9.3446e-04 - accuracy: 0.9995 - val_loss: 0.9149 - val_accuracy: 0.8780\n",
      "Epoch 111/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.5936e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.5936e-04 - accuracy: 1.0000 - val_loss: 0.9559 - val_accuracy: 0.8780\n",
      "Epoch 112/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.6574e-05 - accuracy: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 7.6574e-05 - accuracy: 1.0000 - val_loss: 0.8285 - val_accuracy: 0.8732\n",
      "Epoch 113/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 00113: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.8718 - val_accuracy: 0.8683\n",
      "Epoch 114/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.0452e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 3.0452e-04 - accuracy: 1.0000 - val_loss: 0.7157 - val_accuracy: 0.8829\n",
      "Epoch 115/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5648e-04 - accuracy: 1.0000\n",
      "Epoch 00115: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.5648e-04 - accuracy: 1.0000 - val_loss: 0.8094 - val_accuracy: 0.8829\n",
      "Epoch 116/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7534e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.7534e-04 - accuracy: 1.0000 - val_loss: 0.7665 - val_accuracy: 0.8878\n",
      "Epoch 117/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8845e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.8845e-04 - accuracy: 1.0000 - val_loss: 0.7986 - val_accuracy: 0.8878\n",
      "Epoch 118/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.7396e-05 - accuracy: 1.0000\n",
      "Epoch 00118: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 353ms/step - loss: 4.7396e-05 - accuracy: 1.0000 - val_loss: 0.7866 - val_accuracy: 0.8829\n",
      "Epoch 119/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.5626e-05 - accuracy: 1.0000\n",
      "Epoch 00119: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.5626e-05 - accuracy: 1.0000 - val_loss: 0.6540 - val_accuracy: 0.8732\n",
      "Epoch 120/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2131e-04 - accuracy: 1.0000\n",
      "Epoch 00120: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.2131e-04 - accuracy: 1.0000 - val_loss: 0.8152 - val_accuracy: 0.8634\n",
      "Epoch 121/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0442e-04 - accuracy: 1.0000\n",
      "Epoch 00121: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 2.0442e-04 - accuracy: 1.0000 - val_loss: 0.8595 - val_accuracy: 0.8585\n",
      "Epoch 122/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8816e-05 - accuracy: 1.0000\n",
      "Epoch 00122: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.8816e-05 - accuracy: 1.0000 - val_loss: 0.6988 - val_accuracy: 0.8683\n",
      "Epoch 123/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.3381e-05 - accuracy: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.3381e-05 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 0.8829\n",
      "Epoch 124/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.2439e-05 - accuracy: 1.0000\n",
      "Epoch 00124: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 5.2439e-05 - accuracy: 1.0000 - val_loss: 0.8385 - val_accuracy: 0.8439\n",
      "Epoch 125/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.1402e-05 - accuracy: 1.0000\n",
      "Epoch 00125: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.1402e-05 - accuracy: 1.0000 - val_loss: 0.7878 - val_accuracy: 0.8634\n",
      "Epoch 126/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.6289e-05 - accuracy: 1.0000\n",
      "Epoch 00126: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.6289e-05 - accuracy: 1.0000 - val_loss: 0.7406 - val_accuracy: 0.8829\n",
      "Epoch 127/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.3731e-05 - accuracy: 1.0000\n",
      "Epoch 00127: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.3731e-05 - accuracy: 1.0000 - val_loss: 0.8752 - val_accuracy: 0.8537\n",
      "Epoch 128/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.1411e-05 - accuracy: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.1411e-05 - accuracy: 1.0000 - val_loss: 0.5844 - val_accuracy: 0.8878\n",
      "Epoch 129/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7686e-05 - accuracy: 1.0000\n",
      "Epoch 00129: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 1.7686e-05 - accuracy: 1.0000 - val_loss: 0.8601 - val_accuracy: 0.8634\n",
      "Epoch 130/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.8927e-05 - accuracy: 1.0000\n",
      "Epoch 00130: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 352ms/step - loss: 1.8927e-05 - accuracy: 1.0000 - val_loss: 0.7420 - val_accuracy: 0.8732\n",
      "Epoch 131/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8295e-05 - accuracy: 1.0000\n",
      "Epoch 00131: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 2.8295e-05 - accuracy: 1.0000 - val_loss: 0.8367 - val_accuracy: 0.8683\n",
      "Epoch 132/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.1081e-05 - accuracy: 1.0000\n",
      "Epoch 00132: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 3.1081e-05 - accuracy: 1.0000 - val_loss: 0.6379 - val_accuracy: 0.8780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.6811e-05 - accuracy: 1.0000\n",
      "Epoch 00133: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 351ms/step - loss: 1.6811e-05 - accuracy: 1.0000 - val_loss: 0.7617 - val_accuracy: 0.8829\n",
      "Epoch 134/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 9.6640e-06 - accuracy: 1.0000\n",
      "Epoch 00134: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 20s 350ms/step - loss: 9.6640e-06 - accuracy: 1.0000 - val_loss: 0.8648 - val_accuracy: 0.8780\n",
      "Epoch 135/2000\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.1223e-05 - accuracy: 1.0000\n",
      "Epoch 00135: val_loss did not improve from 0.53624\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.88780\n",
      "58/58 [==============================] - 21s 356ms/step - loss: 2.1223e-05 - accuracy: 1.0000 - val_loss: 0.8204 - val_accuracy: 0.8732\n",
      "Epoch 136/2000\n",
      "22/58 [==========>...................] - ETA: 12s - loss: 5.0364e-05 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-6ea1acc3aa21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreLR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcp2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \"\"\"\n\u001b[0;32m   1814\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit_generator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1816\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m           \u001b[0mnumpy_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index=1\n",
    "result = 0\n",
    "for train_index,val_index in kfold.split(x,y):\n",
    "    if index>=5:\n",
    "        optimizer = Adam(lr=0.003,epsilon=None)\n",
    "        modelpath = './home_models_0205/02_04_AI_val_loss_index_{}.h5'.format(index)\n",
    "        modelpath2 = './home_models_0205/02_04_AI_val_accuracy_index_{}.h5'.format(index)\n",
    "        cp = ModelCheckpoint(monitor = 'val_loss',filepath=modelpath,save_best_only=True,verbose=1)\n",
    "        cp2 = ModelCheckpoint(monitor = 'val_accuracy',filepath=modelpath2,save_best_only=True,verbose=1)\n",
    "\n",
    "\n",
    "        x_train = x[train_index]\n",
    "        x_val = x[val_index]\n",
    "        y_train = y[train_index]\n",
    "        y_val = y[val_index]\n",
    "\n",
    "        onehot = OneHotEncoder()\n",
    "        y_train = onehot.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "        y_val = onehot.fit_transform(y_val.reshape(-1,1)).toarray().astype('float32')\n",
    "\n",
    "        train_generator = datagen.flow(x_train,y_train,batch_size=32)\n",
    "        val_generator = datagen.flow(x_val,y_val)\n",
    "        model = modeling()\n",
    "        model.compile(loss = 'categorical_crossentropy',optimizer = optimizer,metrics=['accuracy'])\n",
    "        model.fit_generator(train_generator,validation_data = val_generator,epochs=epochs,callbacks=[cp,es,reLR,cp2])\n",
    "\n",
    "        model = load_model(modelpath)\n",
    "        model2 = load_model(modelpath2)\n",
    "        df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "        x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred2 = model2.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred,axis=-1)\n",
    "        y_pred2 = np.argmax(y_pred2,axis=-1)\n",
    "        df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "        df_sub['digit'] = y_pred\n",
    "        df_sub.to_csv('./home_models_0205/loss_kfold_{}.csv'.format(index))\n",
    "        df_sub['digit'] = y_pred2\n",
    "        df_sub.to_csv('./home_models_0205/accuracy_kfold_{}.csv'.format(index))\n",
    "\n",
    "        print(index, \" 번째 학습을 완료했습니다.\")\n",
    "        index+=1\n",
    "    \n",
    "    index+=1\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "0.905\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44FNCmNG-Mkd"
   },
   "outputs": [],
   "source": [
    "model = load_model('./AI_models/02_04_AI_val_loss_index_3.h5')\n",
    "#model = load_model('./models/02_03_imger_best_index_1.h5')\n",
    "df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred,axis=-1)\n",
    "df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "df_sub['digit'] = y_pred\n",
    "df_sub.to_csv('0204_f2.csv')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "  2/640 [..............................] - ETA: 33sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_predict_batch_end` time: 0.0995s). Check your callbacks.\n",
      "640/640 [==============================] - 64s 100ms/step\n",
      "  1/640 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_predict_batch_end` time: 0.0522s). Check your callbacks.\n",
      "640/640 [==============================] - 28s 44ms/step\n",
      "2\n",
      "  2/640 [..............................] - ETA: 32sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_predict_batch_end` time: 0.1004s). Check your callbacks.\n",
      "640/640 [==============================] - 64s 101ms/step\n",
      "  1/640 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_predict_batch_end` time: 0.0370s). Check your callbacks.\n",
      "640/640 [==============================] - 28s 44ms/step\n",
      "3\n",
      "  2/640 [..............................] - ETA: 35sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_predict_batch_end` time: 0.1028s). Check your callbacks.\n",
      "640/640 [==============================] - 64s 100ms/step\n",
      "  1/640 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_predict_batch_end` time: 0.0370s). Check your callbacks.\n",
      "640/640 [==============================] - 28s 44ms/step\n",
      "4\n",
      "  2/640 [..............................] - ETA: 34sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_predict_batch_end` time: 0.1019s). Check your callbacks.\n",
      "640/640 [==============================] - 64s 100ms/step\n",
      "  1/640 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_predict_batch_end` time: 0.0512s). Check your callbacks.\n",
      "640/640 [==============================] - 28s 44ms/step\n",
      "5\n",
      "  2/640 [..............................] - ETA: 37sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_predict_batch_end` time: 0.0993s). Check your callbacks.\n",
      "640/640 [==============================] - 65s 102ms/step\n",
      "  1/640 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_predict_batch_end` time: 0.0354s). Check your callbacks.\n",
      "640/640 [==============================] - 28s 45ms/step\n",
      "6\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ./home_models/02_04_AI_val_loss_index_6.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-b357cc3cd200>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdf_sub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'digit'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdf_sub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./test_img_generator/val_accuracy_kfold_{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmake_acc_loss_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-b357cc3cd200>\u001b[0m in \u001b[0;36mmake_acc_loss_csv\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./home_models/02_04_AI_val_loss_index_{}.h5'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./home_models/02_04_AI_val_accuracy_index_{}.h5'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m       \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    108\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[0;32m    111\u001b[0m                   (export_dir,\n\u001b[0;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: ./home_models/02_04_AI_val_loss_index_6.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "def make_acc_loss_csv(index):\n",
    "    for i in range(1,index+1):\n",
    "        print(i)\n",
    "        model1 = load_model('./home_models/02_04_AI_val_loss_index_{}.h5'.format(i))\n",
    "        model2 = load_model('./home_models/02_04_AI_val_accuracy_index_{}.h5'.format(i))\n",
    "        df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "        x_test = df.values[:,1:].reshape(-1,28,28,1).astype('float32')/255.0\n",
    "        datagen = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "        test_generator = datagen.flow(x_test,shuffle=False)\n",
    "        y_pred = model.predict_generator(test_generator,verbose=True)\n",
    "        y_pred = np.argmax(y_pred,axis=-1)\n",
    "        y_pred2 = model2.predict_generator(test_generator,verbose=True)\n",
    "        y_pred2 = np.argmax(y_pred2,axis=-1)\n",
    "        df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "        df_sub['digit'] = y_pred\n",
    "        df_sub.to_csv('./test_img_generator/val_loss_kfold_{}.csv'.format(i))\n",
    "        df_sub['digit'] = y_pred2\n",
    "        df_sub.to_csv('./test_img_generator/val_accuracy_kfold_{}.csv'.format(i))\n",
    "make_acc_loss_csv(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-81beedd58516>:9: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "  1/640 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_predict_batch_end` time: 0.2361s). Check your callbacks.\n",
      "640/640 [==============================] - 158s 247ms/step\n",
      "  2/640 [..............................] - ETA: 1:17WARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_predict_batch_end` time: 0.2402s). Check your callbacks.\n",
      "640/640 [==============================] - 159s 248ms/step\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,1024,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_5/conv2d_58/Conv2D (defined at <ipython-input-7-81beedd58516>:9) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_29544]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-81beedd58516>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdf_sub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'digit'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdf_sub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./test_img_generator/val_accuracy_kfold_{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmake_acc_loss_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-81beedd58516>\u001b[0m in \u001b[0;36mmake_acc_loss_csv\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mdatagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight_shift_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth_shift_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtest_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0my_pred2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[1;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1874\u001b[0m     \"\"\"\n\u001b[0;32m   1875\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict_generator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1876\u001b[1;33m     return self.predict(\n\u001b[0m\u001b[0;32m   1877\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1878\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    844\u001b[0m               *args, **kwds)\n\u001b[0;32m    845\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,1024,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_5/conv2d_58/Conv2D (defined at <ipython-input-7-81beedd58516>:9) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_29544]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "def make_acc_loss_csv(index):\n",
    "    for i in range(1,index+1):\n",
    "        model1 = load_model('./AI_models/02_04_AI_val_loss_index_{}.h5'.format(i))\n",
    "        model2 = load_model('./AI_models/02_04_AI_val_accuracy_index_{}.h5'.format(i))\n",
    "        df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "        x_test = df.values[:,1:].reshape(-1,28,28,1).astype('float32')/255.0\n",
    "        datagen = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "        test_generator = datagen.flow(x_test,shuffle=False)\n",
    "        y_pred = model1.predict_generator(test_generator,verbose=True)\n",
    "        y_pred = np.argmax(y_pred,axis=-1)\n",
    "        y_pred2 = model2.predict_generator(test_generator,verbose=True)\n",
    "        y_pred2 = np.argmax(y_pred2,axis=-1)\n",
    "        df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "        df_sub['digit'] = y_pred\n",
    "        df_sub.to_csv('./test_img_generator/val_loss_kfold_{}.csv'.format(i))\n",
    "        df_sub['digit'] = y_pred2\n",
    "        df_sub.to_csv('./test_img_generator/val_accuracy_kfold_{}.csv'.format(i))\n",
    "make_acc_loss_csv(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembles(index):\n",
    "    accuracy=[]\n",
    "    for i in index:\n",
    "    #for i in range(1,index+1):\n",
    "        a = pd.read_csv(\"./AI_models/accuracy_kfold_{}.csv\".format(i),index_col=0)\n",
    "        accuracy.append(a.values)\n",
    "    accuracy = np.concatenate(accuracy,axis=1)\n",
    "            \n",
    "    loss=[]\n",
    "    for i in index:\n",
    "    #for i in range(1,index+1):\n",
    "        b = pd.read_csv(\"./AI_models/loss_kfold_{}.csv\".format(i),index_col=0)\n",
    "        loss.append(b.values)        \n",
    "    loss = np.concatenate(loss,axis=1)\n",
    "    \n",
    "    accuracy_list=[]\n",
    "    for j in range(len(accuracy)):\n",
    "        count_list=[0,0,0,0,0,0,0,0,0,0]\n",
    "        k = list(accuracy[j])\n",
    "        for n in k:\n",
    "            count_list[n]+=1\n",
    "        accuracy_mode = count_list.index(max(count_list))\n",
    "        accuracy_list.append(accuracy_mode)\n",
    "    accuracy_list = np.array(accuracy_list).reshape(-1,1)\n",
    "    df = pd.read_csv(\"submission.csv\",index_col=0)\n",
    "    df['digit'] = accuracy_list\n",
    "    df.to_csv('./AI_models/accuracy_ensembles.csv')\n",
    "    \n",
    "    loss_list=[]\n",
    "    for j in range(len(loss)):\n",
    "        count_list=[0,0,0,0,0,0,0,0,0,0]\n",
    "        k = list(loss[j])\n",
    "        for n in k:\n",
    "            count_list[n]+=1\n",
    "        loss_mode = count_list.index(max(count_list))\n",
    "        loss_list.append(loss_mode)\n",
    "    loss_list = np.array(loss_list).reshape(-1,1)\n",
    "    df = pd.read_csv(\"submission.csv\",index_col=0)\n",
    "    df['digit'] = accuracy_list\n",
    "    df.to_csv('./AI_models/loss_ensembles.csv')\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    result = np.concatenate([accuracy,loss],axis=1)\n",
    "    mode_list=[]\n",
    "    for j in range(len(result)):\n",
    "        count_list=[0,0,0,0,0,0,0,0,0,0]\n",
    "        k = list(result[j])\n",
    "        for n in k:\n",
    "            count_list[n]+=1\n",
    "        mode = count_list.index(max(count_list))\n",
    "        mode_list.append(mode)\n",
    "    mode_list = np.array(mode_list).reshape(-1,1)\n",
    "    df = pd.read_csv(\"submission.csv\",index_col=0)\n",
    "    df['digit'] = mode_list\n",
    "    df.to_csv('./AI_models/both_ensembles'+'.csv')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles([1,3,4,5,6,7,9,10,11,12,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5_Acvtq-Mkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7pmh6Ej-Mkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = inputs\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = _x\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048)(x)\n",
    "    x = Dense(10,activation='softmax')(x)\n",
    "    outputs=x\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras import Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers, initializers, regularizers, metrics\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add\n",
    " \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_dir = os.path.join('./dataset/1/images/train')\n",
    "val_dir = os.path.join('./dataset/1/images/val')\n",
    " \n",
    " \n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size=16, target_size=(224, 224), color_mode='rgb')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, batch_size=16, target_size=(224, 224), color_mode='rgb')\n",
    " \n",
    "# number of classes\n",
    "K = 4\n",
    "\n",
    "input_tensor = Input(shape=(224, 224, 3), dtype='float32', name='input')\n",
    " \n",
    "def conv1_layer(x):    \n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D(padding=(1,1))(x)\n",
    " \n",
    "    return x   \n",
    " \n",
    "    \n",
    "def conv2_layer(x):         \n",
    "    x = MaxPooling2D((3, 3), 2)(x)     \n",
    " \n",
    "    shortcut = x\n",
    " \n",
    "    for i in range(3):\n",
    "        if (i == 0):\n",
    "            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            shortcut = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(shortcut)            \n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    " \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    " \n",
    "        else:\n",
    "            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)            \n",
    " \n",
    "            x = Add()([x, shortcut])   \n",
    "            x = Activation('relu')(x)  \n",
    " \n",
    "            shortcut = x        \n",
    "    \n",
    "    return x\n",
    " \n",
    " \n",
    " \n",
    "def conv3_layer(x):        \n",
    "    shortcut = x    \n",
    "    \n",
    "    for i in range(4):     \n",
    "        if(i == 0):            \n",
    "            x = Conv2D(128, (1, 1), strides=(2, 2), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)        \n",
    "            \n",
    "            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)  \n",
    " \n",
    "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            shortcut = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)            \n",
    " \n",
    "            x = Add()([x, shortcut])    \n",
    "            x = Activation('relu')(x)    \n",
    " \n",
    "            shortcut = x              \n",
    "        \n",
    "        else:\n",
    "            x = Conv2D(128, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)            \n",
    " \n",
    "            x = Add()([x, shortcut])     \n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            shortcut = x      \n",
    "            \n",
    "    return x\n",
    " \n",
    " \n",
    " \n",
    "def conv4_layer(x):\n",
    "    shortcut = x        \n",
    "  \n",
    "    for i in range(6):     \n",
    "        if(i == 0):            \n",
    "            x = Conv2D(256, (1, 1), strides=(2, 2), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)        \n",
    "            \n",
    "            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)  \n",
    " \n",
    "            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            shortcut = Conv2D(1024, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    " \n",
    "            x = Add()([x, shortcut]) \n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            shortcut = x               \n",
    "        \n",
    "        else:\n",
    "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)            \n",
    " \n",
    "            x = Add()([x, shortcut])    \n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            shortcut = x      \n",
    " \n",
    "    return x\n",
    " \n",
    " \n",
    " \n",
    "def conv5_layer(x):\n",
    "    shortcut = x    \n",
    "  \n",
    "    for i in range(3):     \n",
    "        if(i == 0):            \n",
    "            x = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)        \n",
    "            \n",
    "            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)  \n",
    " \n",
    "            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            shortcut = Conv2D(2048, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)            \n",
    " \n",
    "            x = Add()([x, shortcut])  \n",
    "            x = Activation('relu')(x)      \n",
    " \n",
    "            shortcut = x               \n",
    "        \n",
    "        else:\n",
    "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)           \n",
    "            \n",
    "            x = Add()([x, shortcut]) \n",
    "            x = Activation('relu')(x)       \n",
    " \n",
    "            shortcut = x                  \n",
    " \n",
    "    return x\n",
    " \n",
    " \n",
    " \n",
    "x = conv1_layer(input_tensor)\n",
    "x = conv2_layer(x)\n",
    "x = conv3_layer(x)\n",
    "x = conv4_layer(x)\n",
    "x = conv5_layer(x)\n",
    " \n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output_tensor = Dense(K, activation='softmax')(x)\n",
    " \n",
    "resnet50 = Model(input_tensor, output_tensor)\n",
    "resnet50.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
