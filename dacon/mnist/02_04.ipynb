{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ooCpvRmT-MkR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dropout,Input,Activation,Dense\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GysUTZTp-MkX"
   },
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s4kX312e-MkY"
   },
   "outputs": [],
   "source": [
    "def modeling():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = inputs\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = _x\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048)(x)\n",
    "    x = Dense(10,activation='softmax')(x)\n",
    "    outputs=x\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cccyJfWO-MkY"
   },
   "source": [
    "# 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W73nGvZM-MkZ"
   },
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "es = EarlyStopping(monitor='val_loss',patience=160)\n",
    "reLR = ReduceLROnPlateau(patience=120,verbose=1,factor=0.8)\n",
    "kfold = StratifiedKFold(n_splits=40,random_state=42,shuffle=True)\n",
    "\n",
    "datagen = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "datagen2 = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWqLv-B1-MkZ"
   },
   "source": [
    "# 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j6UbQKZk-MkZ",
    "outputId": "26984b86-65c4-4adf-c0dc-bcb09f17d346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 28, 28, 1) (2048,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 9, 0, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "y = df.values[:,0].astype('int32')\n",
    "x = df.values[:,2:].astype('float32')/255.0\n",
    "# print(x.shape,y.shape)               # (2048, 28, 28) (2048,)\n",
    "#onehot = OneHotEncoder()\n",
    "#y = onehot.fit_transform(y.reshape(-1,1)).toarray().astype('float32')\n",
    "x = x.reshape(-1,28,28,1)\n",
    "# x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.15)\n",
    "# x_train = x_train.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# x_val = x_val.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# print(x_train.shape,x_val.shape,y_train.shape,y_val.shape)\n",
    "print(x.shape,y.shape) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmMESLTT-Mkb",
    "outputId": "6e97efa1-d705-4b3d-9e71-4261745da5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-01ffc4fd4d71>:27: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 8s - loss: 7.6581 - accuracy: 0.0469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0929s vs `on_train_batch_end` time: 0.1795s). Check your callbacks.\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9512 - accuracy: 0.1047\n",
      "Epoch 00001: val_loss improved from inf to 49090.88281, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.11765, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 24s 386ms/step - loss: 3.9512 - accuracy: 0.1047 - val_loss: 49090.8828 - val_accuracy: 0.1176\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2494 - accuracy: 0.1612\n",
      "Epoch 00002: val_loss improved from 49090.88281 to 2355.42407, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 315ms/step - loss: 2.2494 - accuracy: 0.1612 - val_loss: 2355.4241 - val_accuracy: 0.1176\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1041 - accuracy: 0.2479\n",
      "Epoch 00003: val_loss improved from 2355.42407 to 2.77620, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 314ms/step - loss: 2.1041 - accuracy: 0.2479 - val_loss: 2.7762 - val_accuracy: 0.0980\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9296 - accuracy: 0.3135\n",
      "Epoch 00004: val_loss improved from 2.77620 to 2.75024, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 317ms/step - loss: 1.9296 - accuracy: 0.3135 - val_loss: 2.7502 - val_accuracy: 0.0980\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6231 - accuracy: 0.4246\n",
      "Epoch 00005: val_loss did not improve from 2.75024\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 17s 274ms/step - loss: 1.6231 - accuracy: 0.4246 - val_loss: 3.1598 - val_accuracy: 0.1176\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3084 - accuracy: 0.5488\n",
      "Epoch 00006: val_loss did not improve from 2.75024\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 1.3084 - accuracy: 0.5488 - val_loss: 29.8067 - val_accuracy: 0.0980\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1335 - accuracy: 0.6024\n",
      "Epoch 00007: val_loss did not improve from 2.75024\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 1.1335 - accuracy: 0.6024 - val_loss: 8.6876 - val_accuracy: 0.0588\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9130 - accuracy: 0.6870\n",
      "Epoch 00008: val_loss did not improve from 2.75024\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.9130 - accuracy: 0.6870 - val_loss: 7.6584 - val_accuracy: 0.0980\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8148 - accuracy: 0.7306\n",
      "Epoch 00009: val_loss did not improve from 2.75024\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.8148 - accuracy: 0.7306 - val_loss: 7.6399 - val_accuracy: 0.0980\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6716 - accuracy: 0.7727\n",
      "Epoch 00010: val_loss did not improve from 2.75024\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.11765 to 0.17647, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 30s 480ms/step - loss: 0.6716 - accuracy: 0.7727 - val_loss: 6.4983 - val_accuracy: 0.1765\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.7717\n",
      "Epoch 00011: val_loss improved from 2.75024 to 2.56876, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.17647 to 0.45098, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 46s 724ms/step - loss: 0.6860 - accuracy: 0.7717 - val_loss: 2.5688 - val_accuracy: 0.4510\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6344 - accuracy: 0.7822\n",
      "Epoch 00012: val_loss improved from 2.56876 to 2.01348, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.45098 to 0.52941, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 23s 364ms/step - loss: 0.6344 - accuracy: 0.7822 - val_loss: 2.0135 - val_accuracy: 0.5294\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5968 - accuracy: 0.8032\n",
      "Epoch 00013: val_loss improved from 2.01348 to 0.65721, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.52941 to 0.82353, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 23s 361ms/step - loss: 0.5968 - accuracy: 0.8032 - val_loss: 0.6572 - val_accuracy: 0.8235\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.8423\n",
      "Epoch 00014: val_loss improved from 0.65721 to 0.60858, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.82353\n",
      "63/63 [==============================] - 19s 309ms/step - loss: 0.4410 - accuracy: 0.8423 - val_loss: 0.6086 - val_accuracy: 0.7843\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3907 - accuracy: 0.8673\n",
      "Epoch 00015: val_loss did not improve from 0.60858\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.82353\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.3907 - accuracy: 0.8673 - val_loss: 0.7256 - val_accuracy: 0.8039\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3933 - accuracy: 0.8668\n",
      "Epoch 00016: val_loss did not improve from 0.60858\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.82353\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.3933 - accuracy: 0.8668 - val_loss: 0.8966 - val_accuracy: 0.7255\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3595 - accuracy: 0.8803\n",
      "Epoch 00017: val_loss improved from 0.60858 to 0.59465, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.82353\n",
      "63/63 [==============================] - 20s 324ms/step - loss: 0.3595 - accuracy: 0.8803 - val_loss: 0.5946 - val_accuracy: 0.8235\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.8808\n",
      "Epoch 00018: val_loss improved from 0.59465 to 0.41361, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.82353 to 0.84314, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 34s 544ms/step - loss: 0.3618 - accuracy: 0.8808 - val_loss: 0.4136 - val_accuracy: 0.8431\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2928 - accuracy: 0.9004\n",
      "Epoch 00019: val_loss did not improve from 0.41361\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.84314\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.2928 - accuracy: 0.9004 - val_loss: 0.4846 - val_accuracy: 0.8431\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.9064\n",
      "Epoch 00020: val_loss did not improve from 0.41361\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.84314\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.2726 - accuracy: 0.9064 - val_loss: 0.8487 - val_accuracy: 0.7255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.8993\n",
      "Epoch 00021: val_loss did not improve from 0.41361\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.84314 to 0.90196, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 31s 493ms/step - loss: 0.2791 - accuracy: 0.8993 - val_loss: 0.4783 - val_accuracy: 0.9020\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.9124\n",
      "Epoch 00022: val_loss improved from 0.41361 to 0.30182, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 31s 496ms/step - loss: 0.2476 - accuracy: 0.9124 - val_loss: 0.3018 - val_accuracy: 0.8824\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2770 - accuracy: 0.9064\n",
      "Epoch 00023: val_loss did not improve from 0.30182\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.2770 - accuracy: 0.9064 - val_loss: 1.5915 - val_accuracy: 0.6471\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.9049\n",
      "Epoch 00024: val_loss did not improve from 0.30182\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.2670 - accuracy: 0.9049 - val_loss: 0.9930 - val_accuracy: 0.7647\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9379\n",
      "Epoch 00025: val_loss did not improve from 0.30182\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.2050 - accuracy: 0.9379 - val_loss: 0.7270 - val_accuracy: 0.8235\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2271 - accuracy: 0.9269\n",
      "Epoch 00026: val_loss improved from 0.30182 to 0.13594, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.90196 to 0.94118, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 43s 690ms/step - loss: 0.2271 - accuracy: 0.9269 - val_loss: 0.1359 - val_accuracy: 0.9412\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9339\n",
      "Epoch 00027: val_loss did not improve from 0.13594\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 17s 275ms/step - loss: 0.1968 - accuracy: 0.9339 - val_loss: 0.4149 - val_accuracy: 0.8824\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9049\n",
      "Epoch 00028: val_loss did not improve from 0.13594\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.2703 - accuracy: 0.9049 - val_loss: 0.4132 - val_accuracy: 0.8431\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.9234\n",
      "Epoch 00029: val_loss did not improve from 0.13594\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.2043 - accuracy: 0.9234 - val_loss: 0.4031 - val_accuracy: 0.8431\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9444\n",
      "Epoch 00030: val_loss improved from 0.13594 to 0.04887, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.94118 to 0.98039, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 44s 691ms/step - loss: 0.1553 - accuracy: 0.9444 - val_loss: 0.0489 - val_accuracy: 0.9804\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9599\n",
      "Epoch 00031: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 17s 275ms/step - loss: 0.1108 - accuracy: 0.9599 - val_loss: 0.2877 - val_accuracy: 0.8824\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9514\n",
      "Epoch 00032: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.1313 - accuracy: 0.9514 - val_loss: 0.3347 - val_accuracy: 0.8627\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9554\n",
      "Epoch 00033: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.1292 - accuracy: 0.9554 - val_loss: 0.4891 - val_accuracy: 0.8824\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9359\n",
      "Epoch 00034: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1820 - accuracy: 0.9359 - val_loss: 0.3624 - val_accuracy: 0.8431\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9299\n",
      "Epoch 00035: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2163 - accuracy: 0.9299 - val_loss: 0.5026 - val_accuracy: 0.8039\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9174\n",
      "Epoch 00036: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2574 - accuracy: 0.9174 - val_loss: 3.7064 - val_accuracy: 0.4510\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9634\n",
      "Epoch 00037: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1106 - accuracy: 0.9634 - val_loss: 0.4259 - val_accuracy: 0.9216\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9624\n",
      "Epoch 00038: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1060 - accuracy: 0.9624 - val_loss: 0.2598 - val_accuracy: 0.9020\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9479\n",
      "Epoch 00039: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1474 - accuracy: 0.9479 - val_loss: 0.5078 - val_accuracy: 0.8824\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.9459\n",
      "Epoch 00040: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1593 - accuracy: 0.9459 - val_loss: 0.3276 - val_accuracy: 0.9020\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9544\n",
      "Epoch 00041: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1211 - accuracy: 0.9544 - val_loss: 0.2106 - val_accuracy: 0.9412\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9675\n",
      "Epoch 00042: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0803 - accuracy: 0.9675 - val_loss: 0.2706 - val_accuracy: 0.9412\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9810\n",
      "Epoch 00043: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0620 - accuracy: 0.9810 - val_loss: 0.2082 - val_accuracy: 0.9412\n",
      "Epoch 44/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9835\n",
      "Epoch 00044: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0587 - accuracy: 0.9835 - val_loss: 0.1898 - val_accuracy: 0.9412\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9760\n",
      "Epoch 00045: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0817 - accuracy: 0.9760 - val_loss: 0.1717 - val_accuracy: 0.9608\n",
      "Epoch 46/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9614\n",
      "Epoch 00046: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1420 - accuracy: 0.9614 - val_loss: 0.3668 - val_accuracy: 0.8824\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9429\n",
      "Epoch 00047: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1917 - accuracy: 0.9429 - val_loss: 0.2576 - val_accuracy: 0.9216\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1111 - accuracy: 0.9644\n",
      "Epoch 00048: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1111 - accuracy: 0.9644 - val_loss: 0.2879 - val_accuracy: 0.9020\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9675\n",
      "Epoch 00049: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1033 - accuracy: 0.9675 - val_loss: 0.4747 - val_accuracy: 0.9020\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9780\n",
      "Epoch 00050: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0717 - accuracy: 0.9780 - val_loss: 0.2960 - val_accuracy: 0.9216\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9730\n",
      "Epoch 00051: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0672 - accuracy: 0.9730 - val_loss: 0.4166 - val_accuracy: 0.8824\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9750\n",
      "Epoch 00052: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0642 - accuracy: 0.9750 - val_loss: 0.1192 - val_accuracy: 0.9412\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9755\n",
      "Epoch 00053: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0849 - accuracy: 0.9755 - val_loss: 0.3633 - val_accuracy: 0.8627\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9634\n",
      "Epoch 00054: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1012 - accuracy: 0.9634 - val_loss: 0.8339 - val_accuracy: 0.8627\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9624\n",
      "Epoch 00055: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1160 - accuracy: 0.9624 - val_loss: 0.7853 - val_accuracy: 0.7451\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9509\n",
      "Epoch 00056: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1616 - accuracy: 0.9509 - val_loss: 0.7875 - val_accuracy: 0.8431\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9529\n",
      "Epoch 00057: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1454 - accuracy: 0.9529 - val_loss: 0.4601 - val_accuracy: 0.8431\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9700\n",
      "Epoch 00058: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0795 - accuracy: 0.9700 - val_loss: 0.3987 - val_accuracy: 0.9412\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9750\n",
      "Epoch 00059: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0776 - accuracy: 0.9750 - val_loss: 0.3355 - val_accuracy: 0.8824\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9624\n",
      "Epoch 00060: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1386 - accuracy: 0.9624 - val_loss: 0.7579 - val_accuracy: 0.8627\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9609\n",
      "Epoch 00061: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1358 - accuracy: 0.9609 - val_loss: 0.5016 - val_accuracy: 0.9020\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9795\n",
      "Epoch 00062: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0689 - accuracy: 0.9795 - val_loss: 0.6995 - val_accuracy: 0.8627\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9770\n",
      "Epoch 00063: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 0.5003 - val_accuracy: 0.9216\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9820\n",
      "Epoch 00064: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0572 - accuracy: 0.9820 - val_loss: 0.1410 - val_accuracy: 0.9412\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9890\n",
      "Epoch 00065: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0376 - accuracy: 0.9890 - val_loss: 0.2719 - val_accuracy: 0.9216\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9945\n",
      "Epoch 00066: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0193 - accuracy: 0.9945 - val_loss: 0.5608 - val_accuracy: 0.9216\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9910\n",
      "Epoch 00067: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0368 - accuracy: 0.9910 - val_loss: 0.4250 - val_accuracy: 0.9020\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9609\n",
      "Epoch 00068: val_loss did not improve from 0.04887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1355 - accuracy: 0.9609 - val_loss: 0.7397 - val_accuracy: 0.8431\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9790\n",
      "Epoch 00069: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0688 - accuracy: 0.9790 - val_loss: 0.4401 - val_accuracy: 0.9020\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9594\n",
      "Epoch 00070: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1416 - accuracy: 0.9594 - val_loss: 0.3177 - val_accuracy: 0.9216\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9594\n",
      "Epoch 00071: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1281 - accuracy: 0.9594 - val_loss: 0.6207 - val_accuracy: 0.8235\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.9299\n",
      "Epoch 00072: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2821 - accuracy: 0.9299 - val_loss: 249.7348 - val_accuracy: 0.1373\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9254\n",
      "Epoch 00073: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2465 - accuracy: 0.9254 - val_loss: 3.8261 - val_accuracy: 0.5490\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9389\n",
      "Epoch 00074: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2050 - accuracy: 0.9389 - val_loss: 1.1591 - val_accuracy: 0.7059\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9810\n",
      "Epoch 00075: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0524 - accuracy: 0.9810 - val_loss: 0.5718 - val_accuracy: 0.8627\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9750\n",
      "Epoch 00076: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0699 - accuracy: 0.9750 - val_loss: 0.2387 - val_accuracy: 0.9804\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9895\n",
      "Epoch 00077: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0301 - accuracy: 0.9895 - val_loss: 0.2909 - val_accuracy: 0.9412\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9855\n",
      "Epoch 00078: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0510 - accuracy: 0.9855 - val_loss: 0.3839 - val_accuracy: 0.9608\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9765\n",
      "Epoch 00079: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0964 - accuracy: 0.9765 - val_loss: 1.1712 - val_accuracy: 0.8431\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9850\n",
      "Epoch 00080: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0433 - accuracy: 0.9850 - val_loss: 0.2438 - val_accuracy: 0.9412\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9805\n",
      "Epoch 00081: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0550 - accuracy: 0.9805 - val_loss: 0.3480 - val_accuracy: 0.9412\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9825\n",
      "Epoch 00082: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0475 - accuracy: 0.9825 - val_loss: 0.3168 - val_accuracy: 0.9412\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9644\n",
      "Epoch 00083: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1204 - accuracy: 0.9644 - val_loss: 0.3796 - val_accuracy: 0.9412\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9675\n",
      "Epoch 00084: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0997 - accuracy: 0.9675 - val_loss: 0.4465 - val_accuracy: 0.9216\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9790\n",
      "Epoch 00085: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0649 - accuracy: 0.9790 - val_loss: 0.2518 - val_accuracy: 0.9412\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9815\n",
      "Epoch 00086: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0531 - accuracy: 0.9815 - val_loss: 0.1733 - val_accuracy: 0.9412\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9840\n",
      "Epoch 00087: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0577 - accuracy: 0.9840 - val_loss: 0.3394 - val_accuracy: 0.9020\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9785\n",
      "Epoch 00088: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0608 - accuracy: 0.9785 - val_loss: 0.5192 - val_accuracy: 0.9412\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9920\n",
      "Epoch 00089: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 0.1449 - val_accuracy: 0.9216\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9594\n",
      "Epoch 00090: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1372 - accuracy: 0.9594 - val_loss: 0.3050 - val_accuracy: 0.8824\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9549\n",
      "Epoch 00091: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1524 - accuracy: 0.9549 - val_loss: 0.7100 - val_accuracy: 0.8824\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9730\n",
      "Epoch 00092: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1004 - accuracy: 0.9730 - val_loss: 0.5473 - val_accuracy: 0.8627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9780\n",
      "Epoch 00093: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0758 - accuracy: 0.9780 - val_loss: 0.5020 - val_accuracy: 0.8627\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9855\n",
      "Epoch 00094: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0356 - accuracy: 0.9855 - val_loss: 0.2559 - val_accuracy: 0.9216\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9885\n",
      "Epoch 00095: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0341 - accuracy: 0.9885 - val_loss: 0.2054 - val_accuracy: 0.9216\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9930\n",
      "Epoch 00096: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0152 - accuracy: 0.9930 - val_loss: 0.4100 - val_accuracy: 0.9020\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9965\n",
      "Epoch 00097: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 0.4051 - val_accuracy: 0.9412\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9985\n",
      "Epoch 00098: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.3418 - val_accuracy: 0.9608\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00099: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3698 - val_accuracy: 0.9608\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 00100: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.2217 - val_accuracy: 0.9608\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2901e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.2901e-04 - accuracy: 1.0000 - val_loss: 0.5054 - val_accuracy: 0.9608\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 00102: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.3036 - val_accuracy: 0.9412\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9105e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.9105e-04 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9608\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0634e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.0634e-04 - accuracy: 1.0000 - val_loss: 0.3141 - val_accuracy: 0.9608\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.2578e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.2578e-04 - accuracy: 1.0000 - val_loss: 0.4051 - val_accuracy: 0.9608\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.6851e-04 - accuracy: 1.0000\n",
      "Epoch 00106: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.6851e-04 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 0.9608\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9452e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9452e-04 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 0.9608\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5704e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5704e-04 - accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9608\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2990e-04 - accuracy: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.2990e-04 - accuracy: 1.0000 - val_loss: 0.2207 - val_accuracy: 0.9608\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4019e-04 - accuracy: 1.0000\n",
      "Epoch 00110: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4019e-04 - accuracy: 1.0000 - val_loss: 0.2734 - val_accuracy: 0.9608\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.9293e-05 - accuracy: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.9293e-05 - accuracy: 1.0000 - val_loss: 0.2471 - val_accuracy: 0.9608\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.7060e-05 - accuracy: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.7060e-05 - accuracy: 1.0000 - val_loss: 0.4026 - val_accuracy: 0.9608\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0204e-04 - accuracy: 1.0000\n",
      "Epoch 00113: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.0204e-04 - accuracy: 1.0000 - val_loss: 0.2788 - val_accuracy: 0.9412\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.4689e-05 - accuracy: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.4689e-05 - accuracy: 1.0000 - val_loss: 0.3612 - val_accuracy: 0.9412\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.4863e-05 - accuracy: 1.0000\n",
      "Epoch 00115: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.4863e-05 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9412\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9910\n",
      "Epoch 00116: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0396 - accuracy: 0.9910 - val_loss: 1.7829 - val_accuracy: 0.8039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7424 - accuracy: 0.8558\n",
      "Epoch 00117: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.7424 - accuracy: 0.8558 - val_loss: 1.7986 - val_accuracy: 0.7451\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9279\n",
      "Epoch 00118: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2484 - accuracy: 0.9279 - val_loss: 0.7470 - val_accuracy: 0.8627\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9670\n",
      "Epoch 00119: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1001 - accuracy: 0.9670 - val_loss: 0.3972 - val_accuracy: 0.9020\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9805\n",
      "Epoch 00120: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0468 - accuracy: 0.9805 - val_loss: 0.2052 - val_accuracy: 0.9804\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9870\n",
      "Epoch 00121: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0313 - accuracy: 0.9870 - val_loss: 0.2554 - val_accuracy: 0.9412\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9945\n",
      "Epoch 00122: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0149 - accuracy: 0.9945 - val_loss: 0.4489 - val_accuracy: 0.9020\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9895\n",
      "Epoch 00123: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0350 - accuracy: 0.9895 - val_loss: 0.4371 - val_accuracy: 0.9216\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9820\n",
      "Epoch 00124: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0465 - accuracy: 0.9820 - val_loss: 0.5255 - val_accuracy: 0.8824\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9614\n",
      "Epoch 00125: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1292 - accuracy: 0.9614 - val_loss: 0.3759 - val_accuracy: 0.8824\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9564\n",
      "Epoch 00126: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1461 - accuracy: 0.9564 - val_loss: 0.1362 - val_accuracy: 0.9608\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9840\n",
      "Epoch 00127: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0429 - accuracy: 0.9840 - val_loss: 0.4648 - val_accuracy: 0.9412\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9910\n",
      "Epoch 00128: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0293 - accuracy: 0.9910 - val_loss: 0.2310 - val_accuracy: 0.9608\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9965\n",
      "Epoch 00129: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.3762 - val_accuracy: 0.9608\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9980\n",
      "Epoch 00130: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0053 - accuracy: 0.9980 - val_loss: 0.4536 - val_accuracy: 0.9216\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9960\n",
      "Epoch 00131: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.3366 - val_accuracy: 0.9608\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9955\n",
      "Epoch 00132: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0126 - accuracy: 0.9955 - val_loss: 0.2035 - val_accuracy: 0.9804\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9785\n",
      "Epoch 00133: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0710 - accuracy: 0.9785 - val_loss: 0.4040 - val_accuracy: 0.9216\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9795\n",
      "Epoch 00134: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0632 - accuracy: 0.9795 - val_loss: 0.6366 - val_accuracy: 0.9020\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9544\n",
      "Epoch 00135: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1574 - accuracy: 0.9544 - val_loss: 0.5601 - val_accuracy: 0.8627\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9750\n",
      "Epoch 00136: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0883 - accuracy: 0.9750 - val_loss: 0.1784 - val_accuracy: 0.9412\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9885\n",
      "Epoch 00137: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 0.3664 - val_accuracy: 0.9412\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9905\n",
      "Epoch 00138: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0286 - accuracy: 0.9905 - val_loss: 0.3949 - val_accuracy: 0.9608\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9985\n",
      "Epoch 00139: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.1250 - val_accuracy: 0.9608\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9915\n",
      "Epoch 00140: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0274 - accuracy: 0.9915 - val_loss: 0.3432 - val_accuracy: 0.9216\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9950\n",
      "Epoch 00141: val_loss did not improve from 0.04887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0222 - accuracy: 0.9950 - val_loss: 0.3890 - val_accuracy: 0.9412\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9830\n",
      "Epoch 00142: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0509 - accuracy: 0.9830 - val_loss: 0.9128 - val_accuracy: 0.8431\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9865\n",
      "Epoch 00143: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0403 - accuracy: 0.9865 - val_loss: 0.5465 - val_accuracy: 0.9608\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9910\n",
      "Epoch 00144: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0274 - accuracy: 0.9910 - val_loss: 0.1891 - val_accuracy: 0.9216\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9925\n",
      "Epoch 00145: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0344 - accuracy: 0.9925 - val_loss: 0.4143 - val_accuracy: 0.9412\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9945\n",
      "Epoch 00146: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0227 - accuracy: 0.9945 - val_loss: 0.2914 - val_accuracy: 0.9412\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9785\n",
      "Epoch 00147: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0888 - accuracy: 0.9785 - val_loss: 0.3054 - val_accuracy: 0.9216\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9695\n",
      "Epoch 00148: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1057 - accuracy: 0.9695 - val_loss: 0.2040 - val_accuracy: 0.9412\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9765\n",
      "Epoch 00149: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0844 - accuracy: 0.9765 - val_loss: 0.1073 - val_accuracy: 0.9608\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9780\n",
      "Epoch 00150: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0940 - accuracy: 0.9780 - val_loss: 0.6821 - val_accuracy: 0.9020\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9875\n",
      "Epoch 00151: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0409 - accuracy: 0.9875 - val_loss: 0.7966 - val_accuracy: 0.9020\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9975\n",
      "Epoch 00152: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.1926 - val_accuracy: 0.9804\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9970\n",
      "Epoch 00153: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0064 - accuracy: 0.9970 - val_loss: 0.3205 - val_accuracy: 0.9608\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9960\n",
      "Epoch 00154: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0177 - accuracy: 0.9960 - val_loss: 0.0598 - val_accuracy: 0.9804\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9985\n",
      "Epoch 00155: val_loss did not improve from 0.04887\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.0773 - val_accuracy: 0.9608\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9990\n",
      "Epoch 00156: val_loss improved from 0.04887 to 0.01909, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00156: val_accuracy improved from 0.98039 to 1.00000, saving model to ./AI_models\\02_04_AI_val_accuracy_index_14.h5\n",
      "63/63 [==============================] - 44s 696ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0191 - val_accuracy: 1.0000\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.2108e-04 - accuracy: 1.0000\n",
      "Epoch 00157: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 17s 275ms/step - loss: 5.2108e-04 - accuracy: 1.0000 - val_loss: 0.2339 - val_accuracy: 0.9608\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0236e-04 - accuracy: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 3.0236e-04 - accuracy: 1.0000 - val_loss: 0.2900 - val_accuracy: 0.9608\n",
      "Epoch 159/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.7374e-04 - accuracy: 1.0000\n",
      "Epoch 00159: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 4.7374e-04 - accuracy: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.9412\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3959e-04 - accuracy: 1.0000\n",
      "Epoch 00160: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3959e-04 - accuracy: 1.0000 - val_loss: 0.2458 - val_accuracy: 0.9804\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1184e-04 - accuracy: 1.0000\n",
      "Epoch 00161: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.1184e-04 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9608\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5375e-04 - accuracy: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.5375e-04 - accuracy: 1.0000 - val_loss: 0.3318 - val_accuracy: 0.9608\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4700e-04 - accuracy: 1.0000\n",
      "Epoch 00163: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4700e-04 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9412\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.8088e-04 - accuracy: 1.0000\n",
      "Epoch 00164: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.8088e-04 - accuracy: 1.0000 - val_loss: 0.5392 - val_accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9950\n",
      "Epoch 00165: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0175 - accuracy: 0.9950 - val_loss: 0.2403 - val_accuracy: 0.9020\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9945\n",
      "Epoch 00166: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0148 - accuracy: 0.9945 - val_loss: 0.2102 - val_accuracy: 0.9804\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9860\n",
      "Epoch 00167: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0593 - accuracy: 0.9860 - val_loss: 0.2853 - val_accuracy: 0.9412\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9584\n",
      "Epoch 00168: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1815 - accuracy: 0.9584 - val_loss: 0.7457 - val_accuracy: 0.8824\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9820\n",
      "Epoch 00169: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0664 - accuracy: 0.9820 - val_loss: 0.7172 - val_accuracy: 0.9216\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9895\n",
      "Epoch 00170: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0335 - accuracy: 0.9895 - val_loss: 0.5475 - val_accuracy: 0.9412\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9885\n",
      "Epoch 00171: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0449 - accuracy: 0.9885 - val_loss: 0.3537 - val_accuracy: 0.9412\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9920\n",
      "Epoch 00172: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0245 - accuracy: 0.9920 - val_loss: 0.3698 - val_accuracy: 0.9412\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9935\n",
      "Epoch 00173: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.4317 - val_accuracy: 0.9020\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00174: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.1994 - val_accuracy: 0.9804\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7350e-04 - accuracy: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.7350e-04 - accuracy: 1.0000 - val_loss: 0.1740 - val_accuracy: 0.9804\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 00176: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.1746 - val_accuracy: 0.9804\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9990\n",
      "Epoch 00177: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0026 - accuracy: 0.9990 - val_loss: 0.6389 - val_accuracy: 0.9608\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9985\n",
      "Epoch 00178: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0033 - accuracy: 0.9985 - val_loss: 0.5760 - val_accuracy: 0.9608\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00179: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.9608\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 00180: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0964 - val_accuracy: 0.9608\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9970\n",
      "Epoch 00181: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0114 - accuracy: 0.9970 - val_loss: 0.8094 - val_accuracy: 0.9020\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9880\n",
      "Epoch 00182: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0343 - accuracy: 0.9880 - val_loss: 0.1925 - val_accuracy: 0.9804\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9795\n",
      "Epoch 00183: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0901 - accuracy: 0.9795 - val_loss: 0.2509 - val_accuracy: 0.9216\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9895\n",
      "Epoch 00184: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0306 - accuracy: 0.9895 - val_loss: 0.0529 - val_accuracy: 0.9804\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9965\n",
      "Epoch 00185: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0138 - accuracy: 0.9965 - val_loss: 0.4191 - val_accuracy: 0.9608\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9975\n",
      "Epoch 00186: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0052 - accuracy: 0.9975 - val_loss: 0.2658 - val_accuracy: 0.9804\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9930\n",
      "Epoch 00187: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0266 - accuracy: 0.9930 - val_loss: 0.7040 - val_accuracy: 0.9020\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9925\n",
      "Epoch 00188: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0272 - accuracy: 0.9925 - val_loss: 0.2333 - val_accuracy: 0.9020\n",
      "Epoch 189/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9910\n",
      "Epoch 00189: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0430 - accuracy: 0.9910 - val_loss: 0.1140 - val_accuracy: 0.9608\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9910\n",
      "Epoch 00190: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0432 - accuracy: 0.9910 - val_loss: 0.1503 - val_accuracy: 0.9412\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9845\n",
      "Epoch 00191: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0515 - accuracy: 0.9845 - val_loss: 0.6805 - val_accuracy: 0.8824\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9910\n",
      "Epoch 00192: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0311 - accuracy: 0.9910 - val_loss: 0.3754 - val_accuracy: 0.9608\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9980\n",
      "Epoch 00193: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0038 - accuracy: 0.9980 - val_loss: 0.2400 - val_accuracy: 0.9608\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9980\n",
      "Epoch 00194: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 0.3650 - val_accuracy: 0.9216\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9980\n",
      "Epoch 00195: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.1521 - val_accuracy: 0.9608\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9960\n",
      "Epoch 00196: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 0.1795 - val_accuracy: 0.9412\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9950\n",
      "Epoch 00197: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0200 - accuracy: 0.9950 - val_loss: 0.4544 - val_accuracy: 0.9608\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9905\n",
      "Epoch 00198: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0293 - accuracy: 0.9905 - val_loss: 0.5618 - val_accuracy: 0.9412\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9965\n",
      "Epoch 00199: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.3537 - val_accuracy: 0.9608\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9960\n",
      "Epoch 00200: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0219 - accuracy: 0.9960 - val_loss: 0.2659 - val_accuracy: 0.9804\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9920\n",
      "Epoch 00201: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00201: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0240 - accuracy: 0.9920 - val_loss: 0.7633 - val_accuracy: 0.9216\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9930\n",
      "Epoch 00202: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00202: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0169 - accuracy: 0.9930 - val_loss: 0.1932 - val_accuracy: 0.9412\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9875\n",
      "Epoch 00203: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00203: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0402 - accuracy: 0.9875 - val_loss: 0.5463 - val_accuracy: 0.8824\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9905\n",
      "Epoch 00204: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00204: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0372 - accuracy: 0.9905 - val_loss: 0.7705 - val_accuracy: 0.8824\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9795\n",
      "Epoch 00205: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00205: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0781 - accuracy: 0.9795 - val_loss: 1.1280 - val_accuracy: 0.8431\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9855\n",
      "Epoch 00206: val_loss did not improve from 0.01909\n",
      "\n",
      "Epoch 00206: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0459 - accuracy: 0.9855 - val_loss: 0.2482 - val_accuracy: 0.9216\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9950\n",
      "Epoch 00207: val_loss improved from 0.01909 to 0.01865, saving model to ./AI_models\\02_04_AI_val_loss_index_14.h5\n",
      "\n",
      "Epoch 00207: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 31s 491ms/step - loss: 0.0243 - accuracy: 0.9950 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9945\n",
      "Epoch 00208: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00208: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 0.1923 - val_accuracy: 0.9608\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9925\n",
      "Epoch 00209: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00209: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.0277 - accuracy: 0.9925 - val_loss: 0.3653 - val_accuracy: 0.9216\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9950\n",
      "Epoch 00210: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00210: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0141 - accuracy: 0.9950 - val_loss: 0.2842 - val_accuracy: 0.9608\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9980\n",
      "Epoch 00211: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00211: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0710 - val_accuracy: 0.9804\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 00212: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00212: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.1622 - val_accuracy: 0.9804\n",
      "Epoch 213/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 00213: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00213: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.1677 - val_accuracy: 0.9608\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.2299e-04 - accuracy: 1.0000\n",
      "Epoch 00214: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00214: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.2299e-04 - accuracy: 1.0000 - val_loss: 0.2318 - val_accuracy: 0.9608\n",
      "Epoch 215/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7204e-04 - accuracy: 1.0000\n",
      "Epoch 00215: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00215: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.7204e-04 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9804\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 00216: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00216: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.7430 - val_accuracy: 0.9412\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9955\n",
      "Epoch 00217: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00217: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 0.1798 - val_accuracy: 0.9804\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9915\n",
      "Epoch 00218: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00218: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0289 - accuracy: 0.9915 - val_loss: 0.3679 - val_accuracy: 0.8824\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9835\n",
      "Epoch 00219: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00219: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1000 - accuracy: 0.9835 - val_loss: 2.9573 - val_accuracy: 0.7451\n",
      "Epoch 220/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9895\n",
      "Epoch 00220: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00220: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0382 - accuracy: 0.9895 - val_loss: 0.8138 - val_accuracy: 0.9216\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9925\n",
      "Epoch 00221: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00221: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0241 - accuracy: 0.9925 - val_loss: 0.2508 - val_accuracy: 0.9804\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9950\n",
      "Epoch 00222: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00222: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.6388 - val_accuracy: 0.9412\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9920\n",
      "Epoch 00223: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00223: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0307 - accuracy: 0.9920 - val_loss: 0.5877 - val_accuracy: 0.9412\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9955\n",
      "Epoch 00224: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00224: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0178 - accuracy: 0.9955 - val_loss: 0.0859 - val_accuracy: 0.9804\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9965\n",
      "Epoch 00225: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00225: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0157 - accuracy: 0.9965 - val_loss: 0.5934 - val_accuracy: 0.9412\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9970\n",
      "Epoch 00226: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00226: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 0.0266 - val_accuracy: 0.9804\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9985\n",
      "Epoch 00227: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00227: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.3568 - val_accuracy: 0.9412\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9940\n",
      "Epoch 00228: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00228: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0278 - accuracy: 0.9940 - val_loss: 0.5264 - val_accuracy: 0.8824\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9975\n",
      "Epoch 00229: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00229: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.7909 - val_accuracy: 0.8627\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 00230: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00230: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.9030 - val_accuracy: 0.9216\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.2304e-04 - accuracy: 1.0000\n",
      "Epoch 00231: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00231: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.2304e-04 - accuracy: 1.0000 - val_loss: 0.8595 - val_accuracy: 0.9412\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 00232: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00232: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.8076 - val_accuracy: 0.9412\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9980\n",
      "Epoch 00233: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00233: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.5617 - val_accuracy: 0.9412\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9960\n",
      "Epoch 00234: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00234: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0084 - accuracy: 0.9960 - val_loss: 0.4422 - val_accuracy: 0.9608\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9920\n",
      "Epoch 00235: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00235: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0251 - accuracy: 0.9920 - val_loss: 0.2782 - val_accuracy: 0.9020\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9900\n",
      "Epoch 00236: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00236: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0397 - accuracy: 0.9900 - val_loss: 3.4634 - val_accuracy: 0.7843\n",
      "Epoch 237/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.1899 - accuracy: 0.9690\n",
      "Epoch 00237: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00237: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1899 - accuracy: 0.9690 - val_loss: 7.1899 - val_accuracy: 0.5686\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9790\n",
      "Epoch 00238: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00238: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0802 - accuracy: 0.9790 - val_loss: 0.7479 - val_accuracy: 0.8824\n",
      "Epoch 239/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9915\n",
      "Epoch 00239: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00239: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0344 - accuracy: 0.9915 - val_loss: 0.4686 - val_accuracy: 0.9412\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9925\n",
      "Epoch 00240: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00240: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0165 - accuracy: 0.9925 - val_loss: 0.8705 - val_accuracy: 0.9412\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9940\n",
      "Epoch 00241: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00241: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0209 - accuracy: 0.9940 - val_loss: 0.2160 - val_accuracy: 0.9608\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9975\n",
      "Epoch 00242: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00242: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0130 - accuracy: 0.9975 - val_loss: 0.4387 - val_accuracy: 0.9412\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9910\n",
      "Epoch 00243: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00243: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0351 - accuracy: 0.9910 - val_loss: 0.6698 - val_accuracy: 0.9216\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9975\n",
      "Epoch 00244: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00244: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0123 - accuracy: 0.9975 - val_loss: 0.1241 - val_accuracy: 0.9608\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9970\n",
      "Epoch 00245: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00245: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.3657 - val_accuracy: 0.9216\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8936e-04 - accuracy: 1.0000\n",
      "Epoch 00246: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00246: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.8936e-04 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 0.9608\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.0888e-04 - accuracy: 1.0000\n",
      "Epoch 00247: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00247: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.0888e-04 - accuracy: 1.0000 - val_loss: 0.4998 - val_accuracy: 0.9608\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0949e-04 - accuracy: 1.0000\n",
      "Epoch 00248: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00248: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0949e-04 - accuracy: 1.0000 - val_loss: 0.3962 - val_accuracy: 0.9608\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5368e-04 - accuracy: 1.0000\n",
      "Epoch 00249: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00249: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5368e-04 - accuracy: 1.0000 - val_loss: 0.1705 - val_accuracy: 0.9412\n",
      "Epoch 250/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.9056e-04 - accuracy: 1.0000\n",
      "Epoch 00250: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00250: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.9056e-04 - accuracy: 1.0000 - val_loss: 0.2346 - val_accuracy: 0.9608\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3832e-04 - accuracy: 1.0000\n",
      "Epoch 00251: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00251: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3832e-04 - accuracy: 1.0000 - val_loss: 0.1396 - val_accuracy: 0.9804\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.2349e-05 - accuracy: 1.0000\n",
      "Epoch 00252: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00252: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 6.2349e-05 - accuracy: 1.0000 - val_loss: 0.2626 - val_accuracy: 0.9608\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.5513e-05 - accuracy: 1.0000\n",
      "Epoch 00253: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00253: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.5513e-05 - accuracy: 1.0000 - val_loss: 0.2455 - val_accuracy: 0.9608\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.0479e-05 - accuracy: 1.0000\n",
      "Epoch 00254: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00254: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.0479e-05 - accuracy: 1.0000 - val_loss: 0.2163 - val_accuracy: 0.9412\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6246e-05 - accuracy: 1.0000\n",
      "Epoch 00255: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00255: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.6246e-05 - accuracy: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.9608\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.9257e-04 - accuracy: 1.0000\n",
      "Epoch 00256: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00256: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.9257e-04 - accuracy: 1.0000 - val_loss: 0.1396 - val_accuracy: 0.9608\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8838e-05 - accuracy: 1.0000\n",
      "Epoch 00257: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00257: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.8838e-05 - accuracy: 1.0000 - val_loss: 0.3178 - val_accuracy: 0.9608\n",
      "Epoch 258/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.8865e-05 - accuracy: 1.0000\n",
      "Epoch 00258: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00258: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.8865e-05 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9412\n",
      "Epoch 259/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2232e-05 - accuracy: 1.0000\n",
      "Epoch 00259: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00259: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.2232e-05 - accuracy: 1.0000 - val_loss: 0.3880 - val_accuracy: 0.9412\n",
      "Epoch 260/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.0762e-05 - accuracy: 1.0000\n",
      "Epoch 00260: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00260: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.0762e-05 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.9046e-05 - accuracy: 1.0000\n",
      "Epoch 00261: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00261: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.9046e-05 - accuracy: 1.0000 - val_loss: 0.4743 - val_accuracy: 0.9412\n",
      "Epoch 262/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6954e-05 - accuracy: 1.0000\n",
      "Epoch 00262: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00262: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6954e-05 - accuracy: 1.0000 - val_loss: 0.2142 - val_accuracy: 0.9804\n",
      "Epoch 263/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0135e-05 - accuracy: 1.0000\n",
      "Epoch 00263: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00263: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.0135e-05 - accuracy: 1.0000 - val_loss: 0.3467 - val_accuracy: 0.9412\n",
      "Epoch 264/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.3822e-05 - accuracy: 1.0000\n",
      "Epoch 00264: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00264: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.3822e-05 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9608\n",
      "Epoch 265/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7506e-05 - accuracy: 1.0000\n",
      "Epoch 00265: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00265: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.7506e-05 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 0.9608\n",
      "Epoch 266/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5299e-05 - accuracy: 1.0000\n",
      "Epoch 00266: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00266: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.5299e-05 - accuracy: 1.0000 - val_loss: 0.2697 - val_accuracy: 0.9608\n",
      "Epoch 267/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.3421e-05 - accuracy: 1.0000\n",
      "Epoch 00267: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00267: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.3421e-05 - accuracy: 1.0000 - val_loss: 0.2324 - val_accuracy: 0.9608\n",
      "Epoch 268/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3332e-05 - accuracy: 1.0000\n",
      "Epoch 00268: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00268: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3332e-05 - accuracy: 1.0000 - val_loss: 0.5289 - val_accuracy: 0.9412\n",
      "Epoch 269/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7788e-05 - accuracy: 1.0000\n",
      "Epoch 00269: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00269: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7788e-05 - accuracy: 1.0000 - val_loss: 0.2634 - val_accuracy: 0.9412\n",
      "Epoch 270/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6933e-05 - accuracy: 1.0000\n",
      "Epoch 00270: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00270: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6933e-05 - accuracy: 1.0000 - val_loss: 0.2669 - val_accuracy: 0.9608\n",
      "Epoch 271/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.9858e-05 - accuracy: 1.0000\n",
      "Epoch 00271: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00271: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.9858e-05 - accuracy: 1.0000 - val_loss: 0.2892 - val_accuracy: 0.9608\n",
      "Epoch 272/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3830e-05 - accuracy: 1.0000\n",
      "Epoch 00272: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00272: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.3830e-05 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 0.9412\n",
      "Epoch 273/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3815e-05 - accuracy: 1.0000\n",
      "Epoch 00273: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00273: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.3815e-05 - accuracy: 1.0000 - val_loss: 0.2711 - val_accuracy: 0.9412\n",
      "Epoch 274/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7415e-05 - accuracy: 1.0000\n",
      "Epoch 00274: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00274: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7415e-05 - accuracy: 1.0000 - val_loss: 0.4187 - val_accuracy: 0.9412\n",
      "Epoch 275/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0358e-05 - accuracy: 1.0000\n",
      "Epoch 00275: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00275: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0358e-05 - accuracy: 1.0000 - val_loss: 0.4588 - val_accuracy: 0.9608\n",
      "Epoch 276/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5121e-05 - accuracy: 1.0000\n",
      "Epoch 00276: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00276: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5121e-05 - accuracy: 1.0000 - val_loss: 0.5257 - val_accuracy: 0.9412\n",
      "Epoch 277/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4089e-05 - accuracy: 1.0000\n",
      "Epoch 00277: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00277: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4089e-05 - accuracy: 1.0000 - val_loss: 0.5469 - val_accuracy: 0.9412\n",
      "Epoch 278/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3030e-05 - accuracy: 1.0000\n",
      "Epoch 00278: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00278: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3030e-05 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9804\n",
      "Epoch 279/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1878e-05 - accuracy: 1.0000\n",
      "Epoch 00279: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00279: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1878e-05 - accuracy: 1.0000 - val_loss: 0.4002 - val_accuracy: 0.9608\n",
      "Epoch 280/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2730e-05 - accuracy: 1.0000\n",
      "Epoch 00280: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00280: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2730e-05 - accuracy: 1.0000 - val_loss: 0.3386 - val_accuracy: 0.9608\n",
      "Epoch 281/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1933e-05 - accuracy: 1.0000\n",
      "Epoch 00281: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00281: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1933e-05 - accuracy: 1.0000 - val_loss: 0.2837 - val_accuracy: 0.9608\n",
      "Epoch 282/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1023e-05 - accuracy: 1.0000\n",
      "Epoch 00282: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00282: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.1023e-05 - accuracy: 1.0000 - val_loss: 0.1992 - val_accuracy: 0.9804\n",
      "Epoch 283/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5150e-05 - accuracy: 1.0000\n",
      "Epoch 00283: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00283: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5150e-05 - accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.9804\n",
      "Epoch 284/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.6123e-06 - accuracy: 1.0000\n",
      "Epoch 00284: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00284: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 8.6123e-06 - accuracy: 1.0000 - val_loss: 0.3970 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.2474e-06 - accuracy: 1.0000\n",
      "Epoch 00285: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00285: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.2474e-06 - accuracy: 1.0000 - val_loss: 0.3030 - val_accuracy: 0.9608\n",
      "Epoch 286/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0636e-05 - accuracy: 1.0000\n",
      "Epoch 00286: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00286: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0636e-05 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9804\n",
      "Epoch 287/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.0253e-06 - accuracy: 1.0000\n",
      "Epoch 00287: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00287: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.0253e-06 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 0.9804\n",
      "Epoch 288/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.2637e-06 - accuracy: 1.0000\n",
      "Epoch 00288: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00288: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.2637e-06 - accuracy: 1.0000 - val_loss: 0.1338 - val_accuracy: 0.9608\n",
      "Epoch 289/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.7369e-06 - accuracy: 1.0000\n",
      "Epoch 00289: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00289: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.7369e-06 - accuracy: 1.0000 - val_loss: 0.1931 - val_accuracy: 0.9804\n",
      "Epoch 290/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.4337e-06 - accuracy: 1.0000\n",
      "Epoch 00290: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00290: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.4337e-06 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9608\n",
      "Epoch 291/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.6595e-06 - accuracy: 1.0000\n",
      "Epoch 00291: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00291: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.6595e-06 - accuracy: 1.0000 - val_loss: 0.1731 - val_accuracy: 0.9608\n",
      "Epoch 292/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.8323e-06 - accuracy: 1.0000\n",
      "Epoch 00292: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00292: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.8323e-06 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 0.9804\n",
      "Epoch 293/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.5764e-06 - accuracy: 1.0000\n",
      "Epoch 00293: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00293: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.5764e-06 - accuracy: 1.0000 - val_loss: 0.2996 - val_accuracy: 0.9608\n",
      "Epoch 294/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1247e-05 - accuracy: 1.0000\n",
      "Epoch 00294: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00294: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.1247e-05 - accuracy: 1.0000 - val_loss: 0.3954 - val_accuracy: 0.9608\n",
      "Epoch 295/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2143e-05 - accuracy: 1.0000\n",
      "Epoch 00295: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00295: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2143e-05 - accuracy: 1.0000 - val_loss: 0.1284 - val_accuracy: 0.9608\n",
      "Epoch 296/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.1310e-06 - accuracy: 1.0000\n",
      "Epoch 00296: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00296: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.1310e-06 - accuracy: 1.0000 - val_loss: 0.2619 - val_accuracy: 0.9412\n",
      "Epoch 297/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.7166e-06 - accuracy: 1.0000\n",
      "Epoch 00297: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00297: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.7166e-06 - accuracy: 1.0000 - val_loss: 0.3872 - val_accuracy: 0.9412\n",
      "Epoch 298/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6023e-06 - accuracy: 1.0000\n",
      "Epoch 00298: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00298: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.6023e-06 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 0.9608\n",
      "Epoch 299/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.2757e-06 - accuracy: 1.0000\n",
      "Epoch 00299: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00299: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.2757e-06 - accuracy: 1.0000 - val_loss: 0.2815 - val_accuracy: 0.9412\n",
      "Epoch 300/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.0368e-06 - accuracy: 1.0000\n",
      "Epoch 00300: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00300: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.0368e-06 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9804\n",
      "Epoch 301/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7878e-06 - accuracy: 1.0000\n",
      "Epoch 00301: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00301: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.7878e-06 - accuracy: 1.0000 - val_loss: 0.3080 - val_accuracy: 0.9608\n",
      "Epoch 302/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7119e-06 - accuracy: 1.0000\n",
      "Epoch 00302: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00302: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.7119e-06 - accuracy: 1.0000 - val_loss: 0.4490 - val_accuracy: 0.9412\n",
      "Epoch 303/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.1639e-06 - accuracy: 1.0000\n",
      "Epoch 00303: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00303: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.1639e-06 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9804\n",
      "Epoch 304/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.7742e-06 - accuracy: 1.0000\n",
      "Epoch 00304: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00304: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.7742e-06 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9608\n",
      "Epoch 305/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.7531e-06 - accuracy: 1.0000\n",
      "Epoch 00305: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00305: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.7531e-06 - accuracy: 1.0000 - val_loss: 0.4514 - val_accuracy: 0.9412\n",
      "Epoch 306/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.3625e-06 - accuracy: 1.0000\n",
      "Epoch 00306: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00306: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.3625e-06 - accuracy: 1.0000 - val_loss: 0.4553 - val_accuracy: 0.9608\n",
      "Epoch 307/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.6722e-06 - accuracy: 1.0000\n",
      "Epoch 00307: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00307: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.6722e-06 - accuracy: 1.0000 - val_loss: 0.2652 - val_accuracy: 0.9412\n",
      "Epoch 308/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0577e-06 - accuracy: 1.0000\n",
      "Epoch 00308: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00308: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.0577e-06 - accuracy: 1.0000 - val_loss: 0.1465 - val_accuracy: 0.9804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8286e-06 - accuracy: 1.0000\n",
      "Epoch 00309: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00309: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.8286e-06 - accuracy: 1.0000 - val_loss: 0.2880 - val_accuracy: 0.9608\n",
      "Epoch 310/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.7941e-06 - accuracy: 1.0000\n",
      "Epoch 00310: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00310: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.7941e-06 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.9608\n",
      "Epoch 311/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.1976e-06 - accuracy: 1.0000\n",
      "Epoch 00311: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00311: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.1976e-06 - accuracy: 1.0000 - val_loss: 0.2842 - val_accuracy: 0.9608\n",
      "Epoch 312/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.5741e-06 - accuracy: 1.0000\n",
      "Epoch 00312: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00312: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.5741e-06 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 0.9804\n",
      "Epoch 313/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.3437e-06 - accuracy: 1.0000\n",
      "Epoch 00313: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00313: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.3437e-06 - accuracy: 1.0000 - val_loss: 0.2587 - val_accuracy: 0.9412\n",
      "Epoch 314/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.7458e-06 - accuracy: 1.0000\n",
      "Epoch 00314: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00314: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.7458e-06 - accuracy: 1.0000 - val_loss: 0.1418 - val_accuracy: 0.9608\n",
      "Epoch 315/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.4292e-06 - accuracy: 1.0000\n",
      "Epoch 00315: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00315: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.4292e-06 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 0.9412\n",
      "Epoch 316/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.2890e-06 - accuracy: 1.0000\n",
      "Epoch 00316: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00316: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.2890e-06 - accuracy: 1.0000 - val_loss: 0.3001 - val_accuracy: 0.9608\n",
      "Epoch 317/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6153e-06 - accuracy: 1.0000\n",
      "Epoch 00317: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00317: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6153e-06 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9412\n",
      "Epoch 318/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.4271e-06 - accuracy: 1.0000\n",
      "Epoch 00318: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00318: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.4271e-06 - accuracy: 1.0000 - val_loss: 0.2894 - val_accuracy: 0.9608\n",
      "Epoch 319/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9824e-06 - accuracy: 1.0000\n",
      "Epoch 00319: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00319: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.9824e-06 - accuracy: 1.0000 - val_loss: 0.3929 - val_accuracy: 0.9412\n",
      "Epoch 320/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5069e-06 - accuracy: 1.0000\n",
      "Epoch 00320: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00320: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.5069e-06 - accuracy: 1.0000 - val_loss: 0.2916 - val_accuracy: 0.9608\n",
      "Epoch 321/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4400e-05 - accuracy: 1.0000\n",
      "Epoch 00321: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00321: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.4400e-05 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.9608\n",
      "Epoch 322/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.9254\n",
      "Epoch 00322: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00322: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.4675 - accuracy: 0.9254 - val_loss: 11.4435 - val_accuracy: 0.6078\n",
      "Epoch 323/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9379\n",
      "Epoch 00323: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00323: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2691 - accuracy: 0.9379 - val_loss: 0.3329 - val_accuracy: 0.9412\n",
      "Epoch 324/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9835\n",
      "Epoch 00324: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00324: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0800 - accuracy: 0.9835 - val_loss: 0.2004 - val_accuracy: 0.9608\n",
      "Epoch 325/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9920\n",
      "Epoch 00325: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00325: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0218 - accuracy: 0.9920 - val_loss: 0.1339 - val_accuracy: 0.9412\n",
      "Epoch 326/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9955\n",
      "Epoch 00326: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00326: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0136 - accuracy: 0.9955 - val_loss: 0.1214 - val_accuracy: 0.9608\n",
      "Epoch 327/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9955\n",
      "Epoch 00327: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00327: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00327: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.1103 - val_accuracy: 0.9412\n",
      "Epoch 328/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 00328: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00328: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.2798 - val_accuracy: 0.9412\n",
      "Epoch 329/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 00329: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00329: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.2731 - val_accuracy: 0.9216\n",
      "Epoch 330/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.0270e-04 - accuracy: 1.0000\n",
      "Epoch 00330: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00330: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.0270e-04 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9412\n",
      "Epoch 331/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5487e-04 - accuracy: 1.0000\n",
      "Epoch 00331: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00331: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.5487e-04 - accuracy: 1.0000 - val_loss: 0.3699 - val_accuracy: 0.9412\n",
      "Epoch 332/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6030e-04 - accuracy: 1.0000\n",
      "Epoch 00332: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00332: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 3.6030e-04 - accuracy: 1.0000 - val_loss: 0.3833 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7083e-04 - accuracy: 1.0000\n",
      "Epoch 00333: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00333: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.7083e-04 - accuracy: 1.0000 - val_loss: 0.1768 - val_accuracy: 0.9412\n",
      "Epoch 334/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.6109e-04 - accuracy: 1.0000\n",
      "Epoch 00334: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00334: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.6109e-04 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9412\n",
      "Epoch 335/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9995\n",
      "Epoch 00335: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00335: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0010 - accuracy: 0.9995 - val_loss: 0.3047 - val_accuracy: 0.9412\n",
      "Epoch 336/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9995\n",
      "Epoch 00336: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00336: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0053 - accuracy: 0.9995 - val_loss: 0.0382 - val_accuracy: 0.9804\n",
      "Epoch 337/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3350e-04 - accuracy: 1.0000\n",
      "Epoch 00337: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00337: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 3.3350e-04 - accuracy: 1.0000 - val_loss: 0.1270 - val_accuracy: 0.9608\n",
      "Epoch 338/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.4361e-04 - accuracy: 1.0000\n",
      "Epoch 00338: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00338: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.4361e-04 - accuracy: 1.0000 - val_loss: 0.2070 - val_accuracy: 0.9216\n",
      "Epoch 339/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8419e-04 - accuracy: 1.0000\n",
      "Epoch 00339: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00339: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.8419e-04 - accuracy: 1.0000 - val_loss: 0.3198 - val_accuracy: 0.9412\n",
      "Epoch 340/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3070e-04 - accuracy: 1.0000\n",
      "Epoch 00340: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00340: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3070e-04 - accuracy: 1.0000 - val_loss: 0.1121 - val_accuracy: 0.9412\n",
      "Epoch 341/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8166e-04 - accuracy: 1.0000\n",
      "Epoch 00341: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00341: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.8166e-04 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9608\n",
      "Epoch 342/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2914e-04 - accuracy: 1.0000\n",
      "Epoch 00342: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00342: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2914e-04 - accuracy: 1.0000 - val_loss: 0.2137 - val_accuracy: 0.9608\n",
      "Epoch 343/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1470e-04 - accuracy: 1.0000\n",
      "Epoch 00343: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00343: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.1470e-04 - accuracy: 1.0000 - val_loss: 0.2122 - val_accuracy: 0.9608\n",
      "Epoch 344/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0701e-04 - accuracy: 1.0000\n",
      "Epoch 00344: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00344: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.0701e-04 - accuracy: 1.0000 - val_loss: 0.1712 - val_accuracy: 0.9608\n",
      "Epoch 345/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.0945e-05 - accuracy: 1.0000\n",
      "Epoch 00345: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00345: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.0945e-05 - accuracy: 1.0000 - val_loss: 0.1094 - val_accuracy: 0.9412\n",
      "Epoch 346/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0587e-04 - accuracy: 1.0000\n",
      "Epoch 00346: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00346: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0587e-04 - accuracy: 1.0000 - val_loss: 0.4623 - val_accuracy: 0.9216\n",
      "Epoch 347/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.9695e-05 - accuracy: 1.0000\n",
      "Epoch 00347: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00347: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.9695e-05 - accuracy: 1.0000 - val_loss: 0.4535 - val_accuracy: 0.9216\n",
      "Epoch 348/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.6995e-05 - accuracy: 1.0000\n",
      "Epoch 00348: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00348: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 7.6995e-05 - accuracy: 1.0000 - val_loss: 0.3206 - val_accuracy: 0.9608\n",
      "Epoch 349/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.3024e-05 - accuracy: 1.0000\n",
      "Epoch 00349: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00349: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 9.3024e-05 - accuracy: 1.0000 - val_loss: 0.2985 - val_accuracy: 0.9412\n",
      "Epoch 350/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.1851e-05 - accuracy: 1.0000\n",
      "Epoch 00350: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00350: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.1851e-05 - accuracy: 1.0000 - val_loss: 0.3269 - val_accuracy: 0.9412\n",
      "Epoch 351/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.6774e-05 - accuracy: 1.0000\n",
      "Epoch 00351: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00351: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.6774e-05 - accuracy: 1.0000 - val_loss: 0.2434 - val_accuracy: 0.9608\n",
      "Epoch 352/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.8116e-05 - accuracy: 1.0000\n",
      "Epoch 00352: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00352: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.8116e-05 - accuracy: 1.0000 - val_loss: 0.2189 - val_accuracy: 0.9608\n",
      "Epoch 353/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0299e-04 - accuracy: 1.0000\n",
      "Epoch 00353: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00353: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0299e-04 - accuracy: 1.0000 - val_loss: 0.1741 - val_accuracy: 0.9608\n",
      "Epoch 354/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0560e-04 - accuracy: 1.0000\n",
      "Epoch 00354: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00354: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0560e-04 - accuracy: 1.0000 - val_loss: 0.2266 - val_accuracy: 0.9412\n",
      "Epoch 355/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.4450e-05 - accuracy: 1.0000\n",
      "Epoch 00355: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00355: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.4450e-05 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9608\n",
      "Epoch 356/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.4685e-05 - accuracy: 1.0000\n",
      "Epoch 00356: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00356: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 8.4685e-05 - accuracy: 1.0000 - val_loss: 0.2219 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9960\n",
      "Epoch 00357: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00357: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0124 - accuracy: 0.9960 - val_loss: 0.4182 - val_accuracy: 0.9216\n",
      "Epoch 358/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9840\n",
      "Epoch 00358: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00358: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0644 - accuracy: 0.9840 - val_loss: 0.5472 - val_accuracy: 0.9020\n",
      "Epoch 359/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9755\n",
      "Epoch 00359: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00359: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0989 - accuracy: 0.9755 - val_loss: 0.0847 - val_accuracy: 0.9804\n",
      "Epoch 360/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9895\n",
      "Epoch 00360: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00360: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0336 - accuracy: 0.9895 - val_loss: 0.0964 - val_accuracy: 0.9804\n",
      "Epoch 361/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9975\n",
      "Epoch 00361: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00361: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.1094 - val_accuracy: 0.9804\n",
      "Epoch 362/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9985\n",
      "Epoch 00362: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00362: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0034 - accuracy: 0.9985 - val_loss: 0.0553 - val_accuracy: 0.9804\n",
      "Epoch 363/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9950\n",
      "Epoch 00363: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00363: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0261 - accuracy: 0.9950 - val_loss: 0.1066 - val_accuracy: 0.9608\n",
      "Epoch 364/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9950\n",
      "Epoch 00364: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00364: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.1661 - val_accuracy: 0.9412\n",
      "Epoch 365/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9975\n",
      "Epoch 00365: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00365: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0130 - accuracy: 0.9975 - val_loss: 0.2997 - val_accuracy: 0.9608\n",
      "Epoch 366/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9940\n",
      "Epoch 00366: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00366: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0208 - accuracy: 0.9940 - val_loss: 0.4051 - val_accuracy: 0.9608\n",
      "Epoch 367/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9950\n",
      "Epoch 00367: val_loss did not improve from 0.01865\n",
      "\n",
      "Epoch 00367: val_accuracy did not improve from 1.00000\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0116 - accuracy: 0.9950 - val_loss: 0.0866 - val_accuracy: 0.9804\n",
      "14  번째 학습을 완료했습니다.\n",
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 8s - loss: 8.6424 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0931s vs `on_train_batch_end` time: 0.1834s). Check your callbacks.\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2304 - accuracy: 0.1002\n",
      "Epoch 00001: val_loss improved from inf to 10704.61523, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.09804, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 23s 369ms/step - loss: 4.2304 - accuracy: 0.1002 - val_loss: 10704.6152 - val_accuracy: 0.0980\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2860 - accuracy: 0.1342\n",
      "Epoch 00002: val_loss improved from 10704.61523 to 7.64382, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.09804 to 0.11765, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 23s 369ms/step - loss: 2.2860 - accuracy: 0.1342 - val_loss: 7.6438 - val_accuracy: 0.1176\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1257 - accuracy: 0.2253\n",
      "Epoch 00003: val_loss improved from 7.64382 to 2.57626, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 318ms/step - loss: 2.1257 - accuracy: 0.2253 - val_loss: 2.5763 - val_accuracy: 0.1176\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8590 - accuracy: 0.3300\n",
      "Epoch 00004: val_loss did not improve from 2.57626\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 1.8590 - accuracy: 0.3300 - val_loss: 3.1282 - val_accuracy: 0.0784\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6018 - accuracy: 0.4457\n",
      "Epoch 00005: val_loss did not improve from 2.57626\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.6018 - accuracy: 0.4457 - val_loss: 4.2038 - val_accuracy: 0.1176\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2549 - accuracy: 0.5548\n",
      "Epoch 00006: val_loss did not improve from 2.57626\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2549 - accuracy: 0.5548 - val_loss: 60.2926 - val_accuracy: 0.1176\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0247 - accuracy: 0.6485\n",
      "Epoch 00007: val_loss did not improve from 2.57626\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.11765 to 0.13725, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 31s 494ms/step - loss: 1.0247 - accuracy: 0.6485 - val_loss: 49.8136 - val_accuracy: 0.1373\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9135 - accuracy: 0.6920\n",
      "Epoch 00008: val_loss did not improve from 2.57626\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.13725\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.9135 - accuracy: 0.6920 - val_loss: 31.2432 - val_accuracy: 0.0980\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7422 - accuracy: 0.7611\n",
      "Epoch 00009: val_loss did not improve from 2.57626\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.13725\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.7422 - accuracy: 0.7611 - val_loss: 5.6391 - val_accuracy: 0.1176\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.7962\n",
      "Epoch 00010: val_loss did not improve from 2.57626\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.13725\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.6045 - accuracy: 0.7962 - val_loss: 3.4868 - val_accuracy: 0.1373\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.8172\n",
      "Epoch 00011: val_loss improved from 2.57626 to 2.08837, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.13725 to 0.49020, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 46s 733ms/step - loss: 0.5282 - accuracy: 0.8172 - val_loss: 2.0884 - val_accuracy: 0.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4855 - accuracy: 0.8373\n",
      "Epoch 00012: val_loss improved from 2.08837 to 1.94751, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.49020 to 0.56863, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 23s 365ms/step - loss: 0.4855 - accuracy: 0.8373 - val_loss: 1.9475 - val_accuracy: 0.5686\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.8363\n",
      "Epoch 00013: val_loss improved from 1.94751 to 1.13810, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.56863 to 0.72549, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 23s 365ms/step - loss: 0.4932 - accuracy: 0.8363 - val_loss: 1.1381 - val_accuracy: 0.7255\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4426 - accuracy: 0.8503\n",
      "Epoch 00014: val_loss improved from 1.13810 to 0.78940, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.72549 to 0.74510, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 23s 369ms/step - loss: 0.4426 - accuracy: 0.8503 - val_loss: 0.7894 - val_accuracy: 0.7451\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.8838\n",
      "Epoch 00015: val_loss improved from 0.78940 to 0.58129, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.74510 to 0.80392, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 23s 367ms/step - loss: 0.3421 - accuracy: 0.8838 - val_loss: 0.5813 - val_accuracy: 0.8039\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.8848\n",
      "Epoch 00016: val_loss did not improve from 0.58129\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.80392\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.3521 - accuracy: 0.8848 - val_loss: 1.5762 - val_accuracy: 0.7059\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.8963\n",
      "Epoch 00017: val_loss did not improve from 0.58129\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.80392\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.3021 - accuracy: 0.8963 - val_loss: 0.8118 - val_accuracy: 0.7647\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.9054\n",
      "Epoch 00018: val_loss did not improve from 0.58129\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.80392\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2644 - accuracy: 0.9054 - val_loss: 0.7220 - val_accuracy: 0.7843\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9134\n",
      "Epoch 00019: val_loss did not improve from 0.58129\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.80392\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2685 - accuracy: 0.9134 - val_loss: 0.7580 - val_accuracy: 0.7647\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.9129\n",
      "Epoch 00020: val_loss improved from 0.58129 to 0.46173, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.80392 to 0.88235, saving model to ./AI_models\\02_04_AI_val_accuracy_index_15.h5\n",
      "63/63 [==============================] - 44s 697ms/step - loss: 0.2524 - accuracy: 0.9129 - val_loss: 0.4617 - val_accuracy: 0.8824\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.9314\n",
      "Epoch 00021: val_loss did not improve from 0.46173\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 17s 275ms/step - loss: 0.2216 - accuracy: 0.9314 - val_loss: 0.9306 - val_accuracy: 0.7647\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9299\n",
      "Epoch 00022: val_loss improved from 0.46173 to 0.41214, saving model to ./AI_models\\02_04_AI_val_loss_index_15.h5\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 32s 502ms/step - loss: 0.1977 - accuracy: 0.9299 - val_loss: 0.4121 - val_accuracy: 0.8824\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.8998\n",
      "Epoch 00023: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.2768 - accuracy: 0.8998 - val_loss: 0.5670 - val_accuracy: 0.8039\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9219\n",
      "Epoch 00024: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.2163 - accuracy: 0.9219 - val_loss: 1.4555 - val_accuracy: 0.7059\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9359\n",
      "Epoch 00025: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.1766 - accuracy: 0.9359 - val_loss: 0.5835 - val_accuracy: 0.8627\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1845 - accuracy: 0.9349\n",
      "Epoch 00026: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.1845 - accuracy: 0.9349 - val_loss: 0.7540 - val_accuracy: 0.7451\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9524\n",
      "Epoch 00027: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1365 - accuracy: 0.9524 - val_loss: 0.4597 - val_accuracy: 0.8431\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9589\n",
      "Epoch 00028: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1096 - accuracy: 0.9589 - val_loss: 0.4868 - val_accuracy: 0.8235\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.8938\n",
      "Epoch 00029: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.3362 - accuracy: 0.8938 - val_loss: 5.7156 - val_accuracy: 0.5686\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.8888\n",
      "Epoch 00030: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.3662 - accuracy: 0.8888 - val_loss: 3.9920 - val_accuracy: 0.6275\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9284\n",
      "Epoch 00031: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2000 - accuracy: 0.9284 - val_loss: 0.7603 - val_accuracy: 0.8431\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9469\n",
      "Epoch 00032: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1331 - accuracy: 0.9469 - val_loss: 0.9450 - val_accuracy: 0.8039\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9700\n",
      "Epoch 00033: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0861 - accuracy: 0.9700 - val_loss: 1.5227 - val_accuracy: 0.7647\n",
      "Epoch 34/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9549\n",
      "Epoch 00034: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1292 - accuracy: 0.9549 - val_loss: 0.7054 - val_accuracy: 0.8431\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9599\n",
      "Epoch 00035: val_loss did not improve from 0.41214\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1292 - accuracy: 0.9599 - val_loss: 1.2077 - val_accuracy: 0.7255\n",
      "Epoch 36/2000\n",
      "13/63 [=====>........................] - ETA: 13s - loss: 0.0859 - accuracy: 0.9615"
     ]
    }
   ],
   "source": [
    "index=1\n",
    "result = 0\n",
    "for train_index,val_index in kfold.split(x,y):\n",
    "    if index<=13:\n",
    "        index+=1\n",
    "        continue\n",
    "        \n",
    "    modelpath = './AI_models/02_04_AI_val_loss_index_{}.h5'.format(index)\n",
    "    modelpath2 = './AI_models/02_04_AI_val_accuracy_index_{}.h5'.format(index)\n",
    "    cp = ModelCheckpoint(monitor = 'val_loss',filepath=modelpath,save_best_only=True,verbose=1)\n",
    "    cp2 = ModelCheckpoint(monitor = 'val_accuracy',filepath=modelpath2,save_best_only=True,verbose=1)\n",
    "\n",
    "\n",
    "    x_train = x[train_index]\n",
    "    x_val = x[val_index]\n",
    "    y_train = y[train_index]\n",
    "    y_val = y[val_index]\n",
    "\n",
    "    onehot = OneHotEncoder()\n",
    "    y_train = onehot.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "    y_val = onehot.fit_transform(y_val.reshape(-1,1)).toarray().astype('float32')\n",
    "\n",
    "    train_generator = datagen.flow(x_train,y_train,batch_size=32)\n",
    "    val_generator = datagen.flow(x_val,y_val)\n",
    "    model = modeling()\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n",
    "    model.fit_generator(train_generator,validation_data = val_generator,epochs=epochs,callbacks=[cp,es,reLR,cp2])\n",
    "\n",
    "    model = load_model(modelpath)\n",
    "    model2 = load_model(modelpath2)\n",
    "    df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "    x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred2 = model2.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred,axis=-1)\n",
    "    y_pred2 = np.argmax(y_pred2,axis=-1)\n",
    "    df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "    df_sub['digit'] = y_pred\n",
    "    df_sub.to_csv('./AI_models/loss_kfold_{}.csv'.format(index))\n",
    "    df_sub['digit'] = y_pred2\n",
    "    df_sub.to_csv('./AI_models/accuracy_kfold_{}.csv'.format(index))\n",
    "\n",
    "    print(index, \" 번째 학습을 완료했습니다.\")\n",
    "    index+=1\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "0.905\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onOYBeVy-Mkb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KswTfNSi-Mkc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "0.87\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx1Z4nhC-Mkc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "44FNCmNG-Mkd"
   },
   "outputs": [],
   "source": [
    "model = load_model('./models/02_03_imger_best_index_1.h5')\n",
    "df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzM0rw6U-Mke",
    "outputId": "0312f408-37af-44e5-b673-febb0961614c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "binary_model = []\n",
    "for i in range(0,10):\n",
    "    print(i)\n",
    "    model = load_model('./binary_models/{}_binary.h5'.format(i))\n",
    "    binary_model.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QwofhPdW-Mke"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "def ordering(array):\n",
    "    temp = array.copy()\n",
    "    result = []\n",
    "    for i in range(len(temp)):\n",
    "        sol = np.argmax(temp)\n",
    "        result.append(sol)\n",
    "        temp[sol]=0\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fI2mNTi-Mkf",
    "outputId": "f253edd1-103b-41f5-8832-f5ac667f5a38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6569414e-08, 2.2260668e-08, 4.6740688e-06, ..., 2.5446711e-10,\n",
       "        5.9703314e-07, 5.4656005e-11],\n",
       "       [2.9817595e-16, 2.0647546e-12, 4.4424861e-16, ..., 2.6480063e-10,\n",
       "        8.9294266e-11, 1.0000000e+00],\n",
       "       [2.4707546e-05, 1.3310514e-01, 8.9886552e-03, ..., 9.1972132e-04,\n",
       "        1.2284001e-02, 2.9377347e-06],\n",
       "       ...,\n",
       "       [8.6063210e-09, 7.5784501e-10, 2.6377617e-10, ..., 3.1178827e-13,\n",
       "        1.2005766e-07, 6.1626551e-15],\n",
       "       [1.0054513e-03, 2.9064235e-01, 1.6038346e-05, ..., 1.7447629e-06,\n",
       "        8.1512779e-03, 4.5916289e-03],\n",
       "       [9.9825722e-01, 6.9838888e-16, 2.4161539e-10, ..., 1.8471900e-15,\n",
       "        6.6174334e-13, 1.4705076e-15]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy1domP0-Mkf",
    "outputId": "db3b1dcb-6d43-4fc7-9938-34990934eb64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, ..., 6, 5, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_notyet = np.argmax(y_pred,axis=-1)\n",
    "y_notyet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1xQykxG-Mkf",
    "outputId": "946d6597-cef3-4297-f226-1a1349f5005d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.88120088e-36],\n",
       "       [1.13777095e-36]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model[3].predict(x_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULE3KyHS-Mkg",
    "outputId": "41ead2b9-7d2e-44a9-a6ba-e8c4645752cd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cc4ab4411252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Jhbsfx-Mkg"
   },
   "outputs": [],
   "source": [
    "k=729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPmflg5c-Mkg",
    "outputId": "05282ba4-d754-4522-aa33-73d47b49b612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래모델 :  9 \n",
      "원래모델확률 :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] \n",
      "바이너리 :  1.6463632e-24\n",
      "0 0.0\n",
      "1 5.6854813e-21\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 3.455557e-05\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 1.2415284e-22\n",
      "9 1.6463632e-24\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "for i in [k]:\n",
    "    temp_result = y_notyet[i]\n",
    "    binary_result = binary_model[temp_result].predict(np.array([x_test[i].reshape(28,28,1)]))\n",
    "    a=np.round(y_pred[i],3)\n",
    "    print(\"원래모델 : \",temp_result,'\\n원래모델확률 : ',a,\"\\n바이너리 : \",binary_result[0][0])\n",
    "    print(\"0\",binary_model[0].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"1\",binary_model[1].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"2\",binary_model[2].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"3\",binary_model[3].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"4\",binary_model[4].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"5\",binary_model[5].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"6\",binary_model[6].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"7\",binary_model[7].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"8\",binary_model[8].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"9\",binary_model[9].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEgcogTL-Mkg",
    "outputId": "b1e9adb4-4593-4bda-ae06-bb49fd8bf6ac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-LIIBisa-Mkh",
    "outputId": "aa584c54-1643-48af-eb35-d853835f2d2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8, 8, ..., 6, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred,axis=-1)\n",
    "df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "df_sub['digit'] = y_pred\n",
    "df_sub.to_csv('test_4.csv')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT0ZAdKJ-Mkh",
    "outputId": "4f04bdd5-32cc-4226-9a6a-19a6aa77a588"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 8, ..., 6, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TjhvZQW-Mkh",
    "outputId": "4b83ad9f-6588-4c0e-c152-b4310a2c4f32"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=5\n",
    "df = pd.read_csv(\"preprocessing_150.csv\",index_col=[0])\n",
    "df2 = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "plt.imshow(df.values[k,2:].reshape(28,28).astype('float32'))\n",
    "plt.show()\n",
    "df2 = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "plt.imshow(df2.values[k,2:].reshape(28,28).astype('float32'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHKAVNiD-Mkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7nEeGDn-Mkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc_-Flz2-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wImlSOL--Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj2wn75b-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7YkDa_J-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6a0jyE7-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jYY0jU-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4NiWL-c-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzhsBOPW-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDPP4Lp2-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wsmnbLS-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4vmV_gu-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxBZ_-66-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBoKPs_E-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwaS-Ang-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5_Acvtq-Mkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7pmh6Ej-Mkk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
