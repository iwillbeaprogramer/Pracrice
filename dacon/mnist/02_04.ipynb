{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ooCpvRmT-MkR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dropout,Input,Activation,Dense\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GysUTZTp-MkX"
   },
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s4kX312e-MkY"
   },
   "outputs": [],
   "source": [
    "def modeling():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = inputs\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = _x\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048)(x)\n",
    "    x = Dense(10,activation='softmax')(x)\n",
    "    outputs=x\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cccyJfWO-MkY"
   },
   "source": [
    "# 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W73nGvZM-MkZ"
   },
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "es = EarlyStopping(monitor='val_loss',patience=160)\n",
    "reLR = ReduceLROnPlateau(patience=120,verbose=1,factor=0.8)\n",
    "kfold = StratifiedKFold(n_splits=40,random_state=42,shuffle=True)\n",
    "\n",
    "datagen = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "datagen2 = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWqLv-B1-MkZ"
   },
   "source": [
    "# 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j6UbQKZk-MkZ",
    "outputId": "26984b86-65c4-4adf-c0dc-bcb09f17d346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 28, 28, 1) (2048,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 9, 0, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "y = df.values[:,0].astype('int32')\n",
    "x = df.values[:,2:].astype('float32')/255.0\n",
    "# print(x.shape,y.shape)               # (2048, 28, 28) (2048,)\n",
    "#onehot = OneHotEncoder()\n",
    "#y = onehot.fit_transform(y.reshape(-1,1)).toarray().astype('float32')\n",
    "x = x.reshape(-1,28,28,1)\n",
    "# x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.15)\n",
    "# x_train = x_train.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# x_val = x_val.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# print(x_train.shape,x_val.shape,y_train.shape,y_val.shape)\n",
    "print(x.shape,y.shape) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LmMESLTT-Mkb",
    "outputId": "6e97efa1-d705-4b3d-9e71-4261745da5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-ed23b405cc77>:27: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 8s - loss: 13.4456 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0998s vs `on_train_batch_end` time: 0.1860s). Check your callbacks.\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8022 - accuracy: 0.1142\n",
      "Epoch 00001: val_loss improved from inf to 1344.52539, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.11765, saving model to ./AI_models\\02_04_AI_val_accuracy_index_17.h5\n",
      "63/63 [==============================] - 25s 397ms/step - loss: 4.8022 - accuracy: 0.1142 - val_loss: 1344.5254 - val_accuracy: 0.1176\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2545 - accuracy: 0.1502\n",
      "Epoch 00002: val_loss improved from 1344.52539 to 2.98320, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 321ms/step - loss: 2.2545 - accuracy: 0.1502 - val_loss: 2.9832 - val_accuracy: 0.0784\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0544 - accuracy: 0.2394\n",
      "Epoch 00003: val_loss improved from 2.98320 to 2.59639, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 322ms/step - loss: 2.0544 - accuracy: 0.2394 - val_loss: 2.5964 - val_accuracy: 0.1176\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9045 - accuracy: 0.3220\n",
      "Epoch 00004: val_loss did not improve from 2.59639\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9045 - accuracy: 0.3220 - val_loss: 3.0429 - val_accuracy: 0.0784\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6793 - accuracy: 0.4081\n",
      "Epoch 00005: val_loss did not improve from 2.59639\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 1.6793 - accuracy: 0.4081 - val_loss: 3.1261 - val_accuracy: 0.0980\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4308 - accuracy: 0.5288\n",
      "Epoch 00006: val_loss did not improve from 2.59639\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.4308 - accuracy: 0.5288 - val_loss: 3.5202 - val_accuracy: 0.0980\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0751 - accuracy: 0.6324\n",
      "Epoch 00007: val_loss did not improve from 2.59639\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.0751 - accuracy: 0.6324 - val_loss: 4.3052 - val_accuracy: 0.0980\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8853 - accuracy: 0.7021\n",
      "Epoch 00008: val_loss did not improve from 2.59639\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.8853 - accuracy: 0.7021 - val_loss: 3.3217 - val_accuracy: 0.0980\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7262 - accuracy: 0.7576\n",
      "Epoch 00009: val_loss did not improve from 2.59639\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.7262 - accuracy: 0.7576 - val_loss: 3.9899 - val_accuracy: 0.0980\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.8097\n",
      "Epoch 00010: val_loss improved from 2.59639 to 2.33267, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.11765 to 0.43137, saving model to ./AI_models\\02_04_AI_val_accuracy_index_17.h5\n",
      "63/63 [==============================] - 50s 789ms/step - loss: 0.5653 - accuracy: 0.8097 - val_loss: 2.3327 - val_accuracy: 0.4314\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.7987\n",
      "Epoch 00011: val_loss improved from 2.33267 to 2.16934, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.43137\n",
      "63/63 [==============================] - 20s 314ms/step - loss: 0.6125 - accuracy: 0.7987 - val_loss: 2.1693 - val_accuracy: 0.4314\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.8317\n",
      "Epoch 00012: val_loss improved from 2.16934 to 0.52179, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.43137 to 0.80392, saving model to ./AI_models\\02_04_AI_val_accuracy_index_17.h5\n",
      "63/63 [==============================] - 38s 605ms/step - loss: 0.5040 - accuracy: 0.8317 - val_loss: 0.5218 - val_accuracy: 0.8039\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.8453\n",
      "Epoch 00013: val_loss did not improve from 0.52179\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.80392\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.4527 - accuracy: 0.8453 - val_loss: 0.7864 - val_accuracy: 0.7843\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4348 - accuracy: 0.8518\n",
      "Epoch 00014: val_loss improved from 0.52179 to 0.44650, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.80392 to 0.88235, saving model to ./AI_models\\02_04_AI_val_accuracy_index_17.h5\n",
      "63/63 [==============================] - 40s 638ms/step - loss: 0.4348 - accuracy: 0.8518 - val_loss: 0.4465 - val_accuracy: 0.8824\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.8713\n",
      "Epoch 00015: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.3935 - accuracy: 0.8713 - val_loss: 0.5724 - val_accuracy: 0.8824\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.8818\n",
      "Epoch 00016: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.3369 - accuracy: 0.8818 - val_loss: 1.7232 - val_accuracy: 0.6078\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.8993\n",
      "Epoch 00017: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.2999 - accuracy: 0.8993 - val_loss: 0.8433 - val_accuracy: 0.7647\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9064\n",
      "Epoch 00018: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.2874 - accuracy: 0.9064 - val_loss: 1.4147 - val_accuracy: 0.7059\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3022 - accuracy: 0.9019\n",
      "Epoch 00019: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.3022 - accuracy: 0.9019 - val_loss: 7.7525 - val_accuracy: 0.3529\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3225 - accuracy: 0.8953\n",
      "Epoch 00020: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.3225 - accuracy: 0.8953 - val_loss: 1.0178 - val_accuracy: 0.8235\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9184\n",
      "Epoch 00021: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.88235 to 0.90196, saving model to ./AI_models\\02_04_AI_val_accuracy_index_17.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 31s 496ms/step - loss: 0.2487 - accuracy: 0.9184 - val_loss: 0.7006 - val_accuracy: 0.9020\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9134\n",
      "Epoch 00022: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.2506 - accuracy: 0.9134 - val_loss: 0.8093 - val_accuracy: 0.8039\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9269\n",
      "Epoch 00023: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.2244 - accuracy: 0.9269 - val_loss: 1.2029 - val_accuracy: 0.7059\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9459\n",
      "Epoch 00024: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1576 - accuracy: 0.9459 - val_loss: 1.1694 - val_accuracy: 0.8235\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9229\n",
      "Epoch 00025: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2389 - accuracy: 0.9229 - val_loss: 0.7089 - val_accuracy: 0.8824\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9129\n",
      "Epoch 00026: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2662 - accuracy: 0.9129 - val_loss: 0.5587 - val_accuracy: 0.8431\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.9299\n",
      "Epoch 00027: val_loss did not improve from 0.44650\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2174 - accuracy: 0.9299 - val_loss: 1.8115 - val_accuracy: 0.6471\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9474\n",
      "Epoch 00028: val_loss improved from 0.44650 to 0.43907, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.90196 to 0.96078, saving model to ./AI_models\\02_04_AI_val_accuracy_index_17.h5\n",
      "63/63 [==============================] - 46s 732ms/step - loss: 0.1379 - accuracy: 0.9474 - val_loss: 0.4391 - val_accuracy: 0.9608\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9534\n",
      "Epoch 00029: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1334 - accuracy: 0.9534 - val_loss: 1.0697 - val_accuracy: 0.8235\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9574\n",
      "Epoch 00030: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1112 - accuracy: 0.9574 - val_loss: 0.5871 - val_accuracy: 0.9216\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9634\n",
      "Epoch 00031: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1042 - accuracy: 0.9634 - val_loss: 0.8393 - val_accuracy: 0.8235\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9579\n",
      "Epoch 00032: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.1365 - accuracy: 0.9579 - val_loss: 1.0255 - val_accuracy: 0.8431\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9414\n",
      "Epoch 00033: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.1581 - accuracy: 0.9414 - val_loss: 3.3936 - val_accuracy: 0.5490\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9354\n",
      "Epoch 00034: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.1936 - accuracy: 0.9354 - val_loss: 1.0386 - val_accuracy: 0.8235\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9299\n",
      "Epoch 00035: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 0.2244 - accuracy: 0.9299 - val_loss: 0.7560 - val_accuracy: 0.8235\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9629\n",
      "Epoch 00036: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.1049 - accuracy: 0.9629 - val_loss: 0.9766 - val_accuracy: 0.8039\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9524\n",
      "Epoch 00037: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1263 - accuracy: 0.9524 - val_loss: 1.2423 - val_accuracy: 0.8431\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9594\n",
      "Epoch 00038: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1086 - accuracy: 0.9594 - val_loss: 0.6995 - val_accuracy: 0.8824\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9720\n",
      "Epoch 00039: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0997 - accuracy: 0.9720 - val_loss: 0.7511 - val_accuracy: 0.8431\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9379\n",
      "Epoch 00040: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.2147 - accuracy: 0.9379 - val_loss: 1.1602 - val_accuracy: 0.8431\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 0.9344\n",
      "Epoch 00041: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.2027 - accuracy: 0.9344 - val_loss: 1.4757 - val_accuracy: 0.8824\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9589\n",
      "Epoch 00042: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.1121 - accuracy: 0.9589 - val_loss: 0.9707 - val_accuracy: 0.8824\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9494\n",
      "Epoch 00043: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1373 - accuracy: 0.9494 - val_loss: 0.7594 - val_accuracy: 0.9020\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9614\n",
      "Epoch 00044: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1225 - accuracy: 0.9614 - val_loss: 0.6673 - val_accuracy: 0.9020\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9730\n",
      "Epoch 00045: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1011 - accuracy: 0.9730 - val_loss: 0.8301 - val_accuracy: 0.8431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9780\n",
      "Epoch 00046: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0553 - accuracy: 0.9780 - val_loss: 0.5092 - val_accuracy: 0.9608\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9695\n",
      "Epoch 00047: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0994 - accuracy: 0.9695 - val_loss: 0.7867 - val_accuracy: 0.8627\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9489\n",
      "Epoch 00048: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 20s 313ms/step - loss: 0.1840 - accuracy: 0.9489 - val_loss: 1.0774 - val_accuracy: 0.8627\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9599\n",
      "Epoch 00049: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 20s 312ms/step - loss: 0.1307 - accuracy: 0.9599 - val_loss: 0.5663 - val_accuracy: 0.9608\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9745\n",
      "Epoch 00050: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.0721 - accuracy: 0.9745 - val_loss: 0.4760 - val_accuracy: 0.9020\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9780\n",
      "Epoch 00051: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0653 - accuracy: 0.9780 - val_loss: 1.3332 - val_accuracy: 0.8039\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9670\n",
      "Epoch 00052: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0880 - accuracy: 0.9670 - val_loss: 1.2822 - val_accuracy: 0.7843\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9835\n",
      "Epoch 00053: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0494 - accuracy: 0.9835 - val_loss: 0.9466 - val_accuracy: 0.8431\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9755\n",
      "Epoch 00054: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0727 - accuracy: 0.9755 - val_loss: 0.9020 - val_accuracy: 0.8431\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9720\n",
      "Epoch 00055: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.0918 - accuracy: 0.9720 - val_loss: 0.5008 - val_accuracy: 0.8824\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9614\n",
      "Epoch 00056: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1243 - accuracy: 0.9614 - val_loss: 0.6211 - val_accuracy: 0.8824\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9790\n",
      "Epoch 00057: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0756 - accuracy: 0.9790 - val_loss: 0.4579 - val_accuracy: 0.8431\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9675\n",
      "Epoch 00058: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0880 - accuracy: 0.9675 - val_loss: 0.5864 - val_accuracy: 0.8627\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9760\n",
      "Epoch 00059: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0671 - accuracy: 0.9760 - val_loss: 0.8621 - val_accuracy: 0.8235\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9670\n",
      "Epoch 00060: val_loss did not improve from 0.43907\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1010 - accuracy: 0.9670 - val_loss: 0.5496 - val_accuracy: 0.9608\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9785\n",
      "Epoch 00061: val_loss improved from 0.43907 to 0.37597, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 31s 488ms/step - loss: 0.0736 - accuracy: 0.9785 - val_loss: 0.3760 - val_accuracy: 0.9216\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9539\n",
      "Epoch 00062: val_loss did not improve from 0.37597\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.1486 - accuracy: 0.9539 - val_loss: 0.8772 - val_accuracy: 0.8824\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9594\n",
      "Epoch 00063: val_loss did not improve from 0.37597\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1272 - accuracy: 0.9594 - val_loss: 1.1282 - val_accuracy: 0.8039\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9484\n",
      "Epoch 00064: val_loss did not improve from 0.37597\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1689 - accuracy: 0.9484 - val_loss: 0.8844 - val_accuracy: 0.8824\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9664\n",
      "Epoch 00065: val_loss did not improve from 0.37597\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.1102 - accuracy: 0.9664 - val_loss: 0.8510 - val_accuracy: 0.8627\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9775\n",
      "Epoch 00066: val_loss did not improve from 0.37597\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0592 - accuracy: 0.9775 - val_loss: 0.8392 - val_accuracy: 0.8824\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9735\n",
      "Epoch 00067: val_loss did not improve from 0.37597\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0810 - accuracy: 0.9735 - val_loss: 0.9378 - val_accuracy: 0.9216\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.9745\n",
      "Epoch 00068: val_loss did not improve from 0.37597\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0925 - accuracy: 0.9745 - val_loss: 0.8586 - val_accuracy: 0.9020\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9534\n",
      "Epoch 00069: val_loss improved from 0.37597 to 0.24197, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 31s 494ms/step - loss: 0.1566 - accuracy: 0.9534 - val_loss: 0.2420 - val_accuracy: 0.9216\n",
      "Epoch 70/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9614\n",
      "Epoch 00070: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.1349 - accuracy: 0.9614 - val_loss: 0.8800 - val_accuracy: 0.8235\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9690\n",
      "Epoch 00071: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0988 - accuracy: 0.9690 - val_loss: 1.0986 - val_accuracy: 0.8039\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9790\n",
      "Epoch 00072: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0675 - accuracy: 0.9790 - val_loss: 1.1928 - val_accuracy: 0.8824\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9950\n",
      "Epoch 00073: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0179 - accuracy: 0.9950 - val_loss: 0.5690 - val_accuracy: 0.9216\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9940\n",
      "Epoch 00074: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0134 - accuracy: 0.9940 - val_loss: 0.7796 - val_accuracy: 0.9216\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9980\n",
      "Epoch 00075: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.5517 - val_accuracy: 0.9608\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9925\n",
      "Epoch 00076: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0253 - accuracy: 0.9925 - val_loss: 0.4643 - val_accuracy: 0.9216\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9850\n",
      "Epoch 00077: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0543 - accuracy: 0.9850 - val_loss: 0.7150 - val_accuracy: 0.8824\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9720\n",
      "Epoch 00078: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0916 - accuracy: 0.9720 - val_loss: 1.2463 - val_accuracy: 0.8235\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9629\n",
      "Epoch 00079: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1304 - accuracy: 0.9629 - val_loss: 12.1506 - val_accuracy: 0.4314\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9624\n",
      "Epoch 00080: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1202 - accuracy: 0.9624 - val_loss: 0.9851 - val_accuracy: 0.8627\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9619\n",
      "Epoch 00081: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1319 - accuracy: 0.9619 - val_loss: 0.5282 - val_accuracy: 0.8627\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9675\n",
      "Epoch 00082: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0971 - accuracy: 0.9675 - val_loss: 0.7862 - val_accuracy: 0.8627\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9795\n",
      "Epoch 00083: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0735 - accuracy: 0.9795 - val_loss: 1.3643 - val_accuracy: 0.8824\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9705\n",
      "Epoch 00084: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0949 - accuracy: 0.9705 - val_loss: 1.3565 - val_accuracy: 0.8431\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9609\n",
      "Epoch 00085: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.1342 - accuracy: 0.9609 - val_loss: 0.6775 - val_accuracy: 0.8824\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9805\n",
      "Epoch 00086: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.0751 - accuracy: 0.9805 - val_loss: 0.4725 - val_accuracy: 0.9020\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9775\n",
      "Epoch 00087: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0734 - accuracy: 0.9775 - val_loss: 0.5638 - val_accuracy: 0.9020\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9915\n",
      "Epoch 00088: val_loss did not improve from 0.24197\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0241 - accuracy: 0.9915 - val_loss: 0.4436 - val_accuracy: 0.9608\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9895\n",
      "Epoch 00089: val_loss improved from 0.24197 to 0.20702, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 30s 483ms/step - loss: 0.0260 - accuracy: 0.9895 - val_loss: 0.2070 - val_accuracy: 0.9216\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9870\n",
      "Epoch 00090: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0478 - accuracy: 0.9870 - val_loss: 0.5883 - val_accuracy: 0.9216\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9750\n",
      "Epoch 00091: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0780 - accuracy: 0.9750 - val_loss: 1.4633 - val_accuracy: 0.8627\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9685\n",
      "Epoch 00092: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1210 - accuracy: 0.9685 - val_loss: 1.2294 - val_accuracy: 0.8627\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 0.9554\n",
      "Epoch 00093: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2153 - accuracy: 0.9554 - val_loss: 0.8174 - val_accuracy: 0.9216\n",
      "Epoch 94/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9710\n",
      "Epoch 00094: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1087 - accuracy: 0.9710 - val_loss: 0.8002 - val_accuracy: 0.8824\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9905\n",
      "Epoch 00095: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0221 - accuracy: 0.9905 - val_loss: 0.6691 - val_accuracy: 0.9608\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9980\n",
      "Epoch 00096: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.5091 - val_accuracy: 0.9608\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9955\n",
      "Epoch 00097: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0163 - accuracy: 0.9955 - val_loss: 0.4718 - val_accuracy: 0.9412\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9995\n",
      "Epoch 00098: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.5188 - val_accuracy: 0.9216\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 00099: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.4430 - val_accuracy: 0.9608\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00100: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4172 - val_accuracy: 0.9608\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.5293e-04 - accuracy: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 7.5293e-04 - accuracy: 1.0000 - val_loss: 0.4537 - val_accuracy: 0.9608\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3127e-04 - accuracy: 1.0000\n",
      "Epoch 00102: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 3.3127e-04 - accuracy: 1.0000 - val_loss: 0.5277 - val_accuracy: 0.9412\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9131e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9131e-04 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.9608\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9157e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9157e-04 - accuracy: 1.0000 - val_loss: 0.4633 - val_accuracy: 0.9608\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5609e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.5609e-04 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.9412\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4427e-04 - accuracy: 1.0000\n",
      "Epoch 00106: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 1.4427e-04 - accuracy: 1.0000 - val_loss: 0.3888 - val_accuracy: 0.9412\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.3985e-04 - accuracy: 0.9995\n",
      "Epoch 00107: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.3985e-04 - accuracy: 0.9995 - val_loss: 0.5299 - val_accuracy: 0.9412\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 00108: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0016 - accuracy: 0.9990 - val_loss: 0.3993 - val_accuracy: 0.9216\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 0.9995\n",
      "Epoch 00109: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.0010 - accuracy: 0.9995 - val_loss: 0.3653 - val_accuracy: 0.9608\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.9737e-04 - accuracy: 0.9995\n",
      "Epoch 00110: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 7.9737e-04 - accuracy: 0.9995 - val_loss: 0.3505 - val_accuracy: 0.9412\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9990\n",
      "Epoch 00111: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.6145 - val_accuracy: 0.9216\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.4185e-04 - accuracy: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 3.4185e-04 - accuracy: 1.0000 - val_loss: 0.4691 - val_accuracy: 0.9216\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1901e-04 - accuracy: 1.0000\n",
      "Epoch 00113: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 2.1901e-04 - accuracy: 1.0000 - val_loss: 0.5718 - val_accuracy: 0.9412\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1698e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 1.1698e-04 - accuracy: 1.0000 - val_loss: 0.5309 - val_accuracy: 0.9412\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.0628e-05 - accuracy: 1.0000\n",
      "Epoch 00115: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 7.0628e-05 - accuracy: 1.0000 - val_loss: 0.5028 - val_accuracy: 0.9216\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8813e-04 - accuracy: 1.0000\n",
      "Epoch 00116: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 1.8813e-04 - accuracy: 1.0000 - val_loss: 0.7740 - val_accuracy: 0.9216\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9167e-04 - accuracy: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 1.9167e-04 - accuracy: 1.0000 - val_loss: 0.5824 - val_accuracy: 0.9020\n",
      "Epoch 118/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 7.7530e-05 - accuracy: 1.0000\n",
      "Epoch 00118: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 7.7530e-05 - accuracy: 1.0000 - val_loss: 0.7416 - val_accuracy: 0.9216\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.0583e-05 - accuracy: 1.0000\n",
      "Epoch 00119: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 7.0583e-05 - accuracy: 1.0000 - val_loss: 0.5732 - val_accuracy: 0.9412\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.5091e-05 - accuracy: 1.0000\n",
      "Epoch 00120: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 4.5091e-05 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 0.9412\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.7833e-05 - accuracy: 1.0000\n",
      "Epoch 00121: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 5.7833e-05 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 0.9216\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.7033e-05 - accuracy: 1.0000\n",
      "Epoch 00122: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 5.7033e-05 - accuracy: 1.0000 - val_loss: 0.7585 - val_accuracy: 0.9020\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7864e-05 - accuracy: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 3.7864e-05 - accuracy: 1.0000 - val_loss: 0.5922 - val_accuracy: 0.9216\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6232e-05 - accuracy: 1.0000\n",
      "Epoch 00124: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 3.6232e-05 - accuracy: 1.0000 - val_loss: 0.6804 - val_accuracy: 0.9216\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.1462e-05 - accuracy: 1.0000\n",
      "Epoch 00125: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 4.1462e-05 - accuracy: 1.0000 - val_loss: 0.7183 - val_accuracy: 0.9020\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3138e-05 - accuracy: 1.0000\n",
      "Epoch 00126: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 3.3138e-05 - accuracy: 1.0000 - val_loss: 0.6103 - val_accuracy: 0.9412\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2446e-05 - accuracy: 1.0000\n",
      "Epoch 00127: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 4.2446e-05 - accuracy: 1.0000 - val_loss: 0.5896 - val_accuracy: 0.9216\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5844e-05 - accuracy: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 2.5844e-05 - accuracy: 1.0000 - val_loss: 0.4331 - val_accuracy: 0.9608\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5052e-05 - accuracy: 1.0000\n",
      "Epoch 00129: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 3.5052e-05 - accuracy: 1.0000 - val_loss: 0.6528 - val_accuracy: 0.9216\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5990e-05 - accuracy: 1.0000\n",
      "Epoch 00130: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 2.5990e-05 - accuracy: 1.0000 - val_loss: 0.5847 - val_accuracy: 0.9216\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3223e-05 - accuracy: 1.0000\n",
      "Epoch 00131: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 2.3223e-05 - accuracy: 1.0000 - val_loss: 0.7891 - val_accuracy: 0.9020\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0210e-05 - accuracy: 1.0000\n",
      "Epoch 00132: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 3.0210e-05 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.9412\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1126e-05 - accuracy: 1.0000\n",
      "Epoch 00133: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 2.1126e-05 - accuracy: 1.0000 - val_loss: 0.7128 - val_accuracy: 0.9412\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9140e-05 - accuracy: 1.0000\n",
      "Epoch 00134: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 1.9140e-05 - accuracy: 1.0000 - val_loss: 0.6286 - val_accuracy: 0.9412\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2819e-05 - accuracy: 1.0000\n",
      "Epoch 00135: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 2.2819e-05 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.9020\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1559e-05 - accuracy: 1.0000\n",
      "Epoch 00136: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 2.1559e-05 - accuracy: 1.0000 - val_loss: 0.6448 - val_accuracy: 0.9020\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4740e-05 - accuracy: 1.0000\n",
      "Epoch 00137: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.96078\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 1.4740e-05 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 0.9216\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2096e-05 - accuracy: 1.0000\n",
      "Epoch 00138: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00138: val_accuracy improved from 0.96078 to 0.98039, saving model to ./AI_models\\02_04_AI_val_accuracy_index_17.h5\n",
      "63/63 [==============================] - 31s 494ms/step - loss: 1.2096e-05 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.9804\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.4240e-05 - accuracy: 1.0000\n",
      "Epoch 00139: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 2.4240e-05 - accuracy: 1.0000 - val_loss: 0.6231 - val_accuracy: 0.9216\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6516e-05 - accuracy: 1.0000\n",
      "Epoch 00140: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 1.6516e-05 - accuracy: 1.0000 - val_loss: 0.5637 - val_accuracy: 0.9216\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9959e-05 - accuracy: 1.0000\n",
      "Epoch 00141: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 1.9959e-05 - accuracy: 1.0000 - val_loss: 0.8136 - val_accuracy: 0.9216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5861e-05 - accuracy: 1.0000\n",
      "Epoch 00142: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 2.5861e-05 - accuracy: 1.0000 - val_loss: 0.6429 - val_accuracy: 0.9216\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2007e-05 - accuracy: 1.0000\n",
      "Epoch 00143: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 1.2007e-05 - accuracy: 1.0000 - val_loss: 0.5675 - val_accuracy: 0.9216\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4386e-05 - accuracy: 1.0000\n",
      "Epoch 00144: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 1.4386e-05 - accuracy: 1.0000 - val_loss: 0.6659 - val_accuracy: 0.9020\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5913e-05 - accuracy: 1.0000\n",
      "Epoch 00145: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 1.5913e-05 - accuracy: 1.0000 - val_loss: 0.5913 - val_accuracy: 0.9020\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9047e-05 - accuracy: 1.0000\n",
      "Epoch 00146: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 20s 314ms/step - loss: 1.9047e-05 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.9412\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1122e-05 - accuracy: 1.0000\n",
      "Epoch 00147: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 20s 315ms/step - loss: 1.1122e-05 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.9412\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4748e-05 - accuracy: 1.0000\n",
      "Epoch 00148: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 308ms/step - loss: 1.4748e-05 - accuracy: 1.0000 - val_loss: 0.5211 - val_accuracy: 0.9412\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3488e-05 - accuracy: 1.0000\n",
      "Epoch 00149: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 20s 324ms/step - loss: 1.3488e-05 - accuracy: 1.0000 - val_loss: 0.5798 - val_accuracy: 0.9412\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5978e-05 - accuracy: 1.0000\n",
      "Epoch 00150: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 1.5978e-05 - accuracy: 1.0000 - val_loss: 0.6609 - val_accuracy: 0.9020\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0748e-05 - accuracy: 1.0000\n",
      "Epoch 00151: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 1.0748e-05 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.9216\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6829e-05 - accuracy: 1.0000\n",
      "Epoch 00152: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 1.6829e-05 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.9216\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2507e-05 - accuracy: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 1.2507e-05 - accuracy: 1.0000 - val_loss: 0.5332 - val_accuracy: 0.9216\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.5927e-06 - accuracy: 1.0000\n",
      "Epoch 00154: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 9.5927e-06 - accuracy: 1.0000 - val_loss: 0.5386 - val_accuracy: 0.9216\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2976e-05 - accuracy: 1.0000\n",
      "Epoch 00155: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 4.2976e-05 - accuracy: 1.0000 - val_loss: 0.5635 - val_accuracy: 0.9216\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3072e-05 - accuracy: 1.0000\n",
      "Epoch 00156: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 2.3072e-05 - accuracy: 1.0000 - val_loss: 0.6457 - val_accuracy: 0.9412\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0754e-05 - accuracy: 1.0000\n",
      "Epoch 00157: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 3.0754e-05 - accuracy: 1.0000 - val_loss: 0.6370 - val_accuracy: 0.9216\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3737e-05 - accuracy: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 1.3737e-05 - accuracy: 1.0000 - val_loss: 0.4908 - val_accuracy: 0.9608\n",
      "Epoch 159/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.8466e-06 - accuracy: 1.0000\n",
      "Epoch 00159: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 9.8466e-06 - accuracy: 1.0000 - val_loss: 0.7048 - val_accuracy: 0.9412\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0710e-05 - accuracy: 1.0000\n",
      "Epoch 00160: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 1.0710e-05 - accuracy: 1.0000 - val_loss: 0.5171 - val_accuracy: 0.9608\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1184e-05 - accuracy: 1.0000\n",
      "Epoch 00161: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 1.1184e-05 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9216\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.8177e-06 - accuracy: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 7.8177e-06 - accuracy: 1.0000 - val_loss: 0.5201 - val_accuracy: 0.9608\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1676e-05 - accuracy: 1.0000\n",
      "Epoch 00163: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 1.1676e-05 - accuracy: 1.0000 - val_loss: 0.5883 - val_accuracy: 0.9608\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.0879e-06 - accuracy: 1.0000\n",
      "Epoch 00164: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 9.0879e-06 - accuracy: 1.0000 - val_loss: 0.6673 - val_accuracy: 0.9412\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.9782e-06 - accuracy: 1.0000\n",
      "Epoch 00165: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 6.9782e-06 - accuracy: 1.0000 - val_loss: 0.5458 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0329 - accuracy: 0.8147\n",
      "Epoch 00166: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 1.0329 - accuracy: 0.8147 - val_loss: 35797.5820 - val_accuracy: 0.0980\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.8302\n",
      "Epoch 00167: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.5294 - accuracy: 0.8302 - val_loss: 257.4628 - val_accuracy: 0.0980\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9149\n",
      "Epoch 00168: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.2624 - accuracy: 0.9149 - val_loss: 6.5442 - val_accuracy: 0.4706\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9524\n",
      "Epoch 00169: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.1352 - accuracy: 0.9524 - val_loss: 0.7048 - val_accuracy: 0.8824\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9524\n",
      "Epoch 00170: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.1273 - accuracy: 0.9524 - val_loss: 0.4613 - val_accuracy: 0.9412\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9564\n",
      "Epoch 00171: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.1345 - accuracy: 0.9564 - val_loss: 4.7153 - val_accuracy: 0.4510\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9664\n",
      "Epoch 00172: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 0.0968 - accuracy: 0.9664 - val_loss: 0.4751 - val_accuracy: 0.8824\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9730\n",
      "Epoch 00173: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0799 - accuracy: 0.9730 - val_loss: 0.3786 - val_accuracy: 0.9412\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9770\n",
      "Epoch 00174: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0677 - accuracy: 0.9770 - val_loss: 0.4418 - val_accuracy: 0.9020\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9825\n",
      "Epoch 00175: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0547 - accuracy: 0.9825 - val_loss: 0.3316 - val_accuracy: 0.9216\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9870\n",
      "Epoch 00176: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0374 - accuracy: 0.9870 - val_loss: 0.4062 - val_accuracy: 0.9216\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9920\n",
      "Epoch 00177: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0228 - accuracy: 0.9920 - val_loss: 0.3610 - val_accuracy: 0.9020\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9840\n",
      "Epoch 00178: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0577 - accuracy: 0.9840 - val_loss: 0.3980 - val_accuracy: 0.9608\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9895\n",
      "Epoch 00179: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0278 - accuracy: 0.9895 - val_loss: 0.3524 - val_accuracy: 0.9216\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9895\n",
      "Epoch 00180: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0347 - accuracy: 0.9895 - val_loss: 0.5988 - val_accuracy: 0.8824\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9970\n",
      "Epoch 00181: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 0.3158 - val_accuracy: 0.9020\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9715\n",
      "Epoch 00182: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0910 - accuracy: 0.9715 - val_loss: 0.5733 - val_accuracy: 0.9412\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9705\n",
      "Epoch 00183: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.1154 - accuracy: 0.9705 - val_loss: 0.2513 - val_accuracy: 0.9412\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9629\n",
      "Epoch 00184: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.1342 - accuracy: 0.9629 - val_loss: 0.8380 - val_accuracy: 0.9020\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9765\n",
      "Epoch 00185: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0688 - accuracy: 0.9765 - val_loss: 0.6668 - val_accuracy: 0.8627\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9935\n",
      "Epoch 00186: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0172 - accuracy: 0.9935 - val_loss: 0.9740 - val_accuracy: 0.9020\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9950\n",
      "Epoch 00187: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0131 - accuracy: 0.9950 - val_loss: 0.7419 - val_accuracy: 0.9216\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9930\n",
      "Epoch 00188: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0235 - accuracy: 0.9930 - val_loss: 0.9501 - val_accuracy: 0.8824\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.9710\n",
      "Epoch 00189: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.1374 - accuracy: 0.9710 - val_loss: 1.0043 - val_accuracy: 0.8431\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.9609\n",
      "Epoch 00190: val_loss did not improve from 0.20702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.1584 - accuracy: 0.9609 - val_loss: 0.8831 - val_accuracy: 0.9216\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9810\n",
      "Epoch 00191: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0613 - accuracy: 0.9810 - val_loss: 0.4561 - val_accuracy: 0.9412\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9885\n",
      "Epoch 00192: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0330 - accuracy: 0.9885 - val_loss: 0.5888 - val_accuracy: 0.9412\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9935\n",
      "Epoch 00193: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0256 - accuracy: 0.9935 - val_loss: 0.5694 - val_accuracy: 0.9020\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9755\n",
      "Epoch 00194: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0909 - accuracy: 0.9755 - val_loss: 0.6718 - val_accuracy: 0.8824\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9865\n",
      "Epoch 00195: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0392 - accuracy: 0.9865 - val_loss: 0.9434 - val_accuracy: 0.8824\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9925\n",
      "Epoch 00196: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.6712 - val_accuracy: 0.9412\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9850\n",
      "Epoch 00197: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 0.0459 - accuracy: 0.9850 - val_loss: 0.7023 - val_accuracy: 0.8824\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9920\n",
      "Epoch 00198: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 0.0261 - accuracy: 0.9920 - val_loss: 0.6801 - val_accuracy: 0.9412\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9930\n",
      "Epoch 00199: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0202 - accuracy: 0.9930 - val_loss: 0.7263 - val_accuracy: 0.9020\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9900\n",
      "Epoch 00200: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0315 - accuracy: 0.9900 - val_loss: 0.3468 - val_accuracy: 0.9020\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9850\n",
      "Epoch 00201: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00201: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0530 - accuracy: 0.9850 - val_loss: 0.8222 - val_accuracy: 0.8235\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9895\n",
      "Epoch 00202: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00202: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0326 - accuracy: 0.9895 - val_loss: 0.3472 - val_accuracy: 0.9412\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9945\n",
      "Epoch 00203: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00203: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0163 - accuracy: 0.9945 - val_loss: 0.7561 - val_accuracy: 0.9020\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9960\n",
      "Epoch 00204: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00204: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0094 - accuracy: 0.9960 - val_loss: 1.0653 - val_accuracy: 0.8824\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9955\n",
      "Epoch 00205: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00205: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0204 - accuracy: 0.9955 - val_loss: 0.5008 - val_accuracy: 0.9020\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9885\n",
      "Epoch 00206: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00206: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0467 - accuracy: 0.9885 - val_loss: 0.3375 - val_accuracy: 0.9804\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9915\n",
      "Epoch 00207: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00207: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0289 - accuracy: 0.9915 - val_loss: 0.6835 - val_accuracy: 0.9608\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9815\n",
      "Epoch 00208: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00208: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0897 - accuracy: 0.9815 - val_loss: 1.0483 - val_accuracy: 0.9020\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9765\n",
      "Epoch 00209: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00209: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0842 - accuracy: 0.9765 - val_loss: 0.9296 - val_accuracy: 0.9020\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9850\n",
      "Epoch 00210: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00210: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0386 - accuracy: 0.9850 - val_loss: 0.7846 - val_accuracy: 0.9412\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9915\n",
      "Epoch 00211: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00211: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0299 - accuracy: 0.9915 - val_loss: 0.5433 - val_accuracy: 0.9412\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9980\n",
      "Epoch 00212: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00212: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.8978 - val_accuracy: 0.8824\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9970\n",
      "Epoch 00213: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00213: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.4957 - val_accuracy: 0.9412\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9990\n",
      "Epoch 00214: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00214: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0021 - accuracy: 0.9990 - val_loss: 0.3515 - val_accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.5611e-04 - accuracy: 0.9995\n",
      "Epoch 00215: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00215: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 9.5611e-04 - accuracy: 0.9995 - val_loss: 0.5212 - val_accuracy: 0.9216\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.7718e-04 - accuracy: 1.0000\n",
      "Epoch 00216: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00216: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 5.7718e-04 - accuracy: 1.0000 - val_loss: 0.5924 - val_accuracy: 0.9412\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9990\n",
      "Epoch 00217: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00217: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0020 - accuracy: 0.9990 - val_loss: 0.9853 - val_accuracy: 0.9020\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n",
      "Epoch 00218: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00218: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.6162 - val_accuracy: 0.8824\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.3217e-04 - accuracy: 1.0000\n",
      "Epoch 00219: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00219: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 7.3217e-04 - accuracy: 1.0000 - val_loss: 0.3538 - val_accuracy: 0.9608\n",
      "Epoch 220/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6935e-04 - accuracy: 1.0000\n",
      "Epoch 00220: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00220: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 3.6935e-04 - accuracy: 1.0000 - val_loss: 0.4748 - val_accuracy: 0.9412\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9980\n",
      "Epoch 00221: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00221: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0031 - accuracy: 0.9980 - val_loss: 0.5323 - val_accuracy: 0.9412\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.1927e-04 - accuracy: 0.9995\n",
      "Epoch 00222: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00222: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 8.1927e-04 - accuracy: 0.9995 - val_loss: 0.4342 - val_accuracy: 0.9412\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9985\n",
      "Epoch 00223: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00223: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0030 - accuracy: 0.9985 - val_loss: 0.6903 - val_accuracy: 0.9020\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9975\n",
      "Epoch 00224: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00224: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0074 - accuracy: 0.9975 - val_loss: 0.6516 - val_accuracy: 0.9216\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9810\n",
      "Epoch 00225: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00225: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0771 - accuracy: 0.9810 - val_loss: 1.1675 - val_accuracy: 0.8824\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9815\n",
      "Epoch 00226: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00226: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0911 - accuracy: 0.9815 - val_loss: 1.5567 - val_accuracy: 0.8627\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9895\n",
      "Epoch 00227: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00227: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0437 - accuracy: 0.9895 - val_loss: 0.8983 - val_accuracy: 0.9412\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9855\n",
      "Epoch 00228: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00228: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0603 - accuracy: 0.9855 - val_loss: 0.4955 - val_accuracy: 0.9216\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9760\n",
      "Epoch 00229: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00229: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.1102 - accuracy: 0.9760 - val_loss: 0.4276 - val_accuracy: 0.9020\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9925\n",
      "Epoch 00230: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00230: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0221 - accuracy: 0.9925 - val_loss: 0.6223 - val_accuracy: 0.9020\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9945\n",
      "Epoch 00231: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00231: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0225 - accuracy: 0.9945 - val_loss: 0.7701 - val_accuracy: 0.8627\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9975\n",
      "Epoch 00232: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00232: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0115 - accuracy: 0.9975 - val_loss: 0.9144 - val_accuracy: 0.9216\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9905\n",
      "Epoch 00233: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00233: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0355 - accuracy: 0.9905 - val_loss: 1.3972 - val_accuracy: 0.9020\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9925\n",
      "Epoch 00234: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00234: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0389 - accuracy: 0.9925 - val_loss: 0.5082 - val_accuracy: 0.8824\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9940\n",
      "Epoch 00235: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00235: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0170 - accuracy: 0.9940 - val_loss: 0.4101 - val_accuracy: 0.9412\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9985\n",
      "Epoch 00236: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00236: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.2279 - val_accuracy: 0.9412\n",
      "Epoch 237/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9985\n",
      "Epoch 00237: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00237: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0028 - accuracy: 0.9985 - val_loss: 0.4594 - val_accuracy: 0.8824\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.1646e-04 - accuracy: 1.0000\n",
      "Epoch 00238: val_loss improved from 0.20702 to 0.11996, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00238: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 30s 475ms/step - loss: 9.1646e-04 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.1949e-04 - accuracy: 1.0000\n",
      "Epoch 00239: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00239: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.1949e-04 - accuracy: 1.0000 - val_loss: 0.2691 - val_accuracy: 0.9608\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3035e-04 - accuracy: 1.0000\n",
      "Epoch 00240: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00240: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 2.3035e-04 - accuracy: 1.0000 - val_loss: 0.1355 - val_accuracy: 0.9412\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5410e-04 - accuracy: 1.0000\n",
      "Epoch 00241: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00241: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 1.5410e-04 - accuracy: 1.0000 - val_loss: 0.1394 - val_accuracy: 0.9412\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5202e-04 - accuracy: 1.0000\n",
      "Epoch 00242: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00242: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 1.5202e-04 - accuracy: 1.0000 - val_loss: 0.3517 - val_accuracy: 0.9412\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.5528e-05 - accuracy: 1.0000\n",
      "Epoch 00243: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00243: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 7.5528e-05 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.9412\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8577e-05 - accuracy: 1.0000\n",
      "Epoch 00244: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00244: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 4.8577e-05 - accuracy: 1.0000 - val_loss: 0.3051 - val_accuracy: 0.9412\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.5078e-05 - accuracy: 1.0000\n",
      "Epoch 00245: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00245: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 8.5078e-05 - accuracy: 1.0000 - val_loss: 0.2504 - val_accuracy: 0.9608\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.3738e-05 - accuracy: 1.0000\n",
      "Epoch 00246: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00246: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 4.3738e-05 - accuracy: 1.0000 - val_loss: 0.2967 - val_accuracy: 0.9412\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.2745e-05 - accuracy: 1.0000\n",
      "Epoch 00247: val_loss did not improve from 0.11996\n",
      "\n",
      "Epoch 00247: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 8.2745e-05 - accuracy: 1.0000 - val_loss: 0.1814 - val_accuracy: 0.9412\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5463e-05 - accuracy: 1.0000\n",
      "Epoch 00248: val_loss improved from 0.11996 to 0.10676, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00248: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 31s 497ms/step - loss: 3.5463e-05 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 0.9412\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6843e-05 - accuracy: 1.0000\n",
      "Epoch 00249: val_loss did not improve from 0.10676\n",
      "\n",
      "Epoch 00249: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 4.6843e-05 - accuracy: 1.0000 - val_loss: 0.3617 - val_accuracy: 0.9412\n",
      "Epoch 250/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4107e-05 - accuracy: 1.0000\n",
      "Epoch 00250: val_loss did not improve from 0.10676\n",
      "\n",
      "Epoch 00250: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 4.4107e-05 - accuracy: 1.0000 - val_loss: 0.3309 - val_accuracy: 0.9216\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6266e-05 - accuracy: 1.0000\n",
      "Epoch 00251: val_loss did not improve from 0.10676\n",
      "\n",
      "Epoch 00251: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 1.6266e-05 - accuracy: 1.0000 - val_loss: 0.1750 - val_accuracy: 0.9412\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7834e-05 - accuracy: 1.0000\n",
      "Epoch 00252: val_loss did not improve from 0.10676\n",
      "\n",
      "Epoch 00252: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 3.7834e-05 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 0.9412\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7070e-05 - accuracy: 1.0000\n",
      "Epoch 00253: val_loss did not improve from 0.10676\n",
      "\n",
      "Epoch 00253: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 2.7070e-05 - accuracy: 1.0000 - val_loss: 0.3979 - val_accuracy: 0.9216\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7365e-05 - accuracy: 1.0000\n",
      "Epoch 00254: val_loss did not improve from 0.10676\n",
      "\n",
      "Epoch 00254: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 2.7365e-05 - accuracy: 1.0000 - val_loss: 0.1944 - val_accuracy: 0.9216\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0675e-05 - accuracy: 1.0000\n",
      "Epoch 00255: val_loss did not improve from 0.10676\n",
      "\n",
      "Epoch 00255: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 3.0675e-05 - accuracy: 1.0000 - val_loss: 0.3095 - val_accuracy: 0.9412\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6576e-05 - accuracy: 1.0000\n",
      "Epoch 00256: val_loss improved from 0.10676 to 0.10300, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00256: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 33s 517ms/step - loss: 4.6576e-05 - accuracy: 1.0000 - val_loss: 0.1030 - val_accuracy: 0.9608\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.8003e-05 - accuracy: 1.0000\n",
      "Epoch 00257: val_loss did not improve from 0.10300\n",
      "\n",
      "Epoch 00257: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 3.8003e-05 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9804\n",
      "Epoch 258/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7193e-05 - accuracy: 1.0000\n",
      "Epoch 00258: val_loss did not improve from 0.10300\n",
      "\n",
      "Epoch 00258: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 2.7193e-05 - accuracy: 1.0000 - val_loss: 0.2897 - val_accuracy: 0.9608\n",
      "Epoch 259/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7938e-05 - accuracy: 1.0000\n",
      "Epoch 00259: val_loss did not improve from 0.10300\n",
      "\n",
      "Epoch 00259: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 2.7938e-05 - accuracy: 1.0000 - val_loss: 0.3863 - val_accuracy: 0.9216\n",
      "Epoch 260/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.1485e-05 - accuracy: 1.0000\n",
      "Epoch 00260: val_loss did not improve from 0.10300\n",
      "\n",
      "Epoch 00260: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 3.1485e-05 - accuracy: 1.0000 - val_loss: 0.1112 - val_accuracy: 0.9608\n",
      "Epoch 261/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0270e-05 - accuracy: 1.0000\n",
      "Epoch 00261: val_loss improved from 0.10300 to 0.10256, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00261: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 32s 516ms/step - loss: 3.0270e-05 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 0.9608\n",
      "Epoch 262/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 2.6859e-05 - accuracy: 1.0000\n",
      "Epoch 00262: val_loss did not improve from 0.10256\n",
      "\n",
      "Epoch 00262: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.6859e-05 - accuracy: 1.0000 - val_loss: 0.3542 - val_accuracy: 0.9412\n",
      "Epoch 263/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9293e-05 - accuracy: 1.0000\n",
      "Epoch 00263: val_loss did not improve from 0.10256\n",
      "\n",
      "Epoch 00263: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.9293e-05 - accuracy: 1.0000 - val_loss: 0.2276 - val_accuracy: 0.9412\n",
      "Epoch 264/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0343e-05 - accuracy: 1.0000\n",
      "Epoch 00264: val_loss did not improve from 0.10256\n",
      "\n",
      "Epoch 00264: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 2.0343e-05 - accuracy: 1.0000 - val_loss: 0.3455 - val_accuracy: 0.9608\n",
      "Epoch 265/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8496e-05 - accuracy: 1.0000\n",
      "Epoch 00265: val_loss did not improve from 0.10256\n",
      "\n",
      "Epoch 00265: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.8496e-05 - accuracy: 1.0000 - val_loss: 0.2840 - val_accuracy: 0.9608\n",
      "Epoch 266/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9421e-05 - accuracy: 1.0000\n",
      "Epoch 00266: val_loss did not improve from 0.10256\n",
      "\n",
      "Epoch 00266: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.9421e-05 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 0.9412\n",
      "Epoch 267/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.4064e-05 - accuracy: 1.0000\n",
      "Epoch 00267: val_loss improved from 0.10256 to 0.05752, saving model to ./AI_models\\02_04_AI_val_loss_index_17.h5\n",
      "\n",
      "Epoch 00267: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 32s 510ms/step - loss: 2.4064e-05 - accuracy: 1.0000 - val_loss: 0.0575 - val_accuracy: 0.9804\n",
      "Epoch 268/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8058e-05 - accuracy: 1.0000\n",
      "Epoch 00268: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00268: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.8058e-05 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9412\n",
      "Epoch 269/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5315e-05 - accuracy: 1.0000\n",
      "Epoch 00269: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00269: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 1.5315e-05 - accuracy: 1.0000 - val_loss: 0.1788 - val_accuracy: 0.9412\n",
      "Epoch 270/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6375e-05 - accuracy: 1.0000\n",
      "Epoch 00270: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00270: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.6375e-05 - accuracy: 1.0000 - val_loss: 0.0901 - val_accuracy: 0.9608\n",
      "Epoch 271/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4939e-05 - accuracy: 1.0000\n",
      "Epoch 00271: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00271: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 1.4939e-05 - accuracy: 1.0000 - val_loss: 0.3192 - val_accuracy: 0.9216\n",
      "Epoch 272/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1288e-05 - accuracy: 1.0000\n",
      "Epoch 00272: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00272: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 2.1288e-05 - accuracy: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9608\n",
      "Epoch 273/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9465e-05 - accuracy: 1.0000\n",
      "Epoch 00273: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00273: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.9465e-05 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9608\n",
      "Epoch 274/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1698e-05 - accuracy: 1.0000\n",
      "Epoch 00274: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00274: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.1698e-05 - accuracy: 1.0000 - val_loss: 0.3035 - val_accuracy: 0.9608\n",
      "Epoch 275/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0120e-05 - accuracy: 1.0000\n",
      "Epoch 00275: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00275: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 3.0120e-05 - accuracy: 1.0000 - val_loss: 0.1753 - val_accuracy: 0.9412\n",
      "Epoch 276/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7156e-05 - accuracy: 1.0000\n",
      "Epoch 00276: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00276: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 2.7156e-05 - accuracy: 1.0000 - val_loss: 0.1705 - val_accuracy: 0.9608\n",
      "Epoch 277/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0480e-05 - accuracy: 1.0000\n",
      "Epoch 00277: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00277: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.0480e-05 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 0.9804\n",
      "Epoch 278/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5926e-05 - accuracy: 1.0000\n",
      "Epoch 00278: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00278: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 1.5926e-05 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9412\n",
      "Epoch 279/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.1691e-06 - accuracy: 1.0000\n",
      "Epoch 00279: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00279: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 7.1691e-06 - accuracy: 1.0000 - val_loss: 0.3935 - val_accuracy: 0.9412\n",
      "Epoch 280/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2802e-05 - accuracy: 1.0000\n",
      "Epoch 00280: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00280: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 1.2802e-05 - accuracy: 1.0000 - val_loss: 0.3211 - val_accuracy: 0.9608\n",
      "Epoch 281/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3732e-05 - accuracy: 1.0000\n",
      "Epoch 00281: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00281: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 1.3732e-05 - accuracy: 1.0000 - val_loss: 0.2039 - val_accuracy: 0.9412\n",
      "Epoch 282/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3457e-05 - accuracy: 1.0000\n",
      "Epoch 00282: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00282: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 1.3457e-05 - accuracy: 1.0000 - val_loss: 0.3216 - val_accuracy: 0.9608\n",
      "Epoch 283/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.9357e-06 - accuracy: 1.0000\n",
      "Epoch 00283: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00283: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.9357e-06 - accuracy: 1.0000 - val_loss: 0.3184 - val_accuracy: 0.9608\n",
      "Epoch 284/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.7654e-06 - accuracy: 1.0000\n",
      "Epoch 00284: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00284: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 9.7654e-06 - accuracy: 1.0000 - val_loss: 0.1008 - val_accuracy: 0.9608\n",
      "Epoch 285/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.1624e-06 - accuracy: 1.0000\n",
      "Epoch 00285: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00285: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.1624e-06 - accuracy: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.6007e-06 - accuracy: 1.0000\n",
      "Epoch 00286: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00286: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.6007e-06 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9608\n",
      "Epoch 287/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.8035e-06 - accuracy: 1.0000\n",
      "Epoch 00287: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00287: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.8035e-06 - accuracy: 1.0000 - val_loss: 0.4579 - val_accuracy: 0.9216\n",
      "Epoch 288/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.2751e-06 - accuracy: 1.0000\n",
      "Epoch 00288: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00288: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 8.2751e-06 - accuracy: 1.0000 - val_loss: 0.3428 - val_accuracy: 0.9412\n",
      "Epoch 289/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3356e-05 - accuracy: 1.0000\n",
      "Epoch 00289: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00289: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.3356e-05 - accuracy: 1.0000 - val_loss: 0.3275 - val_accuracy: 0.9412\n",
      "Epoch 290/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.4093e-06 - accuracy: 1.0000\n",
      "Epoch 00290: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00290: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 5.4093e-06 - accuracy: 1.0000 - val_loss: 0.2776 - val_accuracy: 0.9804\n",
      "Epoch 291/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.6644e-06 - accuracy: 1.0000\n",
      "Epoch 00291: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00291: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 8.6644e-06 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9412\n",
      "Epoch 292/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.9319e-06 - accuracy: 1.0000\n",
      "Epoch 00292: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00292: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.9319e-06 - accuracy: 1.0000 - val_loss: 0.1547 - val_accuracy: 0.9608\n",
      "Epoch 293/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.4335e-06 - accuracy: 1.0000\n",
      "Epoch 00293: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00293: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 8.4335e-06 - accuracy: 1.0000 - val_loss: 0.1765 - val_accuracy: 0.9608\n",
      "Epoch 294/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7176e-06 - accuracy: 1.0000\n",
      "Epoch 00294: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00294: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.7176e-06 - accuracy: 1.0000 - val_loss: 0.1674 - val_accuracy: 0.9412\n",
      "Epoch 295/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.7720e-06 - accuracy: 1.0000\n",
      "Epoch 00295: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00295: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.7720e-06 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9804\n",
      "Epoch 296/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.7191e-06 - accuracy: 1.0000\n",
      "Epoch 00296: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00296: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.7191e-06 - accuracy: 1.0000 - val_loss: 0.1124 - val_accuracy: 0.9608\n",
      "Epoch 297/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.0446e-06 - accuracy: 1.0000\n",
      "Epoch 00297: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00297: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.0446e-06 - accuracy: 1.0000 - val_loss: 0.2037 - val_accuracy: 0.9216\n",
      "Epoch 298/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.8385e-06 - accuracy: 1.0000\n",
      "Epoch 00298: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00298: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.8385e-06 - accuracy: 1.0000 - val_loss: 0.2876 - val_accuracy: 0.9804\n",
      "Epoch 299/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7829e-06 - accuracy: 1.0000\n",
      "Epoch 00299: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00299: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.7829e-06 - accuracy: 1.0000 - val_loss: 0.2935 - val_accuracy: 0.9804\n",
      "Epoch 300/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.3227e-06 - accuracy: 1.0000\n",
      "Epoch 00300: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00300: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 5.3227e-06 - accuracy: 1.0000 - val_loss: 0.1789 - val_accuracy: 0.9608\n",
      "Epoch 301/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.3716e-06 - accuracy: 1.0000\n",
      "Epoch 00301: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00301: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.3716e-06 - accuracy: 1.0000 - val_loss: 0.4198 - val_accuracy: 0.9216\n",
      "Epoch 302/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7348e-06 - accuracy: 1.0000\n",
      "Epoch 00302: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00302: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.7348e-06 - accuracy: 1.0000 - val_loss: 0.1757 - val_accuracy: 0.9216\n",
      "Epoch 303/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.8525e-06 - accuracy: 1.0000\n",
      "Epoch 00303: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00303: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.8525e-06 - accuracy: 1.0000 - val_loss: 0.3637 - val_accuracy: 0.9216\n",
      "Epoch 304/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.0509e-06 - accuracy: 1.0000\n",
      "Epoch 00304: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00304: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.0509e-06 - accuracy: 1.0000 - val_loss: 0.3003 - val_accuracy: 0.9804\n",
      "Epoch 305/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7344e-06 - accuracy: 1.0000\n",
      "Epoch 00305: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00305: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.7344e-06 - accuracy: 1.0000 - val_loss: 0.1663 - val_accuracy: 0.9608\n",
      "Epoch 306/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.1858e-06 - accuracy: 1.0000\n",
      "Epoch 00306: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00306: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.1858e-06 - accuracy: 1.0000 - val_loss: 0.2953 - val_accuracy: 0.9804\n",
      "Epoch 307/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.2915e-06 - accuracy: 1.0000\n",
      "Epoch 00307: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00307: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.2915e-06 - accuracy: 1.0000 - val_loss: 0.3301 - val_accuracy: 0.9608\n",
      "Epoch 308/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.3350e-06 - accuracy: 1.0000\n",
      "Epoch 00308: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00308: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 8.3350e-06 - accuracy: 1.0000 - val_loss: 0.3948 - val_accuracy: 0.9608\n",
      "Epoch 309/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6263e-06 - accuracy: 1.0000\n",
      "Epoch 00309: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00309: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.6263e-06 - accuracy: 1.0000 - val_loss: 0.1205 - val_accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2829e-06 - accuracy: 1.0000\n",
      "Epoch 00310: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00310: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.2829e-06 - accuracy: 1.0000 - val_loss: 0.2682 - val_accuracy: 0.9412\n",
      "Epoch 311/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5871e-06 - accuracy: 1.0000\n",
      "Epoch 00311: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00311: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.5871e-06 - accuracy: 1.0000 - val_loss: 0.3236 - val_accuracy: 0.9608\n",
      "Epoch 312/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.6638e-06 - accuracy: 1.0000\n",
      "Epoch 00312: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00312: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 5.6638e-06 - accuracy: 1.0000 - val_loss: 0.3160 - val_accuracy: 0.9608\n",
      "Epoch 313/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4716e-06 - accuracy: 1.0000\n",
      "Epoch 00313: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00313: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 4.4716e-06 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9608\n",
      "Epoch 314/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0115e-06 - accuracy: 1.0000\n",
      "Epoch 00314: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00314: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.0115e-06 - accuracy: 1.0000 - val_loss: 0.0942 - val_accuracy: 0.9804\n",
      "Epoch 315/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6378 - accuracy: 0.8843\n",
      "Epoch 00315: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00315: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.6378 - accuracy: 0.8843 - val_loss: 5.0643 - val_accuracy: 0.6667\n",
      "Epoch 316/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9384\n",
      "Epoch 00316: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00316: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2244 - accuracy: 0.9384 - val_loss: 0.7761 - val_accuracy: 0.9216\n",
      "Epoch 317/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9720\n",
      "Epoch 00317: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00317: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0790 - accuracy: 0.9720 - val_loss: 0.4763 - val_accuracy: 0.9412\n",
      "Epoch 318/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9895\n",
      "Epoch 00318: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00318: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0393 - accuracy: 0.9895 - val_loss: 0.5901 - val_accuracy: 0.9412\n",
      "Epoch 319/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9945\n",
      "Epoch 00319: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00319: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.3651 - val_accuracy: 0.9608\n",
      "Epoch 320/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9955\n",
      "Epoch 00320: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00320: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0213 - accuracy: 0.9955 - val_loss: 0.7670 - val_accuracy: 0.9020\n",
      "Epoch 321/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9975\n",
      "Epoch 00321: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00321: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0063 - accuracy: 0.9975 - val_loss: 0.7094 - val_accuracy: 0.9216\n",
      "Epoch 322/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9980\n",
      "Epoch 00322: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00322: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 0.3660 - val_accuracy: 0.9608\n",
      "Epoch 323/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9990\n",
      "Epoch 00323: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00323: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.4588 - val_accuracy: 0.9412\n",
      "Epoch 324/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00324: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00324: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4422 - val_accuracy: 0.9412\n",
      "Epoch 325/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9995  \n",
      "Epoch 00325: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00325: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.3429 - val_accuracy: 0.9412\n",
      "Epoch 326/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9985\n",
      "Epoch 00326: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00326: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.8257 - val_accuracy: 0.9412\n",
      "Epoch 327/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9995\n",
      "Epoch 00327: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00327: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.6372 - val_accuracy: 0.9412\n",
      "Epoch 328/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7290e-04 - accuracy: 1.0000\n",
      "Epoch 00328: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00328: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.7290e-04 - accuracy: 1.0000 - val_loss: 0.4471 - val_accuracy: 0.9412\n",
      "Epoch 329/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9920\n",
      "Epoch 00329: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00329: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 0.8497 - val_accuracy: 0.8824\n",
      "Epoch 330/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9930\n",
      "Epoch 00330: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00330: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0265 - accuracy: 0.9930 - val_loss: 0.8538 - val_accuracy: 0.9216\n",
      "Epoch 331/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9875\n",
      "Epoch 00331: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00331: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0424 - accuracy: 0.9875 - val_loss: 1.0882 - val_accuracy: 0.8431\n",
      "Epoch 332/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9855\n",
      "Epoch 00332: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00332: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0422 - accuracy: 0.9855 - val_loss: 0.9534 - val_accuracy: 0.9020\n",
      "Epoch 333/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975\n",
      "Epoch 00333: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00333: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.5146 - val_accuracy: 0.9216\n",
      "Epoch 334/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9990\n",
      "Epoch 00334: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00334: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0066 - accuracy: 0.9990 - val_loss: 0.7751 - val_accuracy: 0.9412\n",
      "Epoch 335/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00335: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00335: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5876 - val_accuracy: 0.9216\n",
      "Epoch 336/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5717e-04 - accuracy: 1.0000\n",
      "Epoch 00336: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00336: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.5717e-04 - accuracy: 1.0000 - val_loss: 0.4660 - val_accuracy: 0.9804\n",
      "Epoch 337/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2374e-04 - accuracy: 1.0000\n",
      "Epoch 00337: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00337: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.2374e-04 - accuracy: 1.0000 - val_loss: 0.4907 - val_accuracy: 0.9608\n",
      "Epoch 338/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4200e-04 - accuracy: 1.0000\n",
      "Epoch 00338: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00338: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4200e-04 - accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 0.9608\n",
      "Epoch 339/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4755e-04 - accuracy: 1.0000\n",
      "Epoch 00339: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00339: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4755e-04 - accuracy: 1.0000 - val_loss: 0.4958 - val_accuracy: 0.9216\n",
      "Epoch 340/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3294e-04 - accuracy: 1.0000\n",
      "Epoch 00340: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00340: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3294e-04 - accuracy: 1.0000 - val_loss: 0.4406 - val_accuracy: 0.9412\n",
      "Epoch 341/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1036e-04 - accuracy: 1.0000\n",
      "Epoch 00341: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00341: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.1036e-04 - accuracy: 1.0000 - val_loss: 0.5127 - val_accuracy: 0.9412\n",
      "Epoch 342/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.3239e-05 - accuracy: 1.0000\n",
      "Epoch 00342: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00342: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.3239e-05 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9804\n",
      "Epoch 343/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.5002e-05 - accuracy: 1.0000\n",
      "Epoch 00343: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00343: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.5002e-05 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 0.9216\n",
      "Epoch 344/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0561e-04 - accuracy: 1.0000\n",
      "Epoch 00344: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00344: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.0561e-04 - accuracy: 1.0000 - val_loss: 0.7859 - val_accuracy: 0.9020\n",
      "Epoch 345/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.3326e-05 - accuracy: 1.0000\n",
      "Epoch 00345: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00345: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.3326e-05 - accuracy: 1.0000 - val_loss: 0.5002 - val_accuracy: 0.9804\n",
      "Epoch 346/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.4632e-05 - accuracy: 1.0000\n",
      "Epoch 00346: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00346: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.4632e-05 - accuracy: 1.0000 - val_loss: 0.4448 - val_accuracy: 0.9412\n",
      "Epoch 347/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.6520e-05 - accuracy: 1.0000\n",
      "Epoch 00347: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00347: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.6520e-05 - accuracy: 1.0000 - val_loss: 0.4900 - val_accuracy: 0.9608\n",
      "Epoch 348/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.1996e-05 - accuracy: 1.0000\n",
      "Epoch 00348: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00348: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.1996e-05 - accuracy: 1.0000 - val_loss: 0.5703 - val_accuracy: 0.9412\n",
      "Epoch 349/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.8800e-05 - accuracy: 1.0000\n",
      "Epoch 00349: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00349: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.8800e-05 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 0.9216\n",
      "Epoch 350/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.4228e-05 - accuracy: 1.0000\n",
      "Epoch 00350: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00350: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.4228e-05 - accuracy: 1.0000 - val_loss: 0.4985 - val_accuracy: 0.9412\n",
      "Epoch 351/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.5327e-05 - accuracy: 1.0000\n",
      "Epoch 00351: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00351: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.5327e-05 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9412\n",
      "Epoch 352/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.9138e-05 - accuracy: 1.0000\n",
      "Epoch 00352: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00352: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.9138e-05 - accuracy: 1.0000 - val_loss: 0.5803 - val_accuracy: 0.9412\n",
      "Epoch 353/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8870e-05 - accuracy: 1.0000\n",
      "Epoch 00353: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00353: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.8870e-05 - accuracy: 1.0000 - val_loss: 0.4226 - val_accuracy: 0.9608\n",
      "Epoch 354/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5076e-05 - accuracy: 1.0000\n",
      "Epoch 00354: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00354: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.5076e-05 - accuracy: 1.0000 - val_loss: 0.4753 - val_accuracy: 0.9608\n",
      "Epoch 355/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.1715e-05 - accuracy: 1.0000\n",
      "Epoch 00355: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00355: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.1715e-05 - accuracy: 1.0000 - val_loss: 0.4168 - val_accuracy: 0.9412\n",
      "Epoch 356/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4934e-05 - accuracy: 1.0000\n",
      "Epoch 00356: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00356: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.4934e-05 - accuracy: 1.0000 - val_loss: 0.4071 - val_accuracy: 0.9804\n",
      "Epoch 357/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.3054e-05 - accuracy: 1.0000\n",
      "Epoch 00357: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00357: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.3054e-05 - accuracy: 1.0000 - val_loss: 0.5404 - val_accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.1681e-05 - accuracy: 1.0000\n",
      "Epoch 00358: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00358: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.1681e-05 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 0.9020\n",
      "Epoch 359/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3638e-05 - accuracy: 1.0000\n",
      "Epoch 00359: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00359: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.3638e-05 - accuracy: 1.0000 - val_loss: 0.7400 - val_accuracy: 0.9608\n",
      "Epoch 360/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2199e-05 - accuracy: 1.0000\n",
      "Epoch 00360: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00360: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.2199e-05 - accuracy: 1.0000 - val_loss: 0.4830 - val_accuracy: 0.9804\n",
      "Epoch 361/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.6107e-05 - accuracy: 1.0000\n",
      "Epoch 00361: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00361: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.6107e-05 - accuracy: 1.0000 - val_loss: 0.5210 - val_accuracy: 0.9412\n",
      "Epoch 362/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7305e-05 - accuracy: 1.0000\n",
      "Epoch 00362: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00362: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.7305e-05 - accuracy: 1.0000 - val_loss: 0.7201 - val_accuracy: 0.9216\n",
      "Epoch 363/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.8499e-05 - accuracy: 1.0000\n",
      "Epoch 00363: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00363: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.8499e-05 - accuracy: 1.0000 - val_loss: 0.6408 - val_accuracy: 0.9216\n",
      "Epoch 364/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3924e-05 - accuracy: 1.0000\n",
      "Epoch 00364: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00364: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.3924e-05 - accuracy: 1.0000 - val_loss: 0.2624 - val_accuracy: 0.9804\n",
      "Epoch 365/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.9031e-05 - accuracy: 1.0000\n",
      "Epoch 00365: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00365: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.9031e-05 - accuracy: 1.0000 - val_loss: 0.4623 - val_accuracy: 0.9608\n",
      "Epoch 366/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.9403e-05 - accuracy: 1.0000\n",
      "Epoch 00366: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00366: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 5.9403e-05 - accuracy: 1.0000 - val_loss: 0.4679 - val_accuracy: 0.9216\n",
      "Epoch 367/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.4619e-05 - accuracy: 1.0000\n",
      "Epoch 00367: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00367: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.4619e-05 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 0.9412\n",
      "Epoch 368/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5500e-05 - accuracy: 1.0000\n",
      "Epoch 00368: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00368: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.5500e-05 - accuracy: 1.0000 - val_loss: 0.5414 - val_accuracy: 0.9412\n",
      "Epoch 369/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1315e-05 - accuracy: 1.0000\n",
      "Epoch 00369: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00369: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.1315e-05 - accuracy: 1.0000 - val_loss: 0.8216 - val_accuracy: 0.9216\n",
      "Epoch 370/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0034e-05 - accuracy: 1.0000\n",
      "Epoch 00370: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00370: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.0034e-05 - accuracy: 1.0000 - val_loss: 0.3218 - val_accuracy: 0.9608\n",
      "Epoch 371/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5202e-05 - accuracy: 1.0000\n",
      "Epoch 00371: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00371: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.5202e-05 - accuracy: 1.0000 - val_loss: 0.5648 - val_accuracy: 0.9216\n",
      "Epoch 372/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8444e-05 - accuracy: 1.0000\n",
      "Epoch 00372: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00372: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.8444e-05 - accuracy: 1.0000 - val_loss: 0.4876 - val_accuracy: 0.9608\n",
      "Epoch 373/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5381e-05 - accuracy: 1.0000\n",
      "Epoch 00373: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00373: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.5381e-05 - accuracy: 1.0000 - val_loss: 0.4902 - val_accuracy: 0.9412\n",
      "Epoch 374/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.6842e-05 - accuracy: 1.0000\n",
      "Epoch 00374: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00374: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.6842e-05 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.9412\n",
      "Epoch 375/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9646e-05 - accuracy: 1.0000\n",
      "Epoch 00375: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00375: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.9646e-05 - accuracy: 1.0000 - val_loss: 0.5553 - val_accuracy: 0.9020\n",
      "Epoch 376/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0367e-05 - accuracy: 1.0000\n",
      "Epoch 00376: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00376: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.0367e-05 - accuracy: 1.0000 - val_loss: 0.5084 - val_accuracy: 0.9412\n",
      "Epoch 377/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4581e-05 - accuracy: 1.0000\n",
      "Epoch 00377: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00377: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.4581e-05 - accuracy: 1.0000 - val_loss: 0.3498 - val_accuracy: 0.9412\n",
      "Epoch 378/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.9747e-05 - accuracy: 1.0000\n",
      "Epoch 00378: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00378: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.9747e-05 - accuracy: 1.0000 - val_loss: 0.7431 - val_accuracy: 0.9216\n",
      "Epoch 379/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.4182e-05 - accuracy: 1.0000\n",
      "Epoch 00379: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00379: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.4182e-05 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 0.9216\n",
      "Epoch 380/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3109e-05 - accuracy: 1.0000\n",
      "Epoch 00380: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00380: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.3109e-05 - accuracy: 1.0000 - val_loss: 0.7819 - val_accuracy: 0.9412\n",
      "Epoch 381/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2059e-05 - accuracy: 1.0000\n",
      "Epoch 00381: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00381: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.2059e-05 - accuracy: 1.0000 - val_loss: 0.5307 - val_accuracy: 0.9020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 382/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4371e-05 - accuracy: 1.0000\n",
      "Epoch 00382: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00382: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.4371e-05 - accuracy: 1.0000 - val_loss: 0.4926 - val_accuracy: 0.9412\n",
      "Epoch 383/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6433e-05 - accuracy: 1.0000\n",
      "Epoch 00383: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00383: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.6433e-05 - accuracy: 1.0000 - val_loss: 0.6664 - val_accuracy: 0.9216\n",
      "Epoch 384/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1898e-05 - accuracy: 1.0000\n",
      "Epoch 00384: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00384: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.1898e-05 - accuracy: 1.0000 - val_loss: 0.4363 - val_accuracy: 0.9412\n",
      "Epoch 385/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3433e-05 - accuracy: 1.0000\n",
      "Epoch 00385: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00385: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.3433e-05 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 0.9608\n",
      "Epoch 386/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1133e-05 - accuracy: 1.0000\n",
      "Epoch 00386: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00386: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.1133e-05 - accuracy: 1.0000 - val_loss: 0.4781 - val_accuracy: 0.9412\n",
      "Epoch 387/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.4911e-06 - accuracy: 1.0000\n",
      "Epoch 00387: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00387: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00387: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 8.4911e-06 - accuracy: 1.0000 - val_loss: 0.5018 - val_accuracy: 0.9412\n",
      "Epoch 388/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3228e-05 - accuracy: 1.0000\n",
      "Epoch 00388: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00388: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.3228e-05 - accuracy: 1.0000 - val_loss: 0.3763 - val_accuracy: 0.9412\n",
      "Epoch 389/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0257e-05 - accuracy: 1.0000\n",
      "Epoch 00389: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00389: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.0257e-05 - accuracy: 1.0000 - val_loss: 0.6588 - val_accuracy: 0.9216\n",
      "Epoch 390/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.8457e-06 - accuracy: 1.0000\n",
      "Epoch 00390: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00390: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.8457e-06 - accuracy: 1.0000 - val_loss: 0.5761 - val_accuracy: 0.9412\n",
      "Epoch 391/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2640e-05 - accuracy: 1.0000\n",
      "Epoch 00391: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00391: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.2640e-05 - accuracy: 1.0000 - val_loss: 0.3782 - val_accuracy: 0.9216\n",
      "Epoch 392/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.3078e-06 - accuracy: 1.0000\n",
      "Epoch 00392: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00392: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.3078e-06 - accuracy: 1.0000 - val_loss: 0.7454 - val_accuracy: 0.9216\n",
      "Epoch 393/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.3237e-06 - accuracy: 1.0000\n",
      "Epoch 00393: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00393: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.3237e-06 - accuracy: 1.0000 - val_loss: 0.6533 - val_accuracy: 0.9216\n",
      "Epoch 394/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.6929e-06 - accuracy: 1.0000\n",
      "Epoch 00394: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00394: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.6929e-06 - accuracy: 1.0000 - val_loss: 0.5366 - val_accuracy: 0.9216\n",
      "Epoch 395/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0281e-05 - accuracy: 1.0000\n",
      "Epoch 00395: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00395: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.0281e-05 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.9216\n",
      "Epoch 396/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.3590e-06 - accuracy: 1.0000\n",
      "Epoch 00396: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00396: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.3590e-06 - accuracy: 1.0000 - val_loss: 0.5440 - val_accuracy: 0.9216\n",
      "Epoch 397/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.2852e-06 - accuracy: 1.0000\n",
      "Epoch 00397: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00397: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 8.2852e-06 - accuracy: 1.0000 - val_loss: 0.7348 - val_accuracy: 0.9216\n",
      "Epoch 398/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0847e-05 - accuracy: 1.0000\n",
      "Epoch 00398: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00398: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.0847e-05 - accuracy: 1.0000 - val_loss: 0.4152 - val_accuracy: 0.9412\n",
      "Epoch 399/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.1532e-06 - accuracy: 1.0000\n",
      "Epoch 00399: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00399: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.1532e-06 - accuracy: 1.0000 - val_loss: 0.5184 - val_accuracy: 0.9608\n",
      "Epoch 400/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4041e-05 - accuracy: 1.0000\n",
      "Epoch 00400: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00400: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.4041e-05 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 0.9216\n",
      "Epoch 401/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.3903e-06 - accuracy: 1.0000\n",
      "Epoch 00401: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00401: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.3903e-06 - accuracy: 1.0000 - val_loss: 0.3842 - val_accuracy: 0.9412\n",
      "Epoch 402/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.3254e-06 - accuracy: 1.0000\n",
      "Epoch 00402: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00402: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 8.3254e-06 - accuracy: 1.0000 - val_loss: 0.5798 - val_accuracy: 0.9216\n",
      "Epoch 403/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.1003e-06 - accuracy: 1.0000\n",
      "Epoch 00403: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00403: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.1003e-06 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 0.9412\n",
      "Epoch 404/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.8082e-06 - accuracy: 1.0000\n",
      "Epoch 00404: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00404: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.8082e-06 - accuracy: 1.0000 - val_loss: 0.8541 - val_accuracy: 0.9216\n",
      "Epoch 405/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0143e-05 - accuracy: 1.0000\n",
      "Epoch 00405: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00405: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.0143e-05 - accuracy: 1.0000 - val_loss: 0.4952 - val_accuracy: 0.9216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.5717e-06 - accuracy: 1.0000\n",
      "Epoch 00406: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00406: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.5717e-06 - accuracy: 1.0000 - val_loss: 0.5101 - val_accuracy: 0.9216\n",
      "Epoch 407/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0568e-05 - accuracy: 1.0000\n",
      "Epoch 00407: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00407: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.0568e-05 - accuracy: 1.0000 - val_loss: 0.6735 - val_accuracy: 0.9216\n",
      "Epoch 408/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6070e-05 - accuracy: 1.0000\n",
      "Epoch 00408: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00408: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.6070e-05 - accuracy: 1.0000 - val_loss: 0.5287 - val_accuracy: 0.9412\n",
      "Epoch 409/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7740e-05 - accuracy: 1.0000\n",
      "Epoch 00409: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00409: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.7740e-05 - accuracy: 1.0000 - val_loss: 0.3363 - val_accuracy: 0.9412\n",
      "Epoch 410/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.6492e-06 - accuracy: 1.0000\n",
      "Epoch 00410: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00410: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.6492e-06 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 0.9412\n",
      "Epoch 411/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.8808e-06 - accuracy: 1.0000\n",
      "Epoch 00411: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00411: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.8808e-06 - accuracy: 1.0000 - val_loss: 0.5039 - val_accuracy: 0.9412\n",
      "Epoch 412/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.5369e-06 - accuracy: 1.0000\n",
      "Epoch 00412: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00412: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.5369e-06 - accuracy: 1.0000 - val_loss: 0.5677 - val_accuracy: 0.9412\n",
      "Epoch 413/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.4280e-06 - accuracy: 1.0000\n",
      "Epoch 00413: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00413: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.4280e-06 - accuracy: 1.0000 - val_loss: 0.5822 - val_accuracy: 0.9608\n",
      "Epoch 414/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7314e-06 - accuracy: 1.0000\n",
      "Epoch 00414: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00414: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.7314e-06 - accuracy: 1.0000 - val_loss: 0.4684 - val_accuracy: 0.9804\n",
      "Epoch 415/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.1327e-06 - accuracy: 1.0000\n",
      "Epoch 00415: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00415: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.1327e-06 - accuracy: 1.0000 - val_loss: 0.4862 - val_accuracy: 0.9412\n",
      "Epoch 416/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.1161e-06 - accuracy: 1.0000\n",
      "Epoch 00416: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00416: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.1161e-06 - accuracy: 1.0000 - val_loss: 0.7959 - val_accuracy: 0.9412\n",
      "Epoch 417/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6483e-06 - accuracy: 1.0000\n",
      "Epoch 00417: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00417: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.6483e-06 - accuracy: 1.0000 - val_loss: 0.5680 - val_accuracy: 0.9608\n",
      "Epoch 418/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3744e-06 - accuracy: 1.0000\n",
      "Epoch 00418: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00418: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.3744e-06 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 0.9608\n",
      "Epoch 419/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0659e-06 - accuracy: 1.0000\n",
      "Epoch 00419: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00419: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.0659e-06 - accuracy: 1.0000 - val_loss: 0.4130 - val_accuracy: 0.9608\n",
      "Epoch 420/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.9866e-06 - accuracy: 1.0000\n",
      "Epoch 00420: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00420: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.9866e-06 - accuracy: 1.0000 - val_loss: 0.4419 - val_accuracy: 0.9608\n",
      "Epoch 421/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5934e-06 - accuracy: 1.0000\n",
      "Epoch 00421: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00421: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.5934e-06 - accuracy: 1.0000 - val_loss: 0.3099 - val_accuracy: 0.9412\n",
      "Epoch 422/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.2659e-06 - accuracy: 1.0000\n",
      "Epoch 00422: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00422: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.2659e-06 - accuracy: 1.0000 - val_loss: 0.6407 - val_accuracy: 0.9216\n",
      "Epoch 423/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6654e-06 - accuracy: 1.0000\n",
      "Epoch 00423: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00423: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6654e-06 - accuracy: 1.0000 - val_loss: 0.3585 - val_accuracy: 0.9412\n",
      "Epoch 424/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.8653e-06 - accuracy: 1.0000\n",
      "Epoch 00424: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00424: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.8653e-06 - accuracy: 1.0000 - val_loss: 0.4445 - val_accuracy: 0.9412\n",
      "Epoch 425/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.8263e-06 - accuracy: 1.0000\n",
      "Epoch 00425: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00425: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.8263e-06 - accuracy: 1.0000 - val_loss: 0.3168 - val_accuracy: 0.9412\n",
      "Epoch 426/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.7932e-06 - accuracy: 1.0000\n",
      "Epoch 00426: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00426: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 4.7932e-06 - accuracy: 1.0000 - val_loss: 0.2915 - val_accuracy: 0.9804\n",
      "Epoch 427/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6347e-06 - accuracy: 1.0000\n",
      "Epoch 00427: val_loss did not improve from 0.05752\n",
      "\n",
      "Epoch 00427: val_accuracy did not improve from 0.98039\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6347e-06 - accuracy: 1.0000 - val_loss: 0.4424 - val_accuracy: 0.9608\n",
      "17  번째 학습을 완료했습니다.\n",
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 8s - loss: 12.2788 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0957s vs `on_train_batch_end` time: 0.1878s). Check your callbacks.\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.8723 - accuracy: 0.1267\n",
      "Epoch 00001: val_loss improved from inf to 32545.77539, saving model to ./AI_models\\02_04_AI_val_loss_index_18.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.11765, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 23s 366ms/step - loss: 3.8723 - accuracy: 0.1267 - val_loss: 32545.7754 - val_accuracy: 0.1176\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2328 - accuracy: 0.1898\n",
      "Epoch 00002: val_loss improved from 32545.77539 to 1690.90503, saving model to ./AI_models\\02_04_AI_val_loss_index_18.h5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 322ms/step - loss: 2.2328 - accuracy: 0.1898 - val_loss: 1690.9050 - val_accuracy: 0.0784\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9195 - accuracy: 0.3115\n",
      "Epoch 00003: val_loss improved from 1690.90503 to 3.13343, saving model to ./AI_models\\02_04_AI_val_loss_index_18.h5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 312ms/step - loss: 1.9195 - accuracy: 0.3115 - val_loss: 3.1334 - val_accuracy: 0.1176\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6209 - accuracy: 0.4311\n",
      "Epoch 00004: val_loss did not improve from 3.13343\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 1.6209 - accuracy: 0.4311 - val_loss: 3.1912 - val_accuracy: 0.0784\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3148 - accuracy: 0.5578\n",
      "Epoch 00005: val_loss improved from 3.13343 to 3.03280, saving model to ./AI_models\\02_04_AI_val_loss_index_18.h5\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 20s 323ms/step - loss: 1.3148 - accuracy: 0.5578 - val_loss: 3.0328 - val_accuracy: 0.0980\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0476 - accuracy: 0.6515\n",
      "Epoch 00006: val_loss did not improve from 3.03280\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.0476 - accuracy: 0.6515 - val_loss: 4.1152 - val_accuracy: 0.0784\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8795 - accuracy: 0.7121\n",
      "Epoch 00007: val_loss did not improve from 3.03280\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.8795 - accuracy: 0.7121 - val_loss: 4.9380 - val_accuracy: 0.0980\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7696 - accuracy: 0.7481\n",
      "Epoch 00008: val_loss did not improve from 3.03280\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.7696 - accuracy: 0.7481 - val_loss: 4.5039 - val_accuracy: 0.0980\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6434 - accuracy: 0.7827\n",
      "Epoch 00009: val_loss did not improve from 3.03280\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.11765\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.6434 - accuracy: 0.7827 - val_loss: 4.6384 - val_accuracy: 0.0980\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.7912\n",
      "Epoch 00010: val_loss improved from 3.03280 to 2.73037, saving model to ./AI_models\\02_04_AI_val_loss_index_18.h5\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.11765 to 0.33333, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 48s 756ms/step - loss: 0.6280 - accuracy: 0.7912 - val_loss: 2.7304 - val_accuracy: 0.3333\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.8147\n",
      "Epoch 00011: val_loss improved from 2.73037 to 1.71255, saving model to ./AI_models\\02_04_AI_val_loss_index_18.h5\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.33333 to 0.54902, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 23s 361ms/step - loss: 0.5619 - accuracy: 0.8147 - val_loss: 1.7126 - val_accuracy: 0.5490\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4794 - accuracy: 0.8363\n",
      "Epoch 00012: val_loss did not improve from 1.71255\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.54902\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.4794 - accuracy: 0.8363 - val_loss: 2.5106 - val_accuracy: 0.4118\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4177 - accuracy: 0.8688\n",
      "Epoch 00013: val_loss improved from 1.71255 to 0.54945, saving model to ./AI_models\\02_04_AI_val_loss_index_18.h5\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.54902 to 0.74510, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 0.4177 - accuracy: 0.8688 - val_loss: 0.5495 - val_accuracy: 0.7451\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3993 - accuracy: 0.8563\n",
      "Epoch 00014: val_loss did not improve from 0.54945\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.74510 to 0.80392, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 19s 306ms/step - loss: 0.3993 - accuracy: 0.8563 - val_loss: 0.6648 - val_accuracy: 0.8039\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3510 - accuracy: 0.8858\n",
      "Epoch 00015: val_loss did not improve from 0.54945\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.80392 to 0.84314, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 20s 310ms/step - loss: 0.3510 - accuracy: 0.8858 - val_loss: 0.6711 - val_accuracy: 0.8431\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.8848\n",
      "Epoch 00016: val_loss did not improve from 0.54945\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.84314 to 0.88235, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 20s 320ms/step - loss: 0.3530 - accuracy: 0.8848 - val_loss: 0.6198 - val_accuracy: 0.8824\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.8893\n",
      "Epoch 00017: val_loss did not improve from 0.54945\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.3084 - accuracy: 0.8893 - val_loss: 0.5631 - val_accuracy: 0.8431\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.8918\n",
      "Epoch 00018: val_loss improved from 0.54945 to 0.28768, saving model to ./AI_models\\02_04_AI_val_loss_index_18.h5\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 31s 493ms/step - loss: 0.3209 - accuracy: 0.8918 - val_loss: 0.2877 - val_accuracy: 0.8824\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.8923\n",
      "Epoch 00019: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.3067 - accuracy: 0.8923 - val_loss: 1.0515 - val_accuracy: 0.7255\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9099\n",
      "Epoch 00020: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2891 - accuracy: 0.9099 - val_loss: 0.4547 - val_accuracy: 0.8824\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.9299\n",
      "Epoch 00021: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.1975 - accuracy: 0.9299 - val_loss: 0.3276 - val_accuracy: 0.8824\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1709 - accuracy: 0.9384\n",
      "Epoch 00022: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1709 - accuracy: 0.9384 - val_loss: 1.3460 - val_accuracy: 0.7059\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9304\n",
      "Epoch 00023: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2137 - accuracy: 0.9304 - val_loss: 0.6634 - val_accuracy: 0.8235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9309\n",
      "Epoch 00024: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2014 - accuracy: 0.9309 - val_loss: 0.6251 - val_accuracy: 0.8431\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9564\n",
      "Epoch 00025: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1201 - accuracy: 0.9564 - val_loss: 0.6972 - val_accuracy: 0.8627\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9369\n",
      "Epoch 00026: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1977 - accuracy: 0.9369 - val_loss: 0.9256 - val_accuracy: 0.8627\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9219\n",
      "Epoch 00027: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.2498 - accuracy: 0.9219 - val_loss: 1.0260 - val_accuracy: 0.7647\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 0.9484\n",
      "Epoch 00028: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1465 - accuracy: 0.9484 - val_loss: 0.6535 - val_accuracy: 0.8431\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9274\n",
      "Epoch 00029: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2248 - accuracy: 0.9274 - val_loss: 1.3135 - val_accuracy: 0.7451\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9404\n",
      "Epoch 00030: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.88235\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1950 - accuracy: 0.9404 - val_loss: 0.9109 - val_accuracy: 0.8039\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9284\n",
      "Epoch 00031: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.88235 to 0.90196, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 31s 495ms/step - loss: 0.2205 - accuracy: 0.9284 - val_loss: 0.6477 - val_accuracy: 0.9020\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9624\n",
      "Epoch 00032: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.1104 - accuracy: 0.9624 - val_loss: 0.7662 - val_accuracy: 0.8235\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9730\n",
      "Epoch 00033: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.0927 - accuracy: 0.9730 - val_loss: 0.8314 - val_accuracy: 0.7647\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9770\n",
      "Epoch 00034: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0778 - accuracy: 0.9770 - val_loss: 0.6455 - val_accuracy: 0.8824\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9574\n",
      "Epoch 00035: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.1099 - accuracy: 0.9574 - val_loss: 0.6150 - val_accuracy: 0.8824\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9444\n",
      "Epoch 00036: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1980 - accuracy: 0.9444 - val_loss: 0.6410 - val_accuracy: 0.8627\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.9384\n",
      "Epoch 00037: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1827 - accuracy: 0.9384 - val_loss: 0.8215 - val_accuracy: 0.8039\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9730\n",
      "Epoch 00038: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0819 - accuracy: 0.9730 - val_loss: 0.5016 - val_accuracy: 0.8627\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9810\n",
      "Epoch 00039: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0535 - accuracy: 0.9810 - val_loss: 0.5288 - val_accuracy: 0.8431\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9795\n",
      "Epoch 00040: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0568 - accuracy: 0.9795 - val_loss: 0.5743 - val_accuracy: 0.8627\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9604\n",
      "Epoch 00041: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1265 - accuracy: 0.9604 - val_loss: 0.9171 - val_accuracy: 0.8431\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9564\n",
      "Epoch 00042: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1405 - accuracy: 0.9564 - val_loss: 1.0445 - val_accuracy: 0.8431\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.9434\n",
      "Epoch 00043: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1922 - accuracy: 0.9434 - val_loss: 0.9493 - val_accuracy: 0.9020\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.9469\n",
      "Epoch 00044: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1591 - accuracy: 0.9469 - val_loss: 1.0348 - val_accuracy: 0.7451\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9614\n",
      "Epoch 00045: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1110 - accuracy: 0.9614 - val_loss: 0.8640 - val_accuracy: 0.7647\n",
      "Epoch 46/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9675\n",
      "Epoch 00046: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1102 - accuracy: 0.9675 - val_loss: 1.1377 - val_accuracy: 0.7255\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9735\n",
      "Epoch 00047: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0833 - accuracy: 0.9735 - val_loss: 1.1267 - val_accuracy: 0.8039\n",
      "Epoch 48/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9835\n",
      "Epoch 00048: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0567 - accuracy: 0.9835 - val_loss: 0.7411 - val_accuracy: 0.9020\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9905\n",
      "Epoch 00049: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0291 - accuracy: 0.9905 - val_loss: 0.5840 - val_accuracy: 0.9020\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9905\n",
      "Epoch 00050: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0304 - accuracy: 0.9905 - val_loss: 0.6842 - val_accuracy: 0.8824\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9940\n",
      "Epoch 00051: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0205 - accuracy: 0.9940 - val_loss: 0.8622 - val_accuracy: 0.8627\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9780\n",
      "Epoch 00052: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.8490 - val_accuracy: 0.8431\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.9489\n",
      "Epoch 00053: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.1978 - accuracy: 0.9489 - val_loss: 1.2036 - val_accuracy: 0.7843\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9479\n",
      "Epoch 00054: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1566 - accuracy: 0.9479 - val_loss: 0.5065 - val_accuracy: 0.8627\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9589\n",
      "Epoch 00055: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.1279 - accuracy: 0.9589 - val_loss: 0.6115 - val_accuracy: 0.8627\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9634\n",
      "Epoch 00056: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 0.1208 - accuracy: 0.9634 - val_loss: 0.7303 - val_accuracy: 0.8627\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9775\n",
      "Epoch 00057: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.0610 - accuracy: 0.9775 - val_loss: 0.8025 - val_accuracy: 0.8627\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9730\n",
      "Epoch 00058: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0806 - accuracy: 0.9730 - val_loss: 0.6672 - val_accuracy: 0.8824\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9639\n",
      "Epoch 00059: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1214 - accuracy: 0.9639 - val_loss: 0.5301 - val_accuracy: 0.9020\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9725\n",
      "Epoch 00060: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0906 - accuracy: 0.9725 - val_loss: 1.7503 - val_accuracy: 0.7059\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9855\n",
      "Epoch 00061: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0500 - accuracy: 0.9855 - val_loss: 0.7531 - val_accuracy: 0.8431\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9785\n",
      "Epoch 00062: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0615 - accuracy: 0.9785 - val_loss: 0.4843 - val_accuracy: 0.8824\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9785\n",
      "Epoch 00063: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0598 - accuracy: 0.9785 - val_loss: 0.6296 - val_accuracy: 0.8627\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.9199\n",
      "Epoch 00064: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.3150 - accuracy: 0.9199 - val_loss: 4.9608 - val_accuracy: 0.5882\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9344\n",
      "Epoch 00065: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.2061 - accuracy: 0.9344 - val_loss: 0.7073 - val_accuracy: 0.8627\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9639\n",
      "Epoch 00066: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.1203 - accuracy: 0.9639 - val_loss: 0.8531 - val_accuracy: 0.8431\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9795\n",
      "Epoch 00067: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 0.0612 - accuracy: 0.9795 - val_loss: 0.8319 - val_accuracy: 0.9020\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9895\n",
      "Epoch 00068: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0306 - accuracy: 0.9895 - val_loss: 0.5834 - val_accuracy: 0.8824\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9629\n",
      "Epoch 00069: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 0.1251 - accuracy: 0.9629 - val_loss: 1.1884 - val_accuracy: 0.7843\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9524\n",
      "Epoch 00070: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 0.2102 - accuracy: 0.9524 - val_loss: 0.8259 - val_accuracy: 0.8627\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9745\n",
      "Epoch 00071: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0765 - accuracy: 0.9745 - val_loss: 0.9175 - val_accuracy: 0.8431\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00072: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.90196\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0207 - accuracy: 0.9940 - val_loss: 0.7132 - val_accuracy: 0.8627\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9960\n",
      "Epoch 00073: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00073: val_accuracy improved from 0.90196 to 0.92157, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 32s 505ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 0.4883 - val_accuracy: 0.9216\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9935\n",
      "Epoch 00074: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0213 - accuracy: 0.9935 - val_loss: 0.4870 - val_accuracy: 0.8627\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9915\n",
      "Epoch 00075: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0241 - accuracy: 0.9915 - val_loss: 1.0146 - val_accuracy: 0.8824\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9885\n",
      "Epoch 00076: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0282 - accuracy: 0.9885 - val_loss: 0.4817 - val_accuracy: 0.9216\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9925\n",
      "Epoch 00077: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 1.1197 - val_accuracy: 0.8627\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9970\n",
      "Epoch 00078: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 1.0423 - val_accuracy: 0.8627\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9945\n",
      "Epoch 00079: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0214 - accuracy: 0.9945 - val_loss: 1.1344 - val_accuracy: 0.8627\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9955\n",
      "Epoch 00080: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0113 - accuracy: 0.9955 - val_loss: 1.2005 - val_accuracy: 0.7843\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9815\n",
      "Epoch 00081: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0542 - accuracy: 0.9815 - val_loss: 1.4697 - val_accuracy: 0.8627\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.9239\n",
      "Epoch 00082: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.3174 - accuracy: 0.9239 - val_loss: 6.6067 - val_accuracy: 0.4510\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9259\n",
      "Epoch 00083: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.2745 - accuracy: 0.9259 - val_loss: 14.5575 - val_accuracy: 0.4902\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9534\n",
      "Epoch 00084: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.1679 - accuracy: 0.9534 - val_loss: 0.8409 - val_accuracy: 0.8627\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9810\n",
      "Epoch 00085: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.92157\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0712 - accuracy: 0.9810 - val_loss: 0.4125 - val_accuracy: 0.9020\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9785\n",
      "Epoch 00086: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00086: val_accuracy improved from 0.92157 to 0.94118, saving model to ./AI_models\\02_04_AI_val_accuracy_index_18.h5\n",
      "63/63 [==============================] - 32s 506ms/step - loss: 0.0632 - accuracy: 0.9785 - val_loss: 0.3712 - val_accuracy: 0.9412\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9750\n",
      "Epoch 00087: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.0826 - accuracy: 0.9750 - val_loss: 1.2949 - val_accuracy: 0.8235\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9845\n",
      "Epoch 00088: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0528 - accuracy: 0.9845 - val_loss: 0.8458 - val_accuracy: 0.8627\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9960\n",
      "Epoch 00089: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0108 - accuracy: 0.9960 - val_loss: 0.8184 - val_accuracy: 0.8824\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9820\n",
      "Epoch 00090: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 0.0689 - accuracy: 0.9820 - val_loss: 0.5246 - val_accuracy: 0.9020\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9835\n",
      "Epoch 00091: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0587 - accuracy: 0.9835 - val_loss: 1.2296 - val_accuracy: 0.7843\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9820\n",
      "Epoch 00092: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0789 - accuracy: 0.9820 - val_loss: 0.6678 - val_accuracy: 0.9020\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9840\n",
      "Epoch 00093: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0594 - accuracy: 0.9840 - val_loss: 0.8297 - val_accuracy: 0.9020\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9910\n",
      "Epoch 00094: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0242 - accuracy: 0.9910 - val_loss: 0.7979 - val_accuracy: 0.8824\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9890\n",
      "Epoch 00095: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0326 - accuracy: 0.9890 - val_loss: 0.7736 - val_accuracy: 0.9020\n",
      "Epoch 96/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9810\n",
      "Epoch 00096: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 0.0746 - accuracy: 0.9810 - val_loss: 0.9295 - val_accuracy: 0.8039\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9830\n",
      "Epoch 00097: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0539 - accuracy: 0.9830 - val_loss: 0.4681 - val_accuracy: 0.9020\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9860\n",
      "Epoch 00098: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 0.0466 - accuracy: 0.9860 - val_loss: 1.0925 - val_accuracy: 0.8627\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9795\n",
      "Epoch 00099: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.0698 - accuracy: 0.9795 - val_loss: 0.8805 - val_accuracy: 0.8627\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9725\n",
      "Epoch 00100: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.1016 - accuracy: 0.9725 - val_loss: 0.8658 - val_accuracy: 0.8627\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1582 - accuracy: 0.9604\n",
      "Epoch 00101: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.1582 - accuracy: 0.9604 - val_loss: 1.6384 - val_accuracy: 0.7451\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9544\n",
      "Epoch 00102: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.1838 - accuracy: 0.9544 - val_loss: 2.0078 - val_accuracy: 0.7451\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9775\n",
      "Epoch 00103: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0700 - accuracy: 0.9775 - val_loss: 1.6608 - val_accuracy: 0.8039\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9835\n",
      "Epoch 00104: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0422 - accuracy: 0.9835 - val_loss: 1.3594 - val_accuracy: 0.8235\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9945\n",
      "Epoch 00105: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 291ms/step - loss: 0.0125 - accuracy: 0.9945 - val_loss: 1.0506 - val_accuracy: 0.8824\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9890\n",
      "Epoch 00106: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0478 - accuracy: 0.9890 - val_loss: 0.5449 - val_accuracy: 0.8824\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9885\n",
      "Epoch 00107: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0243 - accuracy: 0.9885 - val_loss: 0.8623 - val_accuracy: 0.8824\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9935\n",
      "Epoch 00108: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 1.0036 - val_accuracy: 0.9020\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9810\n",
      "Epoch 00109: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0706 - accuracy: 0.9810 - val_loss: 0.7863 - val_accuracy: 0.9020\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9825\n",
      "Epoch 00110: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0598 - accuracy: 0.9825 - val_loss: 0.8357 - val_accuracy: 0.8431\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9715\n",
      "Epoch 00111: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0881 - accuracy: 0.9715 - val_loss: 1.0518 - val_accuracy: 0.8039\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9780\n",
      "Epoch 00112: val_loss did not improve from 0.28768\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.94118\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0761 - accuracy: 0.9780 - val_loss: 2.1895 - val_accuracy: 0.7647\n",
      "Epoch 113/2000\n",
      "20/63 [========>.....................] - ETA: 11s - loss: 0.1431 - accuracy: 0.9678"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ed23b405cc77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreLR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcp2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \"\"\"\n\u001b[0;32m   1814\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit_generator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1816\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m           \u001b[0mnumpy_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index=1\n",
    "result = 0\n",
    "for train_index,val_index in kfold.split(x,y):\n",
    "    if index<=16:\n",
    "        index+=1\n",
    "        continue\n",
    "        \n",
    "    modelpath = './AI_models/02_04_AI_val_loss_index_{}.h5'.format(index)\n",
    "    modelpath2 = './AI_models/02_04_AI_val_accuracy_index_{}.h5'.format(index)\n",
    "    cp = ModelCheckpoint(monitor = 'val_loss',filepath=modelpath,save_best_only=True,verbose=1)\n",
    "    cp2 = ModelCheckpoint(monitor = 'val_accuracy',filepath=modelpath2,save_best_only=True,verbose=1)\n",
    "\n",
    "\n",
    "    x_train = x[train_index]\n",
    "    x_val = x[val_index]\n",
    "    y_train = y[train_index]\n",
    "    y_val = y[val_index]\n",
    "\n",
    "    onehot = OneHotEncoder()\n",
    "    y_train = onehot.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "    y_val = onehot.fit_transform(y_val.reshape(-1,1)).toarray().astype('float32')\n",
    "\n",
    "    train_generator = datagen.flow(x_train,y_train,batch_size=32)\n",
    "    val_generator = datagen.flow(x_val,y_val)\n",
    "    model = modeling()\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n",
    "    model.fit_generator(train_generator,validation_data = val_generator,epochs=epochs,callbacks=[cp,es,reLR,cp2])\n",
    "\n",
    "    model = load_model(modelpath)\n",
    "    model2 = load_model(modelpath2)\n",
    "    df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "    x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred2 = model2.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred,axis=-1)\n",
    "    y_pred2 = np.argmax(y_pred2,axis=-1)\n",
    "    df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "    df_sub['digit'] = y_pred\n",
    "    df_sub.to_csv('./AI_models/loss_kfold_{}.csv'.format(index))\n",
    "    df_sub['digit'] = y_pred2\n",
    "    df_sub.to_csv('./AI_models/accuracy_kfold_{}.csv'.format(index))\n",
    "\n",
    "    print(index, \" 번째 학습을 완료했습니다.\")\n",
    "    index+=1\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "0.905\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onOYBeVy-Mkb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KswTfNSi-Mkc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "0.87\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx1Z4nhC-Mkc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "44FNCmNG-Mkd"
   },
   "outputs": [],
   "source": [
    "model = load_model('./models/02_03_imger_best_index_1.h5')\n",
    "df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzM0rw6U-Mke",
    "outputId": "0312f408-37af-44e5-b673-febb0961614c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "binary_model = []\n",
    "for i in range(0,10):\n",
    "    print(i)\n",
    "    model = load_model('./binary_models/{}_binary.h5'.format(i))\n",
    "    binary_model.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QwofhPdW-Mke"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "def ordering(array):\n",
    "    temp = array.copy()\n",
    "    result = []\n",
    "    for i in range(len(temp)):\n",
    "        sol = np.argmax(temp)\n",
    "        result.append(sol)\n",
    "        temp[sol]=0\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fI2mNTi-Mkf",
    "outputId": "f253edd1-103b-41f5-8832-f5ac667f5a38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6569414e-08, 2.2260668e-08, 4.6740688e-06, ..., 2.5446711e-10,\n",
       "        5.9703314e-07, 5.4656005e-11],\n",
       "       [2.9817595e-16, 2.0647546e-12, 4.4424861e-16, ..., 2.6480063e-10,\n",
       "        8.9294266e-11, 1.0000000e+00],\n",
       "       [2.4707546e-05, 1.3310514e-01, 8.9886552e-03, ..., 9.1972132e-04,\n",
       "        1.2284001e-02, 2.9377347e-06],\n",
       "       ...,\n",
       "       [8.6063210e-09, 7.5784501e-10, 2.6377617e-10, ..., 3.1178827e-13,\n",
       "        1.2005766e-07, 6.1626551e-15],\n",
       "       [1.0054513e-03, 2.9064235e-01, 1.6038346e-05, ..., 1.7447629e-06,\n",
       "        8.1512779e-03, 4.5916289e-03],\n",
       "       [9.9825722e-01, 6.9838888e-16, 2.4161539e-10, ..., 1.8471900e-15,\n",
       "        6.6174334e-13, 1.4705076e-15]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy1domP0-Mkf",
    "outputId": "db3b1dcb-6d43-4fc7-9938-34990934eb64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, ..., 6, 5, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_notyet = np.argmax(y_pred,axis=-1)\n",
    "y_notyet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1xQykxG-Mkf",
    "outputId": "946d6597-cef3-4297-f226-1a1349f5005d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.88120088e-36],\n",
       "       [1.13777095e-36]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model[3].predict(x_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULE3KyHS-Mkg",
    "outputId": "41ead2b9-7d2e-44a9-a6ba-e8c4645752cd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cc4ab4411252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Jhbsfx-Mkg"
   },
   "outputs": [],
   "source": [
    "k=729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPmflg5c-Mkg",
    "outputId": "05282ba4-d754-4522-aa33-73d47b49b612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래모델 :  9 \n",
      "원래모델확률 :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] \n",
      "바이너리 :  1.6463632e-24\n",
      "0 0.0\n",
      "1 5.6854813e-21\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 3.455557e-05\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 1.2415284e-22\n",
      "9 1.6463632e-24\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "for i in [k]:\n",
    "    temp_result = y_notyet[i]\n",
    "    binary_result = binary_model[temp_result].predict(np.array([x_test[i].reshape(28,28,1)]))\n",
    "    a=np.round(y_pred[i],3)\n",
    "    print(\"원래모델 : \",temp_result,'\\n원래모델확률 : ',a,\"\\n바이너리 : \",binary_result[0][0])\n",
    "    print(\"0\",binary_model[0].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"1\",binary_model[1].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"2\",binary_model[2].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"3\",binary_model[3].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"4\",binary_model[4].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"5\",binary_model[5].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"6\",binary_model[6].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"7\",binary_model[7].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"8\",binary_model[8].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"9\",binary_model[9].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEgcogTL-Mkg",
    "outputId": "b1e9adb4-4593-4bda-ae06-bb49fd8bf6ac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-LIIBisa-Mkh",
    "outputId": "aa584c54-1643-48af-eb35-d853835f2d2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8, 8, ..., 6, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred,axis=-1)\n",
    "df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "df_sub['digit'] = y_pred\n",
    "df_sub.to_csv('test_4.csv')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT0ZAdKJ-Mkh",
    "outputId": "4f04bdd5-32cc-4226-9a6a-19a6aa77a588"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 8, ..., 6, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TjhvZQW-Mkh",
    "outputId": "4b83ad9f-6588-4c0e-c152-b4310a2c4f32"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=5\n",
    "df = pd.read_csv(\"preprocessing_150.csv\",index_col=[0])\n",
    "df2 = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "plt.imshow(df.values[k,2:].reshape(28,28).astype('float32'))\n",
    "plt.show()\n",
    "df2 = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "plt.imshow(df2.values[k,2:].reshape(28,28).astype('float32'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHKAVNiD-Mkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7nEeGDn-Mkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc_-Flz2-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wImlSOL--Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj2wn75b-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7YkDa_J-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6a0jyE7-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jYY0jU-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4NiWL-c-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzhsBOPW-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDPP4Lp2-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wsmnbLS-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4vmV_gu-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxBZ_-66-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBoKPs_E-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwaS-Ang-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5_Acvtq-Mkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7pmh6Ej-Mkk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
