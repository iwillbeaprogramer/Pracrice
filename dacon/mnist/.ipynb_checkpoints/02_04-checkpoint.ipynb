{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ooCpvRmT-MkR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dropout,Input,Activation,Dense\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GysUTZTp-MkX"
   },
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s4kX312e-MkY"
   },
   "outputs": [],
   "source": [
    "def modeling():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = inputs\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = _x\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(128,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(256,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(1024,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    _x = Conv2D(512,3,padding='same')(x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    _x = Conv2D(128,3,padding='same')(_x)\n",
    "    _x = BatchNormalization()(_x)\n",
    "    _x = Activation('relu')(_x)\n",
    "    x = x+_x\n",
    "    x = MaxPooling2D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048)(x)\n",
    "    x = Dense(10,activation='softmax')(x)\n",
    "    outputs=x\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cccyJfWO-MkY"
   },
   "source": [
    "# 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W73nGvZM-MkZ"
   },
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "es = EarlyStopping(monitor='val_loss',patience=160)\n",
    "reLR = ReduceLROnPlateau(patience=120,verbose=1,factor=0.8)\n",
    "kfold = StratifiedKFold(n_splits=40,random_state=42,shuffle=True)\n",
    "\n",
    "datagen = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n",
    "datagen2 = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWqLv-B1-MkZ"
   },
   "source": [
    "# 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j6UbQKZk-MkZ",
    "outputId": "26984b86-65c4-4adf-c0dc-bcb09f17d346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 28, 28, 1) (2048,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 9, 0, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "y = df.values[:,0].astype('int32')\n",
    "x = df.values[:,2:].astype('float32')/255.0\n",
    "# print(x.shape,y.shape)               # (2048, 28, 28) (2048,)\n",
    "#onehot = OneHotEncoder()\n",
    "#y = onehot.fit_transform(y.reshape(-1,1)).toarray().astype('float32')\n",
    "x = x.reshape(-1,28,28,1)\n",
    "# x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.15)\n",
    "# x_train = x_train.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# x_val = x_val.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n",
    "# print(x_train.shape,x_val.shape,y_train.shape,y_val.shape)\n",
    "print(x.shape,y.shape) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LmMESLTT-Mkb",
    "outputId": "6e97efa1-d705-4b3d-9e71-4261745da5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 8s - loss: 9.4532 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0967s vs `on_train_batch_end` time: 0.1821s). Check your callbacks.\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6204 - accuracy: 0.1022\n",
      "Epoch 00001: val_loss improved from inf to 22866.07422, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.09615, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 41s 657ms/step - loss: 4.6204 - accuracy: 0.1022 - val_loss: 22866.0742 - val_accuracy: 0.0962\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3174 - accuracy: 0.1373\n",
      "Epoch 00002: val_loss improved from 22866.07422 to 2301.33789, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 20s 321ms/step - loss: 2.3174 - accuracy: 0.1373 - val_loss: 2301.3379 - val_accuracy: 0.0962\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2055 - accuracy: 0.1894\n",
      "Epoch 00003: val_loss improved from 2301.33789 to 1948.64441, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 20s 312ms/step - loss: 2.2055 - accuracy: 0.1894 - val_loss: 1948.6444 - val_accuracy: 0.0962\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1012 - accuracy: 0.2560\n",
      "Epoch 00004: val_loss improved from 1948.64441 to 301.18665, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.09615 to 0.15385, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 33s 531ms/step - loss: 2.1012 - accuracy: 0.2560 - val_loss: 301.1866 - val_accuracy: 0.1538\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7748 - accuracy: 0.3667\n",
      "Epoch 00005: val_loss improved from 301.18665 to 16.41243, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.15385\n",
      "63/63 [==============================] - 20s 319ms/step - loss: 1.7748 - accuracy: 0.3667 - val_loss: 16.4124 - val_accuracy: 0.1346\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5106 - accuracy: 0.4644\n",
      "Epoch 00006: val_loss did not improve from 16.41243\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.15385\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 1.5106 - accuracy: 0.4644 - val_loss: 34.8757 - val_accuracy: 0.1154\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2065 - accuracy: 0.5807\n",
      "Epoch 00007: val_loss did not improve from 16.41243\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.15385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2065 - accuracy: 0.5807 - val_loss: 27.3249 - val_accuracy: 0.0962\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1995 - accuracy: 0.5942\n",
      "Epoch 00008: val_loss did not improve from 16.41243\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.15385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.1995 - accuracy: 0.5942 - val_loss: 50.8659 - val_accuracy: 0.0769\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9700 - accuracy: 0.6759\n",
      "Epoch 00009: val_loss did not improve from 16.41243\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.15385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.9700 - accuracy: 0.6759 - val_loss: 25.9564 - val_accuracy: 0.0962\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8751 - accuracy: 0.7064\n",
      "Epoch 00010: val_loss improved from 16.41243 to 9.43235, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.15385\n",
      "63/63 [==============================] - 35s 561ms/step - loss: 0.8751 - accuracy: 0.7064 - val_loss: 9.4323 - val_accuracy: 0.1538\n",
      "Epoch 11/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7592 - accuracy: 0.7475\n",
      "Epoch 00011: val_loss improved from 9.43235 to 1.12627, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.15385 to 0.65385, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 35s 559ms/step - loss: 0.7592 - accuracy: 0.7475 - val_loss: 1.1263 - val_accuracy: 0.6538\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6502 - accuracy: 0.7861\n",
      "Epoch 00012: val_loss did not improve from 1.12627\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.65385\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.6502 - accuracy: 0.7861 - val_loss: 1.1601 - val_accuracy: 0.6346\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7750 - accuracy: 0.7445\n",
      "Epoch 00013: val_loss did not improve from 1.12627\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.65385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.7750 - accuracy: 0.7445 - val_loss: 3.9947 - val_accuracy: 0.2692\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.8196\n",
      "Epoch 00014: val_loss did not improve from 1.12627\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.65385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.5191 - accuracy: 0.8196 - val_loss: 1.4577 - val_accuracy: 0.5962\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.8241\n",
      "Epoch 00015: val_loss improved from 1.12627 to 1.03402, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.65385 to 0.69231, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 46s 728ms/step - loss: 0.5279 - accuracy: 0.8241 - val_loss: 1.0340 - val_accuracy: 0.6923\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.8592\n",
      "Epoch 00016: val_loss did not improve from 1.03402\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.69231\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.4139 - accuracy: 0.8592 - val_loss: 2.9358 - val_accuracy: 0.4615\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4525 - accuracy: 0.8527\n",
      "Epoch 00017: val_loss did not improve from 1.03402\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.69231\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.4525 - accuracy: 0.8527 - val_loss: 3.1659 - val_accuracy: 0.5192\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.8537\n",
      "Epoch 00018: val_loss improved from 1.03402 to 0.94110, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.69231 to 0.75000, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 44s 697ms/step - loss: 0.4355 - accuracy: 0.8537 - val_loss: 0.9411 - val_accuracy: 0.7500\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.8893\n",
      "Epoch 00019: val_loss did not improve from 0.94110\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.75000\n",
      "63/63 [==============================] - 17s 275ms/step - loss: 0.3283 - accuracy: 0.8893 - val_loss: 1.3248 - val_accuracy: 0.6923\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.8928\n",
      "Epoch 00020: val_loss improved from 0.94110 to 0.57692, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.75000 to 0.82692, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 45s 709ms/step - loss: 0.3187 - accuracy: 0.8928 - val_loss: 0.5769 - val_accuracy: 0.8269\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9028\n",
      "Epoch 00021: val_loss did not improve from 0.57692\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.82692\n",
      "63/63 [==============================] - 17s 275ms/step - loss: 0.2864 - accuracy: 0.9028 - val_loss: 0.6155 - val_accuracy: 0.8077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9018\n",
      "Epoch 00022: val_loss did not improve from 0.57692\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.82692\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.2785 - accuracy: 0.9018 - val_loss: 0.8888 - val_accuracy: 0.7885\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.9203\n",
      "Epoch 00023: val_loss did not improve from 0.57692\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.82692 to 0.84615, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 30s 483ms/step - loss: 0.2524 - accuracy: 0.9203 - val_loss: 0.6890 - val_accuracy: 0.8462\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.9284\n",
      "Epoch 00024: val_loss improved from 0.57692 to 0.56863, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 33s 527ms/step - loss: 0.2107 - accuracy: 0.9284 - val_loss: 0.5686 - val_accuracy: 0.7500\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.9058\n",
      "Epoch 00025: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.2963 - accuracy: 0.9058 - val_loss: 1.0328 - val_accuracy: 0.7885\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9133\n",
      "Epoch 00026: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.2628 - accuracy: 0.9133 - val_loss: 0.6576 - val_accuracy: 0.7885\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.9073\n",
      "Epoch 00027: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.2727 - accuracy: 0.9073 - val_loss: 1.0926 - val_accuracy: 0.7115\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9078\n",
      "Epoch 00028: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2757 - accuracy: 0.9078 - val_loss: 0.8235 - val_accuracy: 0.8462\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9379\n",
      "Epoch 00029: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1634 - accuracy: 0.9379 - val_loss: 1.0570 - val_accuracy: 0.8077\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9299\n",
      "Epoch 00030: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1996 - accuracy: 0.9299 - val_loss: 1.3348 - val_accuracy: 0.6923\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9193\n",
      "Epoch 00031: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2345 - accuracy: 0.9193 - val_loss: 1.3751 - val_accuracy: 0.7115\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9289\n",
      "Epoch 00032: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2145 - accuracy: 0.9289 - val_loss: 1.1013 - val_accuracy: 0.7115\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9349\n",
      "Epoch 00033: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1655 - accuracy: 0.9349 - val_loss: 1.8830 - val_accuracy: 0.6154\n",
      "Epoch 34/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9439\n",
      "Epoch 00034: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1866 - accuracy: 0.9439 - val_loss: 0.9023 - val_accuracy: 0.7692\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9519\n",
      "Epoch 00035: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1330 - accuracy: 0.9519 - val_loss: 1.3511 - val_accuracy: 0.7692\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.9304\n",
      "Epoch 00036: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2086 - accuracy: 0.9304 - val_loss: 0.9569 - val_accuracy: 0.7692\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9534\n",
      "Epoch 00037: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1346 - accuracy: 0.9534 - val_loss: 1.0314 - val_accuracy: 0.7692\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9614\n",
      "Epoch 00038: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1146 - accuracy: 0.9614 - val_loss: 1.0011 - val_accuracy: 0.8269\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1495 - accuracy: 0.9489\n",
      "Epoch 00039: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1495 - accuracy: 0.9489 - val_loss: 0.8208 - val_accuracy: 0.7885\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9514\n",
      "Epoch 00040: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1564 - accuracy: 0.9514 - val_loss: 0.9249 - val_accuracy: 0.7885\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9474\n",
      "Epoch 00041: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1522 - accuracy: 0.9474 - val_loss: 3.1811 - val_accuracy: 0.5385\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9639\n",
      "Epoch 00042: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1100 - accuracy: 0.9639 - val_loss: 3.9531 - val_accuracy: 0.4423\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9589\n",
      "Epoch 00043: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1236 - accuracy: 0.9589 - val_loss: 3.5416 - val_accuracy: 0.4808\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9689\n",
      "Epoch 00044: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0873 - accuracy: 0.9689 - val_loss: 0.6213 - val_accuracy: 0.8077\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9744\n",
      "Epoch 00045: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0749 - accuracy: 0.9744 - val_loss: 0.9508 - val_accuracy: 0.8269\n",
      "Epoch 46/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9719\n",
      "Epoch 00046: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00046: val_accuracy improved from 0.84615 to 0.86538, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 30s 484ms/step - loss: 0.0794 - accuracy: 0.9719 - val_loss: 0.6076 - val_accuracy: 0.8654\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2245 - accuracy: 0.9359\n",
      "Epoch 00047: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.2245 - accuracy: 0.9359 - val_loss: 2.0827 - val_accuracy: 0.5577\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9449\n",
      "Epoch 00048: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.1739 - accuracy: 0.9449 - val_loss: 2.1980 - val_accuracy: 0.6923\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9484\n",
      "Epoch 00049: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1649 - accuracy: 0.9484 - val_loss: 1.1096 - val_accuracy: 0.7885\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9719\n",
      "Epoch 00050: val_loss did not improve from 0.56863\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0826 - accuracy: 0.9719 - val_loss: 1.3830 - val_accuracy: 0.7692\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9664\n",
      "Epoch 00051: val_loss improved from 0.56863 to 0.50808, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 31s 486ms/step - loss: 0.1045 - accuracy: 0.9664 - val_loss: 0.5081 - val_accuracy: 0.8654\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.9529\n",
      "Epoch 00052: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.1456 - accuracy: 0.9529 - val_loss: 4.2652 - val_accuracy: 0.4038\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9699\n",
      "Epoch 00053: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0803 - accuracy: 0.9699 - val_loss: 1.9614 - val_accuracy: 0.6154\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9870\n",
      "Epoch 00054: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0398 - accuracy: 0.9870 - val_loss: 2.3267 - val_accuracy: 0.6346\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9739\n",
      "Epoch 00055: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0783 - accuracy: 0.9739 - val_loss: 3.3807 - val_accuracy: 0.5000\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9729\n",
      "Epoch 00056: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0840 - accuracy: 0.9729 - val_loss: 1.3406 - val_accuracy: 0.7692\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9554\n",
      "Epoch 00057: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1585 - accuracy: 0.9554 - val_loss: 1.3062 - val_accuracy: 0.7692\n",
      "Epoch 58/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9604\n",
      "Epoch 00058: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1278 - accuracy: 0.9604 - val_loss: 0.7420 - val_accuracy: 0.8462\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9684\n",
      "Epoch 00059: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00059: val_accuracy improved from 0.86538 to 0.88462, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 31s 492ms/step - loss: 0.0847 - accuracy: 0.9684 - val_loss: 0.5937 - val_accuracy: 0.8846\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9800\n",
      "Epoch 00060: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.0555 - accuracy: 0.9800 - val_loss: 0.7266 - val_accuracy: 0.8846\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9664\n",
      "Epoch 00061: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.1065 - accuracy: 0.9664 - val_loss: 0.7284 - val_accuracy: 0.8269\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9694\n",
      "Epoch 00062: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0806 - accuracy: 0.9694 - val_loss: 1.0331 - val_accuracy: 0.8269\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9529\n",
      "Epoch 00063: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1800 - accuracy: 0.9529 - val_loss: 2.3662 - val_accuracy: 0.6923\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9449\n",
      "Epoch 00064: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1965 - accuracy: 0.9449 - val_loss: 0.8880 - val_accuracy: 0.8269\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9534\n",
      "Epoch 00065: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.1454 - accuracy: 0.9534 - val_loss: 1.2508 - val_accuracy: 0.8462\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9739\n",
      "Epoch 00066: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0892 - accuracy: 0.9739 - val_loss: 0.7229 - val_accuracy: 0.8846\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9825\n",
      "Epoch 00067: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0594 - accuracy: 0.9825 - val_loss: 0.8230 - val_accuracy: 0.7885\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9835\n",
      "Epoch 00068: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0389 - accuracy: 0.9835 - val_loss: 0.8291 - val_accuracy: 0.8462\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9820\n",
      "Epoch 00069: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0674 - accuracy: 0.9820 - val_loss: 0.6902 - val_accuracy: 0.8269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9910\n",
      "Epoch 00070: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0302 - accuracy: 0.9910 - val_loss: 0.6221 - val_accuracy: 0.8654\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9890\n",
      "Epoch 00071: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0269 - accuracy: 0.9890 - val_loss: 0.9171 - val_accuracy: 0.8269\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9604\n",
      "Epoch 00072: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1577 - accuracy: 0.9604 - val_loss: 1.6301 - val_accuracy: 0.7115\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9389\n",
      "Epoch 00073: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2186 - accuracy: 0.9389 - val_loss: 1.6602 - val_accuracy: 0.7885\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9489\n",
      "Epoch 00074: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1779 - accuracy: 0.9489 - val_loss: 0.8255 - val_accuracy: 0.8654\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9785\n",
      "Epoch 00075: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0686 - accuracy: 0.9785 - val_loss: 1.4531 - val_accuracy: 0.8077\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9744\n",
      "Epoch 00076: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0839 - accuracy: 0.9744 - val_loss: 1.1298 - val_accuracy: 0.7885\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9835\n",
      "Epoch 00077: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0508 - accuracy: 0.9835 - val_loss: 0.6929 - val_accuracy: 0.8269\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9785\n",
      "Epoch 00078: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0639 - accuracy: 0.9785 - val_loss: 0.5396 - val_accuracy: 0.8654\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9825\n",
      "Epoch 00079: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0622 - accuracy: 0.9825 - val_loss: 0.5614 - val_accuracy: 0.8846\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9875\n",
      "Epoch 00080: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0331 - accuracy: 0.9875 - val_loss: 0.9722 - val_accuracy: 0.7885\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9925\n",
      "Epoch 00081: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 1.0781 - val_accuracy: 0.8269\n",
      "Epoch 82/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9890\n",
      "Epoch 00082: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0417 - accuracy: 0.9890 - val_loss: 2.0268 - val_accuracy: 0.7500\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9714\n",
      "Epoch 00083: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0981 - accuracy: 0.9714 - val_loss: 1.1276 - val_accuracy: 0.8654\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9739\n",
      "Epoch 00084: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0883 - accuracy: 0.9739 - val_loss: 2.2704 - val_accuracy: 0.7115\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9790\n",
      "Epoch 00085: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0697 - accuracy: 0.9790 - val_loss: 2.2174 - val_accuracy: 0.7115\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9850\n",
      "Epoch 00086: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0479 - accuracy: 0.9850 - val_loss: 1.5725 - val_accuracy: 0.7885\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9674\n",
      "Epoch 00087: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1038 - accuracy: 0.9674 - val_loss: 0.9150 - val_accuracy: 0.8077\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9484\n",
      "Epoch 00088: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2297 - accuracy: 0.9484 - val_loss: 21.4006 - val_accuracy: 0.2500\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.9198\n",
      "Epoch 00089: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2748 - accuracy: 0.9198 - val_loss: 1.4214 - val_accuracy: 0.7692\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9790\n",
      "Epoch 00090: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0683 - accuracy: 0.9790 - val_loss: 0.8687 - val_accuracy: 0.8654\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9760\n",
      "Epoch 00091: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0687 - accuracy: 0.9760 - val_loss: 0.7988 - val_accuracy: 0.8462\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9755\n",
      "Epoch 00092: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0700 - accuracy: 0.9755 - val_loss: 0.8667 - val_accuracy: 0.8654\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9785\n",
      "Epoch 00093: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0722 - accuracy: 0.9785 - val_loss: 1.5073 - val_accuracy: 0.7692\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9900\n",
      "Epoch 00094: val_loss did not improve from 0.50808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0293 - accuracy: 0.9900 - val_loss: 1.0907 - val_accuracy: 0.8269\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9935\n",
      "Epoch 00095: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.88462\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0173 - accuracy: 0.9935 - val_loss: 0.8842 - val_accuracy: 0.8654\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9870\n",
      "Epoch 00096: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00096: val_accuracy improved from 0.88462 to 0.90385, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 32s 504ms/step - loss: 0.0396 - accuracy: 0.9870 - val_loss: 0.8656 - val_accuracy: 0.9038\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9865\n",
      "Epoch 00097: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0498 - accuracy: 0.9865 - val_loss: 1.4692 - val_accuracy: 0.8077\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9574\n",
      "Epoch 00098: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1766 - accuracy: 0.9574 - val_loss: 2.2464 - val_accuracy: 0.6923\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9805\n",
      "Epoch 00099: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0571 - accuracy: 0.9805 - val_loss: 1.8988 - val_accuracy: 0.7500\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9699\n",
      "Epoch 00100: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1010 - accuracy: 0.9699 - val_loss: 1.6337 - val_accuracy: 0.8269\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9729\n",
      "Epoch 00101: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1001 - accuracy: 0.9729 - val_loss: 1.4929 - val_accuracy: 0.8462\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9810\n",
      "Epoch 00102: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0670 - accuracy: 0.9810 - val_loss: 1.3265 - val_accuracy: 0.8654\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9664\n",
      "Epoch 00103: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1240 - accuracy: 0.9664 - val_loss: 0.9713 - val_accuracy: 0.8269\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9860\n",
      "Epoch 00104: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0364 - accuracy: 0.9860 - val_loss: 1.1200 - val_accuracy: 0.8269\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9805\n",
      "Epoch 00105: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0622 - accuracy: 0.9805 - val_loss: 2.5881 - val_accuracy: 0.6923\n",
      "Epoch 106/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9960\n",
      "Epoch 00106: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0092 - accuracy: 0.9960 - val_loss: 0.8823 - val_accuracy: 0.8269\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9945\n",
      "Epoch 00107: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0261 - accuracy: 0.9945 - val_loss: 0.7097 - val_accuracy: 0.8846\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9835\n",
      "Epoch 00108: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0512 - accuracy: 0.9835 - val_loss: 1.2740 - val_accuracy: 0.7885\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9815\n",
      "Epoch 00109: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0616 - accuracy: 0.9815 - val_loss: 0.7981 - val_accuracy: 0.8462\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1408 - accuracy: 0.9649\n",
      "Epoch 00110: val_loss did not improve from 0.50808\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1408 - accuracy: 0.9649 - val_loss: 10.4172 - val_accuracy: 0.3077\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9554\n",
      "Epoch 00111: val_loss improved from 0.50808 to 0.46461, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 31s 490ms/step - loss: 0.1533 - accuracy: 0.9554 - val_loss: 0.4646 - val_accuracy: 0.9038\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9719\n",
      "Epoch 00112: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0855 - accuracy: 0.9719 - val_loss: 2.0191 - val_accuracy: 0.7692\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9669\n",
      "Epoch 00113: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.1227 - accuracy: 0.9669 - val_loss: 0.9076 - val_accuracy: 0.8269\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9855\n",
      "Epoch 00114: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0439 - accuracy: 0.9855 - val_loss: 1.1939 - val_accuracy: 0.8077\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9985\n",
      "Epoch 00115: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.9053 - val_accuracy: 0.8846\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9985\n",
      "Epoch 00116: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.9350 - val_accuracy: 0.8077\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9900\n",
      "Epoch 00117: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0365 - accuracy: 0.9900 - val_loss: 1.2050 - val_accuracy: 0.8077\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9744\n",
      "Epoch 00118: val_loss did not improve from 0.46461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0933 - accuracy: 0.9744 - val_loss: 1.5913 - val_accuracy: 0.8077\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9875\n",
      "Epoch 00119: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0385 - accuracy: 0.9875 - val_loss: 0.8417 - val_accuracy: 0.8462\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9815\n",
      "Epoch 00120: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0789 - accuracy: 0.9815 - val_loss: 1.1887 - val_accuracy: 0.8462\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9845\n",
      "Epoch 00121: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0426 - accuracy: 0.9845 - val_loss: 1.0540 - val_accuracy: 0.8269\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9760\n",
      "Epoch 00122: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0850 - accuracy: 0.9760 - val_loss: 1.6355 - val_accuracy: 0.8269\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9659\n",
      "Epoch 00123: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1237 - accuracy: 0.9659 - val_loss: 1.8085 - val_accuracy: 0.8077\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9775\n",
      "Epoch 00124: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0916 - accuracy: 0.9775 - val_loss: 1.2802 - val_accuracy: 0.8269\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9775\n",
      "Epoch 00125: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0791 - accuracy: 0.9775 - val_loss: 0.7969 - val_accuracy: 0.8846\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9785\n",
      "Epoch 00126: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0853 - accuracy: 0.9785 - val_loss: 0.6047 - val_accuracy: 0.8846\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9820\n",
      "Epoch 00127: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0530 - accuracy: 0.9820 - val_loss: 1.6725 - val_accuracy: 0.8077\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9895\n",
      "Epoch 00128: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0403 - accuracy: 0.9895 - val_loss: 1.7762 - val_accuracy: 0.8462\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9704\n",
      "Epoch 00129: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1167 - accuracy: 0.9704 - val_loss: 1.6806 - val_accuracy: 0.8077\n",
      "Epoch 130/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9760\n",
      "Epoch 00130: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1012 - accuracy: 0.9760 - val_loss: 1.6848 - val_accuracy: 0.8269\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9800\n",
      "Epoch 00131: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0690 - accuracy: 0.9800 - val_loss: 1.3823 - val_accuracy: 0.8462\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9815\n",
      "Epoch 00132: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0691 - accuracy: 0.9815 - val_loss: 1.9420 - val_accuracy: 0.7692\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9870\n",
      "Epoch 00133: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0338 - accuracy: 0.9870 - val_loss: 1.0451 - val_accuracy: 0.8269\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9885\n",
      "Epoch 00134: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0361 - accuracy: 0.9885 - val_loss: 3.8238 - val_accuracy: 0.5769\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9970\n",
      "Epoch 00135: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 1.1853 - val_accuracy: 0.8462\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9990\n",
      "Epoch 00136: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.6932 - val_accuracy: 0.8462\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9990\n",
      "Epoch 00137: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0026 - accuracy: 0.9990 - val_loss: 0.9671 - val_accuracy: 0.8269\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 00138: val_loss did not improve from 0.46461\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.6032 - val_accuracy: 0.8654\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9930\n",
      "Epoch 00139: val_loss improved from 0.46461 to 0.41590, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 30s 480ms/step - loss: 0.0308 - accuracy: 0.9930 - val_loss: 0.4159 - val_accuracy: 0.8654\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9830\n",
      "Epoch 00140: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.0747 - accuracy: 0.9830 - val_loss: 1.1210 - val_accuracy: 0.8077\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9739\n",
      "Epoch 00141: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.0954 - accuracy: 0.9739 - val_loss: 1.4615 - val_accuracy: 0.7500\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9800\n",
      "Epoch 00142: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0830 - accuracy: 0.9800 - val_loss: 1.6165 - val_accuracy: 0.8077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9875\n",
      "Epoch 00143: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0372 - accuracy: 0.9875 - val_loss: 0.8140 - val_accuracy: 0.8846\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9950\n",
      "Epoch 00144: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0134 - accuracy: 0.9950 - val_loss: 1.2367 - val_accuracy: 0.8269\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 00145: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 1.2981 - val_accuracy: 0.8654\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9915\n",
      "Epoch 00146: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0253 - accuracy: 0.9915 - val_loss: 1.1400 - val_accuracy: 0.7692\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9775\n",
      "Epoch 00147: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0756 - accuracy: 0.9775 - val_loss: 1.0500 - val_accuracy: 0.8846\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9835\n",
      "Epoch 00148: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0582 - accuracy: 0.9835 - val_loss: 0.6125 - val_accuracy: 0.8654\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9855\n",
      "Epoch 00149: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00149: val_accuracy improved from 0.90385 to 0.94231, saving model to ./AI_models\\02_04_AI_val_accuracy_index_1.h5\n",
      "63/63 [==============================] - 31s 491ms/step - loss: 0.0498 - accuracy: 0.9855 - val_loss: 0.5049 - val_accuracy: 0.9423\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9765\n",
      "Epoch 00150: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0823 - accuracy: 0.9765 - val_loss: 0.8930 - val_accuracy: 0.8654\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9539\n",
      "Epoch 00151: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.2248 - accuracy: 0.9539 - val_loss: 19.4572 - val_accuracy: 0.3654\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9749\n",
      "Epoch 00152: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0818 - accuracy: 0.9749 - val_loss: 1.2285 - val_accuracy: 0.8462\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9905\n",
      "Epoch 00153: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0286 - accuracy: 0.9905 - val_loss: 1.3901 - val_accuracy: 0.7692\n",
      "Epoch 154/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9910\n",
      "Epoch 00154: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0297 - accuracy: 0.9910 - val_loss: 1.1033 - val_accuracy: 0.8462\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9950\n",
      "Epoch 00155: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0114 - accuracy: 0.9950 - val_loss: 0.8296 - val_accuracy: 0.8269\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9960\n",
      "Epoch 00156: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 0.5149 - val_accuracy: 0.8846\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9950\n",
      "Epoch 00157: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0166 - accuracy: 0.9950 - val_loss: 1.6018 - val_accuracy: 0.8269\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9940\n",
      "Epoch 00158: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0156 - accuracy: 0.9940 - val_loss: 1.3820 - val_accuracy: 0.8654\n",
      "Epoch 159/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9980\n",
      "Epoch 00159: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0116 - accuracy: 0.9980 - val_loss: 1.4975 - val_accuracy: 0.8462\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9674\n",
      "Epoch 00160: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1396 - accuracy: 0.9674 - val_loss: 1.0593 - val_accuracy: 0.8269\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9875\n",
      "Epoch 00161: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0505 - accuracy: 0.9875 - val_loss: 1.3379 - val_accuracy: 0.8269\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9935\n",
      "Epoch 00162: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0213 - accuracy: 0.9935 - val_loss: 0.7175 - val_accuracy: 0.8846\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9925\n",
      "Epoch 00163: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0241 - accuracy: 0.9925 - val_loss: 1.1837 - val_accuracy: 0.7885\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9900\n",
      "Epoch 00164: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0478 - accuracy: 0.9900 - val_loss: 1.1929 - val_accuracy: 0.8269\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9875\n",
      "Epoch 00165: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0623 - accuracy: 0.9875 - val_loss: 1.0204 - val_accuracy: 0.8654\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9915\n",
      "Epoch 00166: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0207 - accuracy: 0.9915 - val_loss: 1.4646 - val_accuracy: 0.8269\n",
      "Epoch 167/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9875\n",
      "Epoch 00167: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0467 - accuracy: 0.9875 - val_loss: 1.5236 - val_accuracy: 0.8077\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9810\n",
      "Epoch 00168: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0656 - accuracy: 0.9810 - val_loss: 1.1949 - val_accuracy: 0.7885\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9970\n",
      "Epoch 00169: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 1.4942 - val_accuracy: 0.8269\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9935\n",
      "Epoch 00170: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0289 - accuracy: 0.9935 - val_loss: 0.9412 - val_accuracy: 0.9038\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9965\n",
      "Epoch 00171: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0127 - accuracy: 0.9965 - val_loss: 1.0759 - val_accuracy: 0.8846\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9975\n",
      "Epoch 00172: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 1.3200 - val_accuracy: 0.8269\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9985\n",
      "Epoch 00173: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0030 - accuracy: 0.9985 - val_loss: 0.9676 - val_accuracy: 0.8654\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9990\n",
      "Epoch 00174: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 1.1912 - val_accuracy: 0.8462\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.1475e-04 - accuracy: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.1475e-04 - accuracy: 1.0000 - val_loss: 0.8617 - val_accuracy: 0.8462\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0669e-04 - accuracy: 1.0000\n",
      "Epoch 00176: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.0669e-04 - accuracy: 1.0000 - val_loss: 0.5235 - val_accuracy: 0.9038\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8459e-04 - accuracy: 1.0000\n",
      "Epoch 00177: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.8459e-04 - accuracy: 1.0000 - val_loss: 0.5755 - val_accuracy: 0.9038\n",
      "Epoch 178/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9935\n",
      "Epoch 00178: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0266 - accuracy: 0.9935 - val_loss: 1.9002 - val_accuracy: 0.8077\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9795\n",
      "Epoch 00179: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0994 - accuracy: 0.9795 - val_loss: 1.3466 - val_accuracy: 0.8846\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9489\n",
      "Epoch 00180: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2648 - accuracy: 0.9489 - val_loss: 2.5595 - val_accuracy: 0.7308\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9609\n",
      "Epoch 00181: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1808 - accuracy: 0.9609 - val_loss: 2.4428 - val_accuracy: 0.7692\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9674\n",
      "Epoch 00182: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1401 - accuracy: 0.9674 - val_loss: 1.7800 - val_accuracy: 0.8077\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9765\n",
      "Epoch 00183: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1068 - accuracy: 0.9765 - val_loss: 1.8560 - val_accuracy: 0.8077\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9744\n",
      "Epoch 00184: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1015 - accuracy: 0.9744 - val_loss: 0.8419 - val_accuracy: 0.9231\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9945\n",
      "Epoch 00185: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0187 - accuracy: 0.9945 - val_loss: 1.5738 - val_accuracy: 0.7885\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9875\n",
      "Epoch 00186: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0525 - accuracy: 0.9875 - val_loss: 1.5546 - val_accuracy: 0.8269\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9965\n",
      "Epoch 00187: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0078 - accuracy: 0.9965 - val_loss: 0.7237 - val_accuracy: 0.8269\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9985\n",
      "Epoch 00188: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.6512 - val_accuracy: 0.8846\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9990\n",
      "Epoch 00189: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0017 - accuracy: 0.9990 - val_loss: 0.6200 - val_accuracy: 0.8654\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.8346e-04 - accuracy: 1.0000\n",
      "Epoch 00190: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.8346e-04 - accuracy: 1.0000 - val_loss: 0.5414 - val_accuracy: 0.9038\n",
      "Epoch 191/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 4.4703e-04 - accuracy: 1.0000\n",
      "Epoch 00191: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.4703e-04 - accuracy: 1.0000 - val_loss: 0.5387 - val_accuracy: 0.9038\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.4935e-04 - accuracy: 0.9995\n",
      "Epoch 00192: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 5.4935e-04 - accuracy: 0.9995 - val_loss: 0.4434 - val_accuracy: 0.9038\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.4768e-05 - accuracy: 1.0000\n",
      "Epoch 00193: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.4768e-05 - accuracy: 1.0000 - val_loss: 0.5107 - val_accuracy: 0.9038\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.0175e-05 - accuracy: 1.0000\n",
      "Epoch 00194: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.0175e-05 - accuracy: 1.0000 - val_loss: 0.6961 - val_accuracy: 0.9038\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2873e-04 - accuracy: 1.0000\n",
      "Epoch 00195: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.2873e-04 - accuracy: 1.0000 - val_loss: 0.8521 - val_accuracy: 0.8462\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.2910e-05 - accuracy: 1.0000\n",
      "Epoch 00196: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.2910e-05 - accuracy: 1.0000 - val_loss: 0.6946 - val_accuracy: 0.8846\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.9778e-05 - accuracy: 1.0000\n",
      "Epoch 00197: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.9778e-05 - accuracy: 1.0000 - val_loss: 0.5880 - val_accuracy: 0.8846\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.5987e-05 - accuracy: 1.0000\n",
      "Epoch 00198: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.5987e-05 - accuracy: 1.0000 - val_loss: 0.6146 - val_accuracy: 0.8846\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.5744e-04 - accuracy: 0.9995\n",
      "Epoch 00199: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 8.5744e-04 - accuracy: 0.9995 - val_loss: 0.4266 - val_accuracy: 0.8846\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.2838e-04 - accuracy: 1.0000\n",
      "Epoch 00200: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.2838e-04 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.8462\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.5940e-05 - accuracy: 1.0000\n",
      "Epoch 00201: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00201: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.5940e-05 - accuracy: 1.0000 - val_loss: 0.8685 - val_accuracy: 0.8654\n",
      "Epoch 202/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.3036e-05 - accuracy: 1.0000\n",
      "Epoch 00202: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00202: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.3036e-05 - accuracy: 1.0000 - val_loss: 0.7275 - val_accuracy: 0.8846\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5553e-05 - accuracy: 1.0000\n",
      "Epoch 00203: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00203: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.5553e-05 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 0.8846\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.1385e-05 - accuracy: 1.0000\n",
      "Epoch 00204: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00204: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.1385e-05 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 0.9038\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.2597e-05 - accuracy: 1.0000\n",
      "Epoch 00205: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00205: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.2597e-05 - accuracy: 1.0000 - val_loss: 0.5627 - val_accuracy: 0.8654\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.4641e-05 - accuracy: 1.0000\n",
      "Epoch 00206: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00206: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.4641e-05 - accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 0.8654\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.9848e-05 - accuracy: 1.0000\n",
      "Epoch 00207: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00207: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.9848e-05 - accuracy: 1.0000 - val_loss: 0.7477 - val_accuracy: 0.8654\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1714e-05 - accuracy: 1.0000\n",
      "Epoch 00208: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00208: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.1714e-05 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 0.9038\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3861e-05 - accuracy: 1.0000\n",
      "Epoch 00209: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00209: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.3861e-05 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.8846\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5200e-05 - accuracy: 1.0000\n",
      "Epoch 00210: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00210: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.5200e-05 - accuracy: 1.0000 - val_loss: 0.6188 - val_accuracy: 0.8654\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7286e-05 - accuracy: 1.0000\n",
      "Epoch 00211: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00211: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.7286e-05 - accuracy: 1.0000 - val_loss: 0.6686 - val_accuracy: 0.8846\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.6928e-05 - accuracy: 1.0000\n",
      "Epoch 00212: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00212: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 5.6928e-05 - accuracy: 1.0000 - val_loss: 0.7004 - val_accuracy: 0.8846\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8020e-05 - accuracy: 1.0000\n",
      "Epoch 00213: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00213: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.8020e-05 - accuracy: 1.0000 - val_loss: 0.5927 - val_accuracy: 0.9231\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.4608e-05 - accuracy: 1.0000\n",
      "Epoch 00214: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00214: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.4608e-05 - accuracy: 1.0000 - val_loss: 0.8808 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7555e-05 - accuracy: 1.0000\n",
      "Epoch 00215: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00215: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 1.7555e-05 - accuracy: 1.0000 - val_loss: 0.6890 - val_accuracy: 0.8654\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.8184e-05 - accuracy: 1.0000\n",
      "Epoch 00216: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00216: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.8184e-05 - accuracy: 1.0000 - val_loss: 0.8145 - val_accuracy: 0.8462\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5984e-05 - accuracy: 1.0000\n",
      "Epoch 00217: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00217: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.5984e-05 - accuracy: 1.0000 - val_loss: 0.6454 - val_accuracy: 0.8846\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2512e-05 - accuracy: 1.0000\n",
      "Epoch 00218: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00218: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.2512e-05 - accuracy: 1.0000 - val_loss: 0.6530 - val_accuracy: 0.8846\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3123e-05 - accuracy: 1.0000\n",
      "Epoch 00219: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00219: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.3123e-05 - accuracy: 1.0000 - val_loss: 0.5009 - val_accuracy: 0.9038\n",
      "Epoch 220/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9561e-05 - accuracy: 1.0000\n",
      "Epoch 00220: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00220: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9561e-05 - accuracy: 1.0000 - val_loss: 0.4798 - val_accuracy: 0.9231\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0182e-05 - accuracy: 1.0000\n",
      "Epoch 00221: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00221: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.0182e-05 - accuracy: 1.0000 - val_loss: 0.6734 - val_accuracy: 0.8654\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1891e-05 - accuracy: 1.0000\n",
      "Epoch 00222: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00222: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.1891e-05 - accuracy: 1.0000 - val_loss: 0.7229 - val_accuracy: 0.8654\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.1120e-05 - accuracy: 1.0000\n",
      "Epoch 00223: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00223: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.1120e-05 - accuracy: 1.0000 - val_loss: 0.5899 - val_accuracy: 0.8846\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9990\n",
      "Epoch 00224: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00224: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.6363 - val_accuracy: 0.9038\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9509\n",
      "Epoch 00225: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00225: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2676 - accuracy: 0.9509 - val_loss: 2.8917 - val_accuracy: 0.7115\n",
      "Epoch 226/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.9138\n",
      "Epoch 00226: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00226: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.4021 - accuracy: 0.9138 - val_loss: 2.3463 - val_accuracy: 0.6923\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9614\n",
      "Epoch 00227: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00227: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1641 - accuracy: 0.9614 - val_loss: 1.1888 - val_accuracy: 0.7692\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9749\n",
      "Epoch 00228: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00228: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0790 - accuracy: 0.9749 - val_loss: 0.7477 - val_accuracy: 0.8846\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9895\n",
      "Epoch 00229: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00229: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0307 - accuracy: 0.9895 - val_loss: 0.8465 - val_accuracy: 0.8462\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9915\n",
      "Epoch 00230: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00230: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0299 - accuracy: 0.9915 - val_loss: 0.7438 - val_accuracy: 0.9038\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9875\n",
      "Epoch 00231: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00231: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0354 - accuracy: 0.9875 - val_loss: 1.1030 - val_accuracy: 0.8462\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9870\n",
      "Epoch 00232: val_loss did not improve from 0.41590\n",
      "\n",
      "Epoch 00232: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0469 - accuracy: 0.9870 - val_loss: 1.0561 - val_accuracy: 0.8462\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9945\n",
      "Epoch 00233: val_loss improved from 0.41590 to 0.39811, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00233: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 31s 489ms/step - loss: 0.0187 - accuracy: 0.9945 - val_loss: 0.3981 - val_accuracy: 0.8654\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9980\n",
      "Epoch 00234: val_loss did not improve from 0.39811\n",
      "\n",
      "Epoch 00234: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.5596 - val_accuracy: 0.9038\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
      "Epoch 00235: val_loss improved from 0.39811 to 0.39101, saving model to ./AI_models\\02_04_AI_val_loss_index_1.h5\n",
      "\n",
      "Epoch 00235: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 20s 325ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.3910 - val_accuracy: 0.9231\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 00236: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00236: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.9448 - val_accuracy: 0.8846\n",
      "Epoch 237/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9945\n",
      "Epoch 00237: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00237: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0221 - accuracy: 0.9945 - val_loss: 0.8765 - val_accuracy: 0.8846\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9885\n",
      "Epoch 00238: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00238: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0312 - accuracy: 0.9885 - val_loss: 1.0066 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9975\n",
      "Epoch 00239: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00239: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 0.7904 - val_accuracy: 0.8654\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9970\n",
      "Epoch 00240: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00240: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0080 - accuracy: 0.9970 - val_loss: 0.6731 - val_accuracy: 0.9038\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9990\n",
      "Epoch 00241: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00241: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.6498 - val_accuracy: 0.8654\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9950\n",
      "Epoch 00242: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00242: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0139 - accuracy: 0.9950 - val_loss: 0.7822 - val_accuracy: 0.8654\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9950\n",
      "Epoch 00243: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00243: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0176 - accuracy: 0.9950 - val_loss: 0.9793 - val_accuracy: 0.8654\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9915\n",
      "Epoch 00244: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00244: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0267 - accuracy: 0.9915 - val_loss: 1.2405 - val_accuracy: 0.8462\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9970\n",
      "Epoch 00245: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00245: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 0.9150 - val_accuracy: 0.8846\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9975\n",
      "Epoch 00246: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00246: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 1.0266 - val_accuracy: 0.8654\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 00247: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00247: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.5028 - val_accuracy: 0.8846\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9860\n",
      "Epoch 00248: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00248: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0517 - accuracy: 0.9860 - val_loss: 0.6424 - val_accuracy: 0.9038\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9614\n",
      "Epoch 00249: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00249: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1559 - accuracy: 0.9614 - val_loss: 1.7410 - val_accuracy: 0.8077\n",
      "Epoch 250/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9599\n",
      "Epoch 00250: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00250: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2189 - accuracy: 0.9599 - val_loss: 1.8482 - val_accuracy: 0.8462\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9795\n",
      "Epoch 00251: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00251: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0653 - accuracy: 0.9795 - val_loss: 0.8867 - val_accuracy: 0.8462\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9925\n",
      "Epoch 00252: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00252: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0239 - accuracy: 0.9925 - val_loss: 1.6288 - val_accuracy: 0.7692\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9950\n",
      "Epoch 00253: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00253: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0181 - accuracy: 0.9950 - val_loss: 1.3687 - val_accuracy: 0.8462\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9980\n",
      "Epoch 00254: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00254: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0041 - accuracy: 0.9980 - val_loss: 0.9430 - val_accuracy: 0.8846\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9970\n",
      "Epoch 00255: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00255: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 1.0131 - val_accuracy: 0.8846\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 00256: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00256: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 1.0798 - val_accuracy: 0.8462\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9950\n",
      "Epoch 00257: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00257: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0204 - accuracy: 0.9950 - val_loss: 1.2936 - val_accuracy: 0.8077\n",
      "Epoch 258/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9875\n",
      "Epoch 00258: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00258: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0547 - accuracy: 0.9875 - val_loss: 1.7636 - val_accuracy: 0.7692\n",
      "Epoch 259/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9920\n",
      "Epoch 00259: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00259: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0332 - accuracy: 0.9920 - val_loss: 2.1227 - val_accuracy: 0.8077\n",
      "Epoch 260/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9875\n",
      "Epoch 00260: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00260: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0563 - accuracy: 0.9875 - val_loss: 0.9584 - val_accuracy: 0.8846\n",
      "Epoch 261/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9810\n",
      "Epoch 00261: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00261: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0703 - accuracy: 0.9810 - val_loss: 0.9067 - val_accuracy: 0.8654\n",
      "Epoch 262/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9935\n",
      "Epoch 00262: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00262: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0213 - accuracy: 0.9935 - val_loss: 0.9973 - val_accuracy: 0.8269\n",
      "Epoch 263/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9965\n",
      "Epoch 00263: val_loss did not improve from 0.39101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00263: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0091 - accuracy: 0.9965 - val_loss: 1.0906 - val_accuracy: 0.8269\n",
      "Epoch 264/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9990\n",
      "Epoch 00264: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00264: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 0.6359 - val_accuracy: 0.8846\n",
      "Epoch 265/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.0460e-04 - accuracy: 0.9995\n",
      "Epoch 00265: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00265: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.0460e-04 - accuracy: 0.9995 - val_loss: 0.9117 - val_accuracy: 0.8462\n",
      "Epoch 266/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.2444e-04 - accuracy: 1.0000\n",
      "Epoch 00266: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00266: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.2444e-04 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 0.8846\n",
      "Epoch 267/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5443e-04 - accuracy: 1.0000\n",
      "Epoch 00267: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00267: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.5443e-04 - accuracy: 1.0000 - val_loss: 0.7336 - val_accuracy: 0.8462\n",
      "Epoch 268/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2102e-04 - accuracy: 1.0000\n",
      "Epoch 00268: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00268: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.2102e-04 - accuracy: 1.0000 - val_loss: 0.7984 - val_accuracy: 0.8077\n",
      "Epoch 269/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8647e-04 - accuracy: 1.0000\n",
      "Epoch 00269: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00269: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.8647e-04 - accuracy: 1.0000 - val_loss: 0.5355 - val_accuracy: 0.8462\n",
      "Epoch 270/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2589e-04 - accuracy: 1.0000\n",
      "Epoch 00270: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00270: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2589e-04 - accuracy: 1.0000 - val_loss: 0.7933 - val_accuracy: 0.8654\n",
      "Epoch 271/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0307e-04 - accuracy: 1.0000\n",
      "Epoch 00271: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00271: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0307e-04 - accuracy: 1.0000 - val_loss: 0.7643 - val_accuracy: 0.8846\n",
      "Epoch 272/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.1629e-05 - accuracy: 1.0000\n",
      "Epoch 00272: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00272: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.1629e-05 - accuracy: 1.0000 - val_loss: 0.8483 - val_accuracy: 0.8462\n",
      "Epoch 273/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.0128e-05 - accuracy: 1.0000\n",
      "Epoch 00273: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00273: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.0128e-05 - accuracy: 1.0000 - val_loss: 0.7430 - val_accuracy: 0.8846\n",
      "Epoch 274/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.9232e-05 - accuracy: 1.0000\n",
      "Epoch 00274: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00274: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.9232e-05 - accuracy: 1.0000 - val_loss: 0.8051 - val_accuracy: 0.8462\n",
      "Epoch 275/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2197e-04 - accuracy: 1.0000\n",
      "Epoch 00275: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00275: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2197e-04 - accuracy: 1.0000 - val_loss: 0.7870 - val_accuracy: 0.8462\n",
      "Epoch 276/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.1627e-05 - accuracy: 1.0000\n",
      "Epoch 00276: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00276: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.1627e-05 - accuracy: 1.0000 - val_loss: 0.5841 - val_accuracy: 0.8654\n",
      "Epoch 277/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.8580e-05 - accuracy: 1.0000\n",
      "Epoch 00277: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00277: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.8580e-05 - accuracy: 1.0000 - val_loss: 0.7328 - val_accuracy: 0.8654\n",
      "Epoch 278/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.7595e-05 - accuracy: 1.0000\n",
      "Epoch 00278: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00278: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.7595e-05 - accuracy: 1.0000 - val_loss: 0.7659 - val_accuracy: 0.8462\n",
      "Epoch 279/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.2452e-05 - accuracy: 1.0000\n",
      "Epoch 00279: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00279: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.2452e-05 - accuracy: 1.0000 - val_loss: 0.7965 - val_accuracy: 0.8462\n",
      "Epoch 280/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.3070e-05 - accuracy: 1.0000\n",
      "Epoch 00280: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00280: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.3070e-05 - accuracy: 1.0000 - val_loss: 0.6015 - val_accuracy: 0.8462\n",
      "Epoch 281/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0739e-05 - accuracy: 1.0000\n",
      "Epoch 00281: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00281: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.0739e-05 - accuracy: 1.0000 - val_loss: 0.8385 - val_accuracy: 0.8077\n",
      "Epoch 282/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.4871e-05 - accuracy: 1.0000\n",
      "Epoch 00282: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00282: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.4871e-05 - accuracy: 1.0000 - val_loss: 0.7528 - val_accuracy: 0.8269\n",
      "Epoch 283/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2044e-05 - accuracy: 1.0000\n",
      "Epoch 00283: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00283: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.2044e-05 - accuracy: 1.0000 - val_loss: 0.6547 - val_accuracy: 0.8654\n",
      "Epoch 284/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.5236e-05 - accuracy: 1.0000\n",
      "Epoch 00284: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00284: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.5236e-05 - accuracy: 1.0000 - val_loss: 0.5844 - val_accuracy: 0.8462\n",
      "Epoch 285/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.4484e-05 - accuracy: 1.0000\n",
      "Epoch 00285: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00285: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 9.4484e-05 - accuracy: 1.0000 - val_loss: 1.0482 - val_accuracy: 0.8077\n",
      "Epoch 286/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.9110e-05 - accuracy: 1.0000\n",
      "Epoch 00286: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00286: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.9110e-05 - accuracy: 1.0000 - val_loss: 0.8065 - val_accuracy: 0.8077\n",
      "Epoch 287/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 3.8521e-05 - accuracy: 1.0000\n",
      "Epoch 00287: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00287: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.8521e-05 - accuracy: 1.0000 - val_loss: 0.7163 - val_accuracy: 0.8269\n",
      "Epoch 288/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.8942e-05 - accuracy: 1.0000\n",
      "Epoch 00288: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00288: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.8942e-05 - accuracy: 1.0000 - val_loss: 0.6568 - val_accuracy: 0.8269\n",
      "Epoch 289/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.8678e-05 - accuracy: 1.0000\n",
      "Epoch 00289: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00289: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.8678e-05 - accuracy: 1.0000 - val_loss: 0.6508 - val_accuracy: 0.8462\n",
      "Epoch 290/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.2874e-05 - accuracy: 1.0000\n",
      "Epoch 00290: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00290: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.2874e-05 - accuracy: 1.0000 - val_loss: 0.7733 - val_accuracy: 0.8462\n",
      "Epoch 291/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3498e-05 - accuracy: 1.0000\n",
      "Epoch 00291: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00291: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.3498e-05 - accuracy: 1.0000 - val_loss: 0.7521 - val_accuracy: 0.8462\n",
      "Epoch 292/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3198e-05 - accuracy: 1.0000\n",
      "Epoch 00292: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00292: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.3198e-05 - accuracy: 1.0000 - val_loss: 0.7808 - val_accuracy: 0.8462\n",
      "Epoch 293/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3404e-05 - accuracy: 1.0000\n",
      "Epoch 00293: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00293: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.3404e-05 - accuracy: 1.0000 - val_loss: 0.7206 - val_accuracy: 0.8269\n",
      "Epoch 294/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0879e-05 - accuracy: 1.0000\n",
      "Epoch 00294: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00294: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.0879e-05 - accuracy: 1.0000 - val_loss: 0.5907 - val_accuracy: 0.8462\n",
      "Epoch 295/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6367e-05 - accuracy: 1.0000\n",
      "Epoch 00295: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00295: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6367e-05 - accuracy: 1.0000 - val_loss: 0.6422 - val_accuracy: 0.8654\n",
      "Epoch 296/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3012e-05 - accuracy: 1.0000\n",
      "Epoch 00296: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00296: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.3012e-05 - accuracy: 1.0000 - val_loss: 0.8639 - val_accuracy: 0.8269\n",
      "Epoch 297/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6439e-05 - accuracy: 1.0000\n",
      "Epoch 00297: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00297: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6439e-05 - accuracy: 1.0000 - val_loss: 0.7035 - val_accuracy: 0.8846\n",
      "Epoch 298/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7663e-05 - accuracy: 1.0000\n",
      "Epoch 00298: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00298: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7663e-05 - accuracy: 1.0000 - val_loss: 1.0399 - val_accuracy: 0.8077\n",
      "Epoch 299/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9762e-05 - accuracy: 1.0000\n",
      "Epoch 00299: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00299: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.9762e-05 - accuracy: 1.0000 - val_loss: 0.8049 - val_accuracy: 0.8462\n",
      "Epoch 300/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7573e-05 - accuracy: 1.0000\n",
      "Epoch 00300: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00300: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.7573e-05 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 0.8654\n",
      "Epoch 301/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9996e-05 - accuracy: 1.0000\n",
      "Epoch 00301: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00301: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9996e-05 - accuracy: 1.0000 - val_loss: 0.7749 - val_accuracy: 0.8654\n",
      "Epoch 302/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9187e-05 - accuracy: 1.0000\n",
      "Epoch 00302: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00302: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9187e-05 - accuracy: 1.0000 - val_loss: 0.9559 - val_accuracy: 0.8462\n",
      "Epoch 303/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5832e-05 - accuracy: 1.0000\n",
      "Epoch 00303: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00303: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5832e-05 - accuracy: 1.0000 - val_loss: 0.8838 - val_accuracy: 0.8269\n",
      "Epoch 304/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.8371e-05 - accuracy: 1.0000\n",
      "Epoch 00304: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00304: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.8371e-05 - accuracy: 1.0000 - val_loss: 0.6215 - val_accuracy: 0.8462\n",
      "Epoch 305/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3177e-05 - accuracy: 1.0000\n",
      "Epoch 00305: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00305: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.3177e-05 - accuracy: 1.0000 - val_loss: 0.8055 - val_accuracy: 0.8462\n",
      "Epoch 306/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7127e-05 - accuracy: 1.0000\n",
      "Epoch 00306: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00306: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.7127e-05 - accuracy: 1.0000 - val_loss: 0.9787 - val_accuracy: 0.8269\n",
      "Epoch 307/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1986e-05 - accuracy: 1.0000\n",
      "Epoch 00307: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00307: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1986e-05 - accuracy: 1.0000 - val_loss: 0.6670 - val_accuracy: 0.8269\n",
      "Epoch 308/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3135e-05 - accuracy: 1.0000\n",
      "Epoch 00308: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00308: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3135e-05 - accuracy: 1.0000 - val_loss: 0.8324 - val_accuracy: 0.8462\n",
      "Epoch 309/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1322e-05 - accuracy: 1.0000\n",
      "Epoch 00309: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00309: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1322e-05 - accuracy: 1.0000 - val_loss: 0.8505 - val_accuracy: 0.8269\n",
      "Epoch 310/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4006e-05 - accuracy: 1.0000\n",
      "Epoch 00310: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00310: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4006e-05 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 311/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1771e-05 - accuracy: 1.0000\n",
      "Epoch 00311: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00311: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1771e-05 - accuracy: 1.0000 - val_loss: 0.6715 - val_accuracy: 0.8654\n",
      "Epoch 312/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2138e-05 - accuracy: 1.0000\n",
      "Epoch 00312: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00312: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2138e-05 - accuracy: 1.0000 - val_loss: 0.8331 - val_accuracy: 0.8654\n",
      "Epoch 313/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2110e-05 - accuracy: 1.0000\n",
      "Epoch 00313: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00313: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2110e-05 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.8269\n",
      "Epoch 314/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1310e-05 - accuracy: 1.0000\n",
      "Epoch 00314: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00314: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1310e-05 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.8846\n",
      "Epoch 315/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7628e-05 - accuracy: 1.0000\n",
      "Epoch 00315: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00315: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7628e-05 - accuracy: 1.0000 - val_loss: 0.9640 - val_accuracy: 0.8462\n",
      "Epoch 316/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1581e-05 - accuracy: 1.0000\n",
      "Epoch 00316: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00316: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1581e-05 - accuracy: 1.0000 - val_loss: 0.9229 - val_accuracy: 0.8269\n",
      "Epoch 317/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0263e-05 - accuracy: 1.0000\n",
      "Epoch 00317: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00317: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.0263e-05 - accuracy: 1.0000 - val_loss: 0.8244 - val_accuracy: 0.8462\n",
      "Epoch 318/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.6981e-06 - accuracy: 1.0000\n",
      "Epoch 00318: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00318: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 7.6981e-06 - accuracy: 1.0000 - val_loss: 1.0401 - val_accuracy: 0.8077\n",
      "Epoch 319/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.8124e-06 - accuracy: 1.0000\n",
      "Epoch 00319: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00319: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.8124e-06 - accuracy: 1.0000 - val_loss: 0.6654 - val_accuracy: 0.8846\n",
      "Epoch 320/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.8282e-06 - accuracy: 1.0000\n",
      "Epoch 00320: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00320: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.8282e-06 - accuracy: 1.0000 - val_loss: 0.8174 - val_accuracy: 0.8269\n",
      "Epoch 321/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4290e-05 - accuracy: 1.0000\n",
      "Epoch 00321: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00321: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4290e-05 - accuracy: 1.0000 - val_loss: 0.7636 - val_accuracy: 0.8654\n",
      "Epoch 322/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.4044e-06 - accuracy: 1.0000\n",
      "Epoch 00322: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00322: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.4044e-06 - accuracy: 1.0000 - val_loss: 0.8267 - val_accuracy: 0.8846\n",
      "Epoch 323/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7207e-06 - accuracy: 1.0000\n",
      "Epoch 00323: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00323: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.7207e-06 - accuracy: 1.0000 - val_loss: 0.8137 - val_accuracy: 0.8462\n",
      "Epoch 324/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.3311e-06 - accuracy: 1.0000\n",
      "Epoch 00324: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00324: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.3311e-06 - accuracy: 1.0000 - val_loss: 0.8163 - val_accuracy: 0.8269\n",
      "Epoch 325/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.0174e-06 - accuracy: 1.0000\n",
      "Epoch 00325: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00325: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.0174e-06 - accuracy: 1.0000 - val_loss: 0.7930 - val_accuracy: 0.8269\n",
      "Epoch 326/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1132e-05 - accuracy: 1.0000\n",
      "Epoch 00326: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00326: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1132e-05 - accuracy: 1.0000 - val_loss: 0.9098 - val_accuracy: 0.8654\n",
      "Epoch 327/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.2628e-06 - accuracy: 1.0000\n",
      "Epoch 00327: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00327: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.2628e-06 - accuracy: 1.0000 - val_loss: 0.9454 - val_accuracy: 0.8269\n",
      "Epoch 328/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.8564e-06 - accuracy: 1.0000\n",
      "Epoch 00328: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00328: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.8564e-06 - accuracy: 1.0000 - val_loss: 0.7012 - val_accuracy: 0.8654\n",
      "Epoch 329/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.4431e-06 - accuracy: 1.0000\n",
      "Epoch 00329: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00329: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.4431e-06 - accuracy: 1.0000 - val_loss: 0.7117 - val_accuracy: 0.8846\n",
      "Epoch 330/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9933e-06 - accuracy: 1.0000\n",
      "Epoch 00330: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00330: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.9933e-06 - accuracy: 1.0000 - val_loss: 0.9491 - val_accuracy: 0.8462\n",
      "Epoch 331/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.1754e-06 - accuracy: 1.0000\n",
      "Epoch 00331: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00331: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.1754e-06 - accuracy: 1.0000 - val_loss: 0.7370 - val_accuracy: 0.9038\n",
      "Epoch 332/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.0800e-06 - accuracy: 1.0000\n",
      "Epoch 00332: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00332: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.0800e-06 - accuracy: 1.0000 - val_loss: 0.8868 - val_accuracy: 0.8462\n",
      "Epoch 333/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.1706e-06 - accuracy: 1.0000\n",
      "Epoch 00333: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00333: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.1706e-06 - accuracy: 1.0000 - val_loss: 0.7653 - val_accuracy: 0.9038\n",
      "Epoch 334/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6152e-06 - accuracy: 1.0000\n",
      "Epoch 00334: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00334: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.6152e-06 - accuracy: 1.0000 - val_loss: 0.7060 - val_accuracy: 0.8269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.2063e-06 - accuracy: 1.0000\n",
      "Epoch 00335: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00335: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.2063e-06 - accuracy: 1.0000 - val_loss: 0.9786 - val_accuracy: 0.8269\n",
      "Epoch 336/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2249e-06 - accuracy: 1.0000\n",
      "Epoch 00336: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00336: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.2249e-06 - accuracy: 1.0000 - val_loss: 0.6922 - val_accuracy: 0.8654\n",
      "Epoch 337/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.6443e-06 - accuracy: 1.0000\n",
      "Epoch 00337: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00337: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.6443e-06 - accuracy: 1.0000 - val_loss: 0.7909 - val_accuracy: 0.8462\n",
      "Epoch 338/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.7451e-06 - accuracy: 1.0000\n",
      "Epoch 00338: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00338: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.7451e-06 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.8654\n",
      "Epoch 339/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6794e-06 - accuracy: 1.0000\n",
      "Epoch 00339: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00339: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6794e-06 - accuracy: 1.0000 - val_loss: 1.1214 - val_accuracy: 0.8462\n",
      "Epoch 340/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7000e-06 - accuracy: 1.0000\n",
      "Epoch 00340: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00340: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.7000e-06 - accuracy: 1.0000 - val_loss: 0.7816 - val_accuracy: 0.8077\n",
      "Epoch 341/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.4755e-06 - accuracy: 1.0000\n",
      "Epoch 00341: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00341: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.4755e-06 - accuracy: 1.0000 - val_loss: 0.8320 - val_accuracy: 0.8462\n",
      "Epoch 342/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.3301e-06 - accuracy: 1.0000\n",
      "Epoch 00342: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00342: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 8.3301e-06 - accuracy: 1.0000 - val_loss: 0.7124 - val_accuracy: 0.8846\n",
      "Epoch 343/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.1254e-06 - accuracy: 1.0000\n",
      "Epoch 00343: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00343: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 289ms/step - loss: 6.1254e-06 - accuracy: 1.0000 - val_loss: 0.7940 - val_accuracy: 0.8462\n",
      "Epoch 344/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.1568e-06 - accuracy: 1.0000\n",
      "Epoch 00344: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00344: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 5.1568e-06 - accuracy: 1.0000 - val_loss: 0.7937 - val_accuracy: 0.8846\n",
      "Epoch 345/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0817e-05 - accuracy: 1.0000\n",
      "Epoch 00345: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00345: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 20s 310ms/step - loss: 1.0817e-05 - accuracy: 1.0000 - val_loss: 1.0317 - val_accuracy: 0.8462\n",
      "Epoch 346/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6395e-06 - accuracy: 1.0000\n",
      "Epoch 00346: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00346: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 4.6395e-06 - accuracy: 1.0000 - val_loss: 0.6963 - val_accuracy: 0.8654\n",
      "Epoch 347/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.9818e-06 - accuracy: 1.0000\n",
      "Epoch 00347: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00347: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 5.9818e-06 - accuracy: 1.0000 - val_loss: 1.0383 - val_accuracy: 0.8462\n",
      "Epoch 348/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.3472e-06 - accuracy: 1.0000\n",
      "Epoch 00348: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00348: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 4.3472e-06 - accuracy: 1.0000 - val_loss: 0.7615 - val_accuracy: 0.8846\n",
      "Epoch 349/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.0931e-06 - accuracy: 1.0000\n",
      "Epoch 00349: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00349: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.0931e-06 - accuracy: 1.0000 - val_loss: 1.0267 - val_accuracy: 0.8462\n",
      "Epoch 350/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7898e-06 - accuracy: 1.0000\n",
      "Epoch 00350: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00350: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.7898e-06 - accuracy: 1.0000 - val_loss: 0.9389 - val_accuracy: 0.8462\n",
      "Epoch 351/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0830e-06 - accuracy: 1.0000\n",
      "Epoch 00351: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00351: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.0830e-06 - accuracy: 1.0000 - val_loss: 0.8452 - val_accuracy: 0.8654\n",
      "Epoch 352/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.1045e-06 - accuracy: 1.0000\n",
      "Epoch 00352: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00352: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.1045e-06 - accuracy: 1.0000 - val_loss: 0.8271 - val_accuracy: 0.8462\n",
      "Epoch 353/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3211e-06 - accuracy: 1.0000\n",
      "Epoch 00353: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00353: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.3211e-06 - accuracy: 1.0000 - val_loss: 0.8425 - val_accuracy: 0.8654\n",
      "Epoch 354/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.4186e-06 - accuracy: 1.0000\n",
      "Epoch 00354: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00354: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.4186e-06 - accuracy: 1.0000 - val_loss: 0.7100 - val_accuracy: 0.8654\n",
      "Epoch 355/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0994e-06 - accuracy: 1.0000\n",
      "Epoch 00355: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00355: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.0994e-06 - accuracy: 1.0000 - val_loss: 0.9061 - val_accuracy: 0.8654\n",
      "Epoch 356/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.1424e-06 - accuracy: 1.0000\n",
      "Epoch 00356: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00356: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 4.1424e-06 - accuracy: 1.0000 - val_loss: 0.9639 - val_accuracy: 0.8269\n",
      "Epoch 357/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0998e-06 - accuracy: 1.0000\n",
      "Epoch 00357: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00357: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.0998e-06 - accuracy: 1.0000 - val_loss: 0.9009 - val_accuracy: 0.8269\n",
      "Epoch 358/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.8203e-06 - accuracy: 1.0000\n",
      "Epoch 00358: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00358: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 3.8203e-06 - accuracy: 1.0000 - val_loss: 0.7684 - val_accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9967e-06 - accuracy: 1.0000\n",
      "Epoch 00359: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00359: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9967e-06 - accuracy: 1.0000 - val_loss: 0.8044 - val_accuracy: 0.9038\n",
      "Epoch 360/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6068e-06 - accuracy: 1.0000\n",
      "Epoch 00360: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00360: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6068e-06 - accuracy: 1.0000 - val_loss: 0.8004 - val_accuracy: 0.8846\n",
      "Epoch 361/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0336e-06 - accuracy: 1.0000\n",
      "Epoch 00361: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00361: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.0336e-06 - accuracy: 1.0000 - val_loss: 0.8874 - val_accuracy: 0.8846\n",
      "Epoch 362/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9588e-06 - accuracy: 1.0000\n",
      "Epoch 00362: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00362: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9588e-06 - accuracy: 1.0000 - val_loss: 0.6534 - val_accuracy: 0.8654\n",
      "Epoch 363/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2404e-05 - accuracy: 1.0000\n",
      "Epoch 00363: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00363: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2404e-05 - accuracy: 1.0000 - val_loss: 1.0204 - val_accuracy: 0.8462\n",
      "Epoch 364/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.1264e-06 - accuracy: 1.0000\n",
      "Epoch 00364: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00364: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.1264e-06 - accuracy: 1.0000 - val_loss: 0.8085 - val_accuracy: 0.8846\n",
      "Epoch 365/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.8558e-06 - accuracy: 1.0000\n",
      "Epoch 00365: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00365: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 5.8558e-06 - accuracy: 1.0000 - val_loss: 0.7286 - val_accuracy: 0.8846\n",
      "Epoch 366/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.8453e-06 - accuracy: 1.0000\n",
      "Epoch 00366: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00366: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.8453e-06 - accuracy: 1.0000 - val_loss: 0.9025 - val_accuracy: 0.8462\n",
      "Epoch 367/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3674e-06 - accuracy: 1.0000\n",
      "Epoch 00367: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00367: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.3674e-06 - accuracy: 1.0000 - val_loss: 1.0614 - val_accuracy: 0.8462\n",
      "Epoch 368/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5715e-06 - accuracy: 1.0000\n",
      "Epoch 00368: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00368: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5715e-06 - accuracy: 1.0000 - val_loss: 0.7837 - val_accuracy: 0.8846\n",
      "Epoch 369/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4965e-06 - accuracy: 1.0000\n",
      "Epoch 00369: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00369: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4965e-06 - accuracy: 1.0000 - val_loss: 0.8810 - val_accuracy: 0.8462\n",
      "Epoch 370/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7598e-06 - accuracy: 1.0000\n",
      "Epoch 00370: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00370: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.7598e-06 - accuracy: 1.0000 - val_loss: 0.8467 - val_accuracy: 0.8654\n",
      "Epoch 371/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3163e-06 - accuracy: 1.0000\n",
      "Epoch 00371: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00371: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3163e-06 - accuracy: 1.0000 - val_loss: 0.8353 - val_accuracy: 0.8462\n",
      "Epoch 372/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7736e-06 - accuracy: 1.0000\n",
      "Epoch 00372: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00372: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7736e-06 - accuracy: 1.0000 - val_loss: 0.6373 - val_accuracy: 0.8846\n",
      "Epoch 373/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6691e-06 - accuracy: 1.0000\n",
      "Epoch 00373: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00373: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6691e-06 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 0.8846\n",
      "Epoch 374/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4102e-06 - accuracy: 1.0000\n",
      "Epoch 00374: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00374: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4102e-06 - accuracy: 1.0000 - val_loss: 0.8075 - val_accuracy: 0.8462\n",
      "Epoch 375/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5575e-06 - accuracy: 1.0000\n",
      "Epoch 00375: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00375: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5575e-06 - accuracy: 1.0000 - val_loss: 0.7212 - val_accuracy: 0.9231\n",
      "Epoch 376/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7450e-06 - accuracy: 1.0000\n",
      "Epoch 00376: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00376: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7450e-06 - accuracy: 1.0000 - val_loss: 0.7761 - val_accuracy: 0.8654\n",
      "Epoch 377/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9952e-06 - accuracy: 1.0000\n",
      "Epoch 00377: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00377: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9952e-06 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 0.8654\n",
      "Epoch 378/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9604e-06 - accuracy: 1.0000\n",
      "Epoch 00378: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00378: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9604e-06 - accuracy: 1.0000 - val_loss: 0.8679 - val_accuracy: 0.9038\n",
      "Epoch 379/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0950e-06 - accuracy: 1.0000\n",
      "Epoch 00379: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00379: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0950e-06 - accuracy: 1.0000 - val_loss: 0.8759 - val_accuracy: 0.8654\n",
      "Epoch 380/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6090e-06 - accuracy: 1.0000\n",
      "Epoch 00380: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00380: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6090e-06 - accuracy: 1.0000 - val_loss: 0.6408 - val_accuracy: 0.9038\n",
      "Epoch 381/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.9528e-07 - accuracy: 1.0000\n",
      "Epoch 00381: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00381: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.9528e-07 - accuracy: 1.0000 - val_loss: 0.7600 - val_accuracy: 0.8654\n",
      "Epoch 382/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2374e-06 - accuracy: 1.0000\n",
      "Epoch 00382: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00382: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2374e-06 - accuracy: 1.0000 - val_loss: 0.8947 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.8293e-07 - accuracy: 1.0000\n",
      "Epoch 00383: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00383: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.8293e-07 - accuracy: 1.0000 - val_loss: 0.8939 - val_accuracy: 0.8654\n",
      "Epoch 384/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.7950e-07 - accuracy: 1.0000\n",
      "Epoch 00384: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00384: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.7950e-07 - accuracy: 1.0000 - val_loss: 1.0062 - val_accuracy: 0.8654\n",
      "Epoch 385/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0581e-06 - accuracy: 1.0000\n",
      "Epoch 00385: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00385: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0581e-06 - accuracy: 1.0000 - val_loss: 0.7613 - val_accuracy: 0.9038\n",
      "Epoch 386/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.5941e-07 - accuracy: 1.0000\n",
      "Epoch 00386: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00386: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.5941e-07 - accuracy: 1.0000 - val_loss: 0.7477 - val_accuracy: 0.8654\n",
      "Epoch 387/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0579e-06 - accuracy: 1.0000\n",
      "Epoch 00387: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00387: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0579e-06 - accuracy: 1.0000 - val_loss: 0.7445 - val_accuracy: 0.8846\n",
      "Epoch 388/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.1489e-07 - accuracy: 1.0000\n",
      "Epoch 00388: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00388: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 9.1489e-07 - accuracy: 1.0000 - val_loss: 0.9600 - val_accuracy: 0.8462\n",
      "Epoch 389/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2408e-06 - accuracy: 1.0000\n",
      "Epoch 00389: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00389: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.2408e-06 - accuracy: 1.0000 - val_loss: 1.0417 - val_accuracy: 0.8654\n",
      "Epoch 390/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4110e-06 - accuracy: 1.0000\n",
      "Epoch 00390: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00390: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4110e-06 - accuracy: 1.0000 - val_loss: 0.6745 - val_accuracy: 0.8654\n",
      "Epoch 391/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3100e-06 - accuracy: 1.0000\n",
      "Epoch 00391: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00391: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.3100e-06 - accuracy: 1.0000 - val_loss: 0.9640 - val_accuracy: 0.8462\n",
      "Epoch 392/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0556e-06 - accuracy: 1.0000\n",
      "Epoch 00392: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00392: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0556e-06 - accuracy: 1.0000 - val_loss: 1.0401 - val_accuracy: 0.8462\n",
      "Epoch 393/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.6202e-07 - accuracy: 1.0000\n",
      "Epoch 00393: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00393: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.6202e-07 - accuracy: 1.0000 - val_loss: 0.7872 - val_accuracy: 0.9038\n",
      "Epoch 394/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.3800e-07 - accuracy: 1.0000\n",
      "Epoch 00394: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00394: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.3800e-07 - accuracy: 1.0000 - val_loss: 0.7607 - val_accuracy: 0.9038\n",
      "Epoch 395/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.4018e-07 - accuracy: 1.0000\n",
      "Epoch 00395: val_loss did not improve from 0.39101\n",
      "\n",
      "Epoch 00395: val_accuracy did not improve from 0.94231\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.4018e-07 - accuracy: 1.0000 - val_loss: 0.7362 - val_accuracy: 0.8846\n",
      "1  번째 학습을 완료했습니다.\n",
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 8s - loss: 16.2811 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1011s vs `on_train_batch_end` time: 0.1833s). Check your callbacks.\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4280 - accuracy: 0.1102\n",
      "Epoch 00001: val_loss improved from inf to 662.43036, saving model to ./AI_models\\02_04_AI_val_loss_index_2.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.09615, saving model to ./AI_models\\02_04_AI_val_accuracy_index_2.h5\n",
      "63/63 [==============================] - 23s 370ms/step - loss: 4.4280 - accuracy: 0.1102 - val_loss: 662.4304 - val_accuracy: 0.0962\n",
      "Epoch 2/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2594 - accuracy: 0.1804\n",
      "Epoch 00002: val_loss improved from 662.43036 to 12.34850, saving model to ./AI_models\\02_04_AI_val_loss_index_2.h5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 20s 324ms/step - loss: 2.2594 - accuracy: 0.1804 - val_loss: 12.3485 - val_accuracy: 0.0962\n",
      "Epoch 3/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9302 - accuracy: 0.3026\n",
      "Epoch 00003: val_loss did not improve from 12.34850\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 1.9302 - accuracy: 0.3026 - val_loss: 37.8294 - val_accuracy: 0.0962\n",
      "Epoch 4/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7370 - accuracy: 0.3923\n",
      "Epoch 00004: val_loss did not improve from 12.34850\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.7370 - accuracy: 0.3923 - val_loss: 47.7557 - val_accuracy: 0.0962\n",
      "Epoch 5/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3981 - accuracy: 0.5230\n",
      "Epoch 00005: val_loss did not improve from 12.34850\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3981 - accuracy: 0.5230 - val_loss: 19.9125 - val_accuracy: 0.0962\n",
      "Epoch 6/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9611 - accuracy: 0.6774\n",
      "Epoch 00006: val_loss improved from 12.34850 to 3.02550, saving model to ./AI_models\\02_04_AI_val_loss_index_2.h5\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 32s 509ms/step - loss: 0.9611 - accuracy: 0.6774 - val_loss: 3.0255 - val_accuracy: 0.0962\n",
      "Epoch 7/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8115 - accuracy: 0.7335\n",
      "Epoch 00007: val_loss did not improve from 3.02550\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.8115 - accuracy: 0.7335 - val_loss: 3.7025 - val_accuracy: 0.0962\n",
      "Epoch 8/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7172 - accuracy: 0.7660\n",
      "Epoch 00008: val_loss did not improve from 3.02550\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.7172 - accuracy: 0.7660 - val_loss: 4.5852 - val_accuracy: 0.0962\n",
      "Epoch 9/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5729 - accuracy: 0.8181\n",
      "Epoch 00009: val_loss did not improve from 3.02550\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.5729 - accuracy: 0.8181 - val_loss: 5.1586 - val_accuracy: 0.0962\n",
      "Epoch 10/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.8196\n",
      "Epoch 00010: val_loss did not improve from 3.02550\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.09615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.5609 - accuracy: 0.8196 - val_loss: 4.0080 - val_accuracy: 0.0962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.8512\n",
      "Epoch 00011: val_loss improved from 3.02550 to 0.95693, saving model to ./AI_models\\02_04_AI_val_loss_index_2.h5\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.09615 to 0.69231, saving model to ./AI_models\\02_04_AI_val_accuracy_index_2.h5\n",
      "63/63 [==============================] - 46s 723ms/step - loss: 0.4578 - accuracy: 0.8512 - val_loss: 0.9569 - val_accuracy: 0.6923\n",
      "Epoch 12/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3991 - accuracy: 0.8637\n",
      "Epoch 00012: val_loss improved from 0.95693 to 0.67754, saving model to ./AI_models\\02_04_AI_val_loss_index_2.h5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.69231 to 0.78846, saving model to ./AI_models\\02_04_AI_val_accuracy_index_2.h5\n",
      "63/63 [==============================] - 23s 367ms/step - loss: 0.3991 - accuracy: 0.8637 - val_loss: 0.6775 - val_accuracy: 0.7885\n",
      "Epoch 13/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.8923\n",
      "Epoch 00013: val_loss improved from 0.67754 to 0.56185, saving model to ./AI_models\\02_04_AI_val_loss_index_2.h5\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.78846\n",
      "63/63 [==============================] - 20s 321ms/step - loss: 0.3239 - accuracy: 0.8923 - val_loss: 0.5619 - val_accuracy: 0.7500\n",
      "Epoch 14/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8753\n",
      "Epoch 00014: val_loss did not improve from 0.56185\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.78846\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.3715 - accuracy: 0.8753 - val_loss: 0.8528 - val_accuracy: 0.7500\n",
      "Epoch 15/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9158\n",
      "Epoch 00015: val_loss improved from 0.56185 to 0.45459, saving model to ./AI_models\\02_04_AI_val_loss_index_2.h5\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.78846\n",
      "63/63 [==============================] - 21s 328ms/step - loss: 0.2529 - accuracy: 0.9158 - val_loss: 0.4546 - val_accuracy: 0.7885\n",
      "Epoch 16/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.9038\n",
      "Epoch 00016: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.78846\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.2672 - accuracy: 0.9038 - val_loss: 3.2560 - val_accuracy: 0.4423\n",
      "Epoch 17/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.9108\n",
      "Epoch 00017: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.78846\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2724 - accuracy: 0.9108 - val_loss: 1.0784 - val_accuracy: 0.7308\n",
      "Epoch 18/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9133\n",
      "Epoch 00018: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.78846\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2537 - accuracy: 0.9133 - val_loss: 0.6501 - val_accuracy: 0.7692\n",
      "Epoch 19/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.9078\n",
      "Epoch 00019: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.78846\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.2893 - accuracy: 0.9078 - val_loss: 4.5320 - val_accuracy: 0.4231\n",
      "Epoch 20/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9389\n",
      "Epoch 00020: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.78846\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1822 - accuracy: 0.9389 - val_loss: 0.9920 - val_accuracy: 0.7308\n",
      "Epoch 21/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9449\n",
      "Epoch 00021: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.78846 to 0.84615, saving model to ./AI_models\\02_04_AI_val_accuracy_index_2.h5\n",
      "63/63 [==============================] - 31s 487ms/step - loss: 0.1590 - accuracy: 0.9449 - val_loss: 0.9407 - val_accuracy: 0.8462\n",
      "Epoch 22/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.9243\n",
      "Epoch 00022: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.2334 - accuracy: 0.9243 - val_loss: 1.1275 - val_accuracy: 0.7692\n",
      "Epoch 23/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9324\n",
      "Epoch 00023: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.2122 - accuracy: 0.9324 - val_loss: 0.6663 - val_accuracy: 0.7885\n",
      "Epoch 24/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9374\n",
      "Epoch 00024: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1701 - accuracy: 0.9374 - val_loss: 2.2604 - val_accuracy: 0.5962\n",
      "Epoch 25/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9599\n",
      "Epoch 00025: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1314 - accuracy: 0.9599 - val_loss: 0.6149 - val_accuracy: 0.8462\n",
      "Epoch 26/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1643 - accuracy: 0.9479\n",
      "Epoch 00026: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1643 - accuracy: 0.9479 - val_loss: 1.6069 - val_accuracy: 0.6731\n",
      "Epoch 27/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9589\n",
      "Epoch 00027: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1177 - accuracy: 0.9589 - val_loss: 2.7862 - val_accuracy: 0.5385\n",
      "Epoch 28/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9604\n",
      "Epoch 00028: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0984 - accuracy: 0.9604 - val_loss: 1.0569 - val_accuracy: 0.7500\n",
      "Epoch 29/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.9339\n",
      "Epoch 00029: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1781 - accuracy: 0.9339 - val_loss: 1.3467 - val_accuracy: 0.7115\n",
      "Epoch 30/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1884 - accuracy: 0.9389\n",
      "Epoch 00030: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1884 - accuracy: 0.9389 - val_loss: 0.9386 - val_accuracy: 0.7885\n",
      "Epoch 31/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9549\n",
      "Epoch 00031: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1213 - accuracy: 0.9549 - val_loss: 0.9412 - val_accuracy: 0.8269\n",
      "Epoch 32/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9544\n",
      "Epoch 00032: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1481 - accuracy: 0.9544 - val_loss: 2.9289 - val_accuracy: 0.4808\n",
      "Epoch 33/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9609\n",
      "Epoch 00033: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1162 - accuracy: 0.9609 - val_loss: 0.6620 - val_accuracy: 0.8269\n",
      "Epoch 34/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9644\n",
      "Epoch 00034: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1011 - accuracy: 0.9644 - val_loss: 0.7833 - val_accuracy: 0.8269\n",
      "Epoch 35/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9619\n",
      "Epoch 00035: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1045 - accuracy: 0.9619 - val_loss: 1.1183 - val_accuracy: 0.7885\n",
      "Epoch 36/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9434\n",
      "Epoch 00036: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1816 - accuracy: 0.9434 - val_loss: 1.9843 - val_accuracy: 0.7115\n",
      "Epoch 37/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.9218\n",
      "Epoch 00037: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2631 - accuracy: 0.9218 - val_loss: 2.5963 - val_accuracy: 0.7115\n",
      "Epoch 38/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9499\n",
      "Epoch 00038: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1602 - accuracy: 0.9499 - val_loss: 0.6756 - val_accuracy: 0.7885\n",
      "Epoch 39/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9594\n",
      "Epoch 00039: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1165 - accuracy: 0.9594 - val_loss: 0.7796 - val_accuracy: 0.8269\n",
      "Epoch 40/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9659\n",
      "Epoch 00040: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0847 - accuracy: 0.9659 - val_loss: 0.7240 - val_accuracy: 0.7692\n",
      "Epoch 41/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9850\n",
      "Epoch 00041: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0512 - accuracy: 0.9850 - val_loss: 0.6949 - val_accuracy: 0.8462\n",
      "Epoch 42/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9689\n",
      "Epoch 00042: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0962 - accuracy: 0.9689 - val_loss: 1.0557 - val_accuracy: 0.7692\n",
      "Epoch 43/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9875\n",
      "Epoch 00043: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0376 - accuracy: 0.9875 - val_loss: 0.5607 - val_accuracy: 0.8077\n",
      "Epoch 44/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9840\n",
      "Epoch 00044: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0558 - accuracy: 0.9840 - val_loss: 0.8427 - val_accuracy: 0.8269\n",
      "Epoch 45/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9770\n",
      "Epoch 00045: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0693 - accuracy: 0.9770 - val_loss: 1.3084 - val_accuracy: 0.8077\n",
      "Epoch 46/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9399\n",
      "Epoch 00046: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2061 - accuracy: 0.9399 - val_loss: 1.6503 - val_accuracy: 0.6923\n",
      "Epoch 47/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9409\n",
      "Epoch 00047: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.2030 - accuracy: 0.9409 - val_loss: 2.1405 - val_accuracy: 0.6731\n",
      "Epoch 48/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9589\n",
      "Epoch 00048: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 290ms/step - loss: 0.1180 - accuracy: 0.9589 - val_loss: 0.9334 - val_accuracy: 0.8269\n",
      "Epoch 49/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9800\n",
      "Epoch 00049: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0678 - accuracy: 0.9800 - val_loss: 0.7347 - val_accuracy: 0.8077\n",
      "Epoch 50/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9835\n",
      "Epoch 00050: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.84615\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.0452 - accuracy: 0.9835 - val_loss: 0.7592 - val_accuracy: 0.8269\n",
      "Epoch 51/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9674\n",
      "Epoch 00051: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.84615 to 0.86538, saving model to ./AI_models\\02_04_AI_val_accuracy_index_2.h5\n",
      "63/63 [==============================] - 31s 498ms/step - loss: 0.1051 - accuracy: 0.9674 - val_loss: 0.7862 - val_accuracy: 0.8654\n",
      "Epoch 52/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9835\n",
      "Epoch 00052: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0565 - accuracy: 0.9835 - val_loss: 0.5380 - val_accuracy: 0.8462\n",
      "Epoch 53/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9885\n",
      "Epoch 00053: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0392 - accuracy: 0.9885 - val_loss: 0.6044 - val_accuracy: 0.8654\n",
      "Epoch 54/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9594\n",
      "Epoch 00054: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.1318 - accuracy: 0.9594 - val_loss: 2.5234 - val_accuracy: 0.6154\n",
      "Epoch 55/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9469\n",
      "Epoch 00055: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1828 - accuracy: 0.9469 - val_loss: 0.8668 - val_accuracy: 0.8269\n",
      "Epoch 56/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9639\n",
      "Epoch 00056: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1092 - accuracy: 0.9639 - val_loss: 0.5308 - val_accuracy: 0.8077\n",
      "Epoch 57/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9744\n",
      "Epoch 00057: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0680 - accuracy: 0.9744 - val_loss: 0.5956 - val_accuracy: 0.8462\n",
      "Epoch 58/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9850\n",
      "Epoch 00058: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0500 - accuracy: 0.9850 - val_loss: 0.8440 - val_accuracy: 0.8269\n",
      "Epoch 59/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9885\n",
      "Epoch 00059: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0325 - accuracy: 0.9885 - val_loss: 0.7253 - val_accuracy: 0.8462\n",
      "Epoch 60/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9920\n",
      "Epoch 00060: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0190 - accuracy: 0.9920 - val_loss: 0.9182 - val_accuracy: 0.8269\n",
      "Epoch 61/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9915\n",
      "Epoch 00061: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0236 - accuracy: 0.9915 - val_loss: 0.5419 - val_accuracy: 0.8462\n",
      "Epoch 62/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9860\n",
      "Epoch 00062: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0477 - accuracy: 0.9860 - val_loss: 1.1883 - val_accuracy: 0.7692\n",
      "Epoch 63/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9800\n",
      "Epoch 00063: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0635 - accuracy: 0.9800 - val_loss: 1.0118 - val_accuracy: 0.7885\n",
      "Epoch 64/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9775\n",
      "Epoch 00064: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0593 - accuracy: 0.9775 - val_loss: 0.8468 - val_accuracy: 0.8462\n",
      "Epoch 65/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9524\n",
      "Epoch 00065: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1616 - accuracy: 0.9524 - val_loss: 1.5927 - val_accuracy: 0.7308\n",
      "Epoch 66/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9469\n",
      "Epoch 00066: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1938 - accuracy: 0.9469 - val_loss: 1.2744 - val_accuracy: 0.8077\n",
      "Epoch 67/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1389 - accuracy: 0.9599\n",
      "Epoch 00067: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1389 - accuracy: 0.9599 - val_loss: 0.7911 - val_accuracy: 0.8654\n",
      "Epoch 68/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9589\n",
      "Epoch 00068: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1411 - accuracy: 0.9589 - val_loss: 15.6122 - val_accuracy: 0.2885\n",
      "Epoch 69/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9564\n",
      "Epoch 00069: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1519 - accuracy: 0.9564 - val_loss: 3.4184 - val_accuracy: 0.7115\n",
      "Epoch 70/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9684\n",
      "Epoch 00070: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0870 - accuracy: 0.9684 - val_loss: 1.0028 - val_accuracy: 0.7500\n",
      "Epoch 71/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9845\n",
      "Epoch 00071: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0450 - accuracy: 0.9845 - val_loss: 1.5531 - val_accuracy: 0.7692\n",
      "Epoch 72/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9885\n",
      "Epoch 00072: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0415 - accuracy: 0.9885 - val_loss: 0.7364 - val_accuracy: 0.8269\n",
      "Epoch 73/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9845\n",
      "Epoch 00073: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0414 - accuracy: 0.9845 - val_loss: 0.5919 - val_accuracy: 0.8654\n",
      "Epoch 74/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9900\n",
      "Epoch 00074: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0372 - accuracy: 0.9900 - val_loss: 0.8356 - val_accuracy: 0.8077\n",
      "Epoch 75/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9975\n",
      "Epoch 00075: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0115 - accuracy: 0.9975 - val_loss: 0.8225 - val_accuracy: 0.8269\n",
      "Epoch 76/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9955\n",
      "Epoch 00076: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0163 - accuracy: 0.9955 - val_loss: 0.7302 - val_accuracy: 0.8077\n",
      "Epoch 77/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9845\n",
      "Epoch 00077: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.86538\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0450 - accuracy: 0.9845 - val_loss: 1.6262 - val_accuracy: 0.7885\n",
      "Epoch 78/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9734\n",
      "Epoch 00078: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00078: val_accuracy improved from 0.86538 to 0.90385, saving model to ./AI_models\\02_04_AI_val_accuracy_index_2.h5\n",
      "63/63 [==============================] - 30s 479ms/step - loss: 0.0862 - accuracy: 0.9734 - val_loss: 0.6224 - val_accuracy: 0.9038\n",
      "Epoch 79/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9780\n",
      "Epoch 00079: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0793 - accuracy: 0.9780 - val_loss: 0.9574 - val_accuracy: 0.8269\n",
      "Epoch 80/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9439\n",
      "Epoch 00080: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.2111 - accuracy: 0.9439 - val_loss: 2.7105 - val_accuracy: 0.6923\n",
      "Epoch 81/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9755\n",
      "Epoch 00081: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.1015 - accuracy: 0.9755 - val_loss: 0.6495 - val_accuracy: 0.8077\n",
      "Epoch 82/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9875\n",
      "Epoch 00082: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0322 - accuracy: 0.9875 - val_loss: 0.5351 - val_accuracy: 0.9038\n",
      "Epoch 83/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9905\n",
      "Epoch 00083: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0290 - accuracy: 0.9905 - val_loss: 0.9523 - val_accuracy: 0.8462\n",
      "Epoch 84/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9855\n",
      "Epoch 00084: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0449 - accuracy: 0.9855 - val_loss: 3.7146 - val_accuracy: 0.6346\n",
      "Epoch 85/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9835\n",
      "Epoch 00085: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0485 - accuracy: 0.9835 - val_loss: 2.4233 - val_accuracy: 0.7500\n",
      "Epoch 86/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9664\n",
      "Epoch 00086: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1188 - accuracy: 0.9664 - val_loss: 1.2011 - val_accuracy: 0.7692\n",
      "Epoch 87/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9554\n",
      "Epoch 00087: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1776 - accuracy: 0.9554 - val_loss: 1.4927 - val_accuracy: 0.7692\n",
      "Epoch 88/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9644\n",
      "Epoch 00088: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1222 - accuracy: 0.9644 - val_loss: 0.8378 - val_accuracy: 0.7692\n",
      "Epoch 89/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9855\n",
      "Epoch 00089: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0460 - accuracy: 0.9855 - val_loss: 1.0867 - val_accuracy: 0.8077\n",
      "Epoch 90/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9825\n",
      "Epoch 00090: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0629 - accuracy: 0.9825 - val_loss: 1.0665 - val_accuracy: 0.9038\n",
      "Epoch 91/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9790\n",
      "Epoch 00091: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0731 - accuracy: 0.9790 - val_loss: 1.3196 - val_accuracy: 0.8269\n",
      "Epoch 92/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9464\n",
      "Epoch 00092: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.2029 - accuracy: 0.9464 - val_loss: 1.9553 - val_accuracy: 0.8077\n",
      "Epoch 93/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1555 - accuracy: 0.9529\n",
      "Epoch 00093: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1555 - accuracy: 0.9529 - val_loss: 1.5255 - val_accuracy: 0.7885\n",
      "Epoch 94/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9775\n",
      "Epoch 00094: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0877 - accuracy: 0.9775 - val_loss: 1.3088 - val_accuracy: 0.8462\n",
      "Epoch 95/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9765\n",
      "Epoch 00095: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0632 - accuracy: 0.9765 - val_loss: 1.3105 - val_accuracy: 0.8077\n",
      "Epoch 96/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9815\n",
      "Epoch 00096: val_loss did not improve from 0.45459\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0702 - accuracy: 0.9815 - val_loss: 0.9049 - val_accuracy: 0.8462\n",
      "Epoch 97/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9865\n",
      "Epoch 00097: val_loss improved from 0.45459 to 0.39689, saving model to ./AI_models\\02_04_AI_val_loss_index_2.h5\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 30s 484ms/step - loss: 0.0379 - accuracy: 0.9865 - val_loss: 0.3969 - val_accuracy: 0.8846\n",
      "Epoch 98/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9940\n",
      "Epoch 00098: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0260 - accuracy: 0.9940 - val_loss: 0.8133 - val_accuracy: 0.8462\n",
      "Epoch 99/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9810\n",
      "Epoch 00099: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 17s 277ms/step - loss: 0.0614 - accuracy: 0.9810 - val_loss: 0.8481 - val_accuracy: 0.8462\n",
      "Epoch 100/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9920\n",
      "Epoch 00100: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0245 - accuracy: 0.9920 - val_loss: 0.8923 - val_accuracy: 0.8462\n",
      "Epoch 101/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9905\n",
      "Epoch 00101: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0326 - accuracy: 0.9905 - val_loss: 1.4494 - val_accuracy: 0.7692\n",
      "Epoch 102/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9975\n",
      "Epoch 00102: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 1.2715 - val_accuracy: 0.8462\n",
      "Epoch 103/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 0.9985\n",
      "Epoch 00103: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 1.1628 - val_accuracy: 0.8462\n",
      "Epoch 104/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.0438e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.0438e-04 - accuracy: 1.0000 - val_loss: 1.3102 - val_accuracy: 0.8269\n",
      "Epoch 105/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9990\n",
      "Epoch 00105: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.0014 - accuracy: 0.9990 - val_loss: 1.1419 - val_accuracy: 0.8462\n",
      "Epoch 106/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 7.8932e-04 - accuracy: 0.9995\n",
      "Epoch 00106: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.8932e-04 - accuracy: 0.9995 - val_loss: 1.0095 - val_accuracy: 0.8462\n",
      "Epoch 107/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5028e-04 - accuracy: 1.0000\n",
      "Epoch 00107: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5028e-04 - accuracy: 1.0000 - val_loss: 1.2626 - val_accuracy: 0.8462\n",
      "Epoch 108/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1002e-04 - accuracy: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1002e-04 - accuracy: 1.0000 - val_loss: 1.1300 - val_accuracy: 0.8269\n",
      "Epoch 109/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0972e-04 - accuracy: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0972e-04 - accuracy: 1.0000 - val_loss: 1.3166 - val_accuracy: 0.8269\n",
      "Epoch 110/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.6894e-05 - accuracy: 1.0000\n",
      "Epoch 00110: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.6894e-05 - accuracy: 1.0000 - val_loss: 0.8886 - val_accuracy: 0.8846\n",
      "Epoch 111/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.0544e-04 - accuracy: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.0544e-04 - accuracy: 1.0000 - val_loss: 0.9112 - val_accuracy: 0.8462\n",
      "Epoch 112/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5084e-04 - accuracy: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 2.5084e-04 - accuracy: 1.0000 - val_loss: 1.1355 - val_accuracy: 0.8654\n",
      "Epoch 113/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5468e-04 - accuracy: 1.0000\n",
      "Epoch 00113: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.5468e-04 - accuracy: 1.0000 - val_loss: 0.9136 - val_accuracy: 0.8654\n",
      "Epoch 114/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0911e-04 - accuracy: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0911e-04 - accuracy: 1.0000 - val_loss: 1.3805 - val_accuracy: 0.8077\n",
      "Epoch 115/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0129e-04 - accuracy: 1.0000\n",
      "Epoch 00115: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.0129e-04 - accuracy: 1.0000 - val_loss: 1.1028 - val_accuracy: 0.8269\n",
      "Epoch 116/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7211e-05 - accuracy: 1.0000\n",
      "Epoch 00116: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.7211e-05 - accuracy: 1.0000 - val_loss: 1.3483 - val_accuracy: 0.8077\n",
      "Epoch 117/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.5594e-05 - accuracy: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.5594e-05 - accuracy: 1.0000 - val_loss: 0.8741 - val_accuracy: 0.8462\n",
      "Epoch 118/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.3773e-05 - accuracy: 1.0000\n",
      "Epoch 00118: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.3773e-05 - accuracy: 1.0000 - val_loss: 0.9838 - val_accuracy: 0.8654\n",
      "Epoch 119/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4760e-05 - accuracy: 1.0000\n",
      "Epoch 00119: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.4760e-05 - accuracy: 1.0000 - val_loss: 0.9170 - val_accuracy: 0.8462\n",
      "Epoch 120/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.6743e-05 - accuracy: 1.0000\n",
      "Epoch 00120: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.6743e-05 - accuracy: 1.0000 - val_loss: 0.9871 - val_accuracy: 0.8846\n",
      "Epoch 121/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4523e-05 - accuracy: 1.0000\n",
      "Epoch 00121: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.4523e-05 - accuracy: 1.0000 - val_loss: 1.1150 - val_accuracy: 0.8269\n",
      "Epoch 122/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6356e-05 - accuracy: 1.0000\n",
      "Epoch 00122: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6356e-05 - accuracy: 1.0000 - val_loss: 0.9918 - val_accuracy: 0.8654\n",
      "Epoch 123/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.7948e-05 - accuracy: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.7948e-05 - accuracy: 1.0000 - val_loss: 1.1956 - val_accuracy: 0.8269\n",
      "Epoch 124/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.9567e-05 - accuracy: 1.0000\n",
      "Epoch 00124: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.9567e-05 - accuracy: 1.0000 - val_loss: 1.1672 - val_accuracy: 0.8462\n",
      "Epoch 125/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3741e-05 - accuracy: 1.0000\n",
      "Epoch 00125: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.3741e-05 - accuracy: 1.0000 - val_loss: 1.0229 - val_accuracy: 0.8846\n",
      "Epoch 126/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.3465e-05 - accuracy: 1.0000\n",
      "Epoch 00126: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.3465e-05 - accuracy: 1.0000 - val_loss: 0.8998 - val_accuracy: 0.9038\n",
      "Epoch 127/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0009e-05 - accuracy: 1.0000\n",
      "Epoch 00127: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.0009e-05 - accuracy: 1.0000 - val_loss: 0.9568 - val_accuracy: 0.8654\n",
      "Epoch 128/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3654e-05 - accuracy: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.3654e-05 - accuracy: 1.0000 - val_loss: 1.2356 - val_accuracy: 0.8462\n",
      "Epoch 129/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2284e-05 - accuracy: 1.0000\n",
      "Epoch 00129: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.2284e-05 - accuracy: 1.0000 - val_loss: 1.2981 - val_accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.9426e-05 - accuracy: 1.0000\n",
      "Epoch 00130: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.9426e-05 - accuracy: 1.0000 - val_loss: 1.2444 - val_accuracy: 0.8654\n",
      "Epoch 131/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.5737e-05 - accuracy: 1.0000\n",
      "Epoch 00131: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.5737e-05 - accuracy: 1.0000 - val_loss: 1.0721 - val_accuracy: 0.8462\n",
      "Epoch 132/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.6241e-05 - accuracy: 1.0000\n",
      "Epoch 00132: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.6241e-05 - accuracy: 1.0000 - val_loss: 1.3565 - val_accuracy: 0.8077\n",
      "Epoch 133/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9288e-05 - accuracy: 1.0000\n",
      "Epoch 00133: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9288e-05 - accuracy: 1.0000 - val_loss: 0.8228 - val_accuracy: 0.8462\n",
      "Epoch 134/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0376e-05 - accuracy: 1.0000\n",
      "Epoch 00134: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.0376e-05 - accuracy: 1.0000 - val_loss: 1.2842 - val_accuracy: 0.8654\n",
      "Epoch 135/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7364e-05 - accuracy: 1.0000\n",
      "Epoch 00135: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7364e-05 - accuracy: 1.0000 - val_loss: 1.1291 - val_accuracy: 0.8654\n",
      "Epoch 136/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7220e-05 - accuracy: 1.0000\n",
      "Epoch 00136: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7220e-05 - accuracy: 1.0000 - val_loss: 1.0213 - val_accuracy: 0.8654\n",
      "Epoch 137/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.2513e-05 - accuracy: 1.0000\n",
      "Epoch 00137: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.2513e-05 - accuracy: 1.0000 - val_loss: 1.1475 - val_accuracy: 0.8077\n",
      "Epoch 138/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7880e-05 - accuracy: 1.0000\n",
      "Epoch 00138: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7880e-05 - accuracy: 1.0000 - val_loss: 1.2345 - val_accuracy: 0.8462\n",
      "Epoch 139/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8080e-05 - accuracy: 1.0000\n",
      "Epoch 00139: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.8080e-05 - accuracy: 1.0000 - val_loss: 1.1615 - val_accuracy: 0.8654\n",
      "Epoch 140/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3723e-05 - accuracy: 1.0000\n",
      "Epoch 00140: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3723e-05 - accuracy: 1.0000 - val_loss: 0.9098 - val_accuracy: 0.8846\n",
      "Epoch 141/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8840e-05 - accuracy: 1.0000\n",
      "Epoch 00141: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.8840e-05 - accuracy: 1.0000 - val_loss: 1.0825 - val_accuracy: 0.8846\n",
      "Epoch 142/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9602e-05 - accuracy: 1.0000\n",
      "Epoch 00142: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9602e-05 - accuracy: 1.0000 - val_loss: 0.7423 - val_accuracy: 0.8654\n",
      "Epoch 143/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6969e-05 - accuracy: 1.0000\n",
      "Epoch 00143: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6969e-05 - accuracy: 1.0000 - val_loss: 1.0015 - val_accuracy: 0.8654\n",
      "Epoch 144/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2752e-05 - accuracy: 1.0000\n",
      "Epoch 00144: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2752e-05 - accuracy: 1.0000 - val_loss: 1.0618 - val_accuracy: 0.8846\n",
      "Epoch 145/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2967e-05 - accuracy: 1.0000\n",
      "Epoch 00145: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2967e-05 - accuracy: 1.0000 - val_loss: 1.0866 - val_accuracy: 0.8077\n",
      "Epoch 146/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1926e-05 - accuracy: 1.0000\n",
      "Epoch 00146: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1926e-05 - accuracy: 1.0000 - val_loss: 1.3796 - val_accuracy: 0.8462\n",
      "Epoch 147/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5336e-05 - accuracy: 1.0000\n",
      "Epoch 00147: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.5336e-05 - accuracy: 1.0000 - val_loss: 0.8637 - val_accuracy: 0.8654\n",
      "Epoch 148/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.7696e-06 - accuracy: 1.0000\n",
      "Epoch 00148: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.7696e-06 - accuracy: 1.0000 - val_loss: 1.1180 - val_accuracy: 0.8654\n",
      "Epoch 149/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4352e-05 - accuracy: 1.0000\n",
      "Epoch 00149: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4352e-05 - accuracy: 1.0000 - val_loss: 1.1524 - val_accuracy: 0.8654\n",
      "Epoch 150/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3476e-05 - accuracy: 1.0000\n",
      "Epoch 00150: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3476e-05 - accuracy: 1.0000 - val_loss: 0.7930 - val_accuracy: 0.8846\n",
      "Epoch 151/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1096e-05 - accuracy: 1.0000\n",
      "Epoch 00151: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1096e-05 - accuracy: 1.0000 - val_loss: 1.2695 - val_accuracy: 0.8462\n",
      "Epoch 152/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2613e-05 - accuracy: 1.0000\n",
      "Epoch 00152: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2613e-05 - accuracy: 1.0000 - val_loss: 0.9716 - val_accuracy: 0.8654\n",
      "Epoch 153/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7945e-05 - accuracy: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7945e-05 - accuracy: 1.0000 - val_loss: 1.0351 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.5195e-06 - accuracy: 1.0000\n",
      "Epoch 00154: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.5195e-06 - accuracy: 1.0000 - val_loss: 1.0731 - val_accuracy: 0.8846\n",
      "Epoch 155/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0544e-05 - accuracy: 1.0000\n",
      "Epoch 00155: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0544e-05 - accuracy: 1.0000 - val_loss: 1.0128 - val_accuracy: 0.8269\n",
      "Epoch 156/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3010e-05 - accuracy: 1.0000\n",
      "Epoch 00156: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3010e-05 - accuracy: 1.0000 - val_loss: 0.8988 - val_accuracy: 0.8846\n",
      "Epoch 157/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2476e-05 - accuracy: 1.0000\n",
      "Epoch 00157: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2476e-05 - accuracy: 1.0000 - val_loss: 0.9192 - val_accuracy: 0.8654\n",
      "Epoch 158/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.8521e-05 - accuracy: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.8521e-05 - accuracy: 1.0000 - val_loss: 1.0138 - val_accuracy: 0.8654\n",
      "Epoch 159/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.5753e-06 - accuracy: 1.0000\n",
      "Epoch 00159: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.5753e-06 - accuracy: 1.0000 - val_loss: 1.2364 - val_accuracy: 0.8462\n",
      "Epoch 160/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9922e-05 - accuracy: 1.0000\n",
      "Epoch 00160: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.9922e-05 - accuracy: 1.0000 - val_loss: 0.7879 - val_accuracy: 0.8654\n",
      "Epoch 161/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6855e-05 - accuracy: 1.0000\n",
      "Epoch 00161: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6855e-05 - accuracy: 1.0000 - val_loss: 1.2019 - val_accuracy: 0.8654\n",
      "Epoch 162/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.0446e-05 - accuracy: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.0446e-05 - accuracy: 1.0000 - val_loss: 1.2204 - val_accuracy: 0.8269\n",
      "Epoch 163/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.9724e-06 - accuracy: 1.0000\n",
      "Epoch 00163: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.9724e-06 - accuracy: 1.0000 - val_loss: 1.3260 - val_accuracy: 0.8654\n",
      "Epoch 164/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.1802e-06 - accuracy: 1.0000\n",
      "Epoch 00164: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.1802e-06 - accuracy: 1.0000 - val_loss: 1.4874 - val_accuracy: 0.8269\n",
      "Epoch 165/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.5918e-06 - accuracy: 1.0000\n",
      "Epoch 00165: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.5918e-06 - accuracy: 1.0000 - val_loss: 1.0416 - val_accuracy: 0.8654\n",
      "Epoch 166/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.9840e-06 - accuracy: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.9840e-06 - accuracy: 1.0000 - val_loss: 1.0987 - val_accuracy: 0.8654\n",
      "Epoch 167/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0006e-05 - accuracy: 1.0000\n",
      "Epoch 00167: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0006e-05 - accuracy: 1.0000 - val_loss: 0.9789 - val_accuracy: 0.8654\n",
      "Epoch 168/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2472e-05 - accuracy: 1.0000\n",
      "Epoch 00168: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2472e-05 - accuracy: 1.0000 - val_loss: 1.1514 - val_accuracy: 0.8462\n",
      "Epoch 169/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1042e-05 - accuracy: 1.0000\n",
      "Epoch 00169: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1042e-05 - accuracy: 1.0000 - val_loss: 1.1743 - val_accuracy: 0.8654\n",
      "Epoch 170/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8094e-06 - accuracy: 1.0000\n",
      "Epoch 00170: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.8094e-06 - accuracy: 1.0000 - val_loss: 1.4280 - val_accuracy: 0.8269\n",
      "Epoch 171/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.7928e-06 - accuracy: 1.0000\n",
      "Epoch 00171: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.7928e-06 - accuracy: 1.0000 - val_loss: 1.0781 - val_accuracy: 0.8654\n",
      "Epoch 172/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.5665e-06 - accuracy: 1.0000\n",
      "Epoch 00172: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.5665e-06 - accuracy: 1.0000 - val_loss: 1.1279 - val_accuracy: 0.8846\n",
      "Epoch 173/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.5499e-06 - accuracy: 1.0000\n",
      "Epoch 00173: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.5499e-06 - accuracy: 1.0000 - val_loss: 0.8334 - val_accuracy: 0.8846\n",
      "Epoch 174/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.7152e-06 - accuracy: 1.0000\n",
      "Epoch 00174: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.7152e-06 - accuracy: 1.0000 - val_loss: 1.0817 - val_accuracy: 0.8846\n",
      "Epoch 175/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0882e-06 - accuracy: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.0882e-06 - accuracy: 1.0000 - val_loss: 1.1356 - val_accuracy: 0.8462\n",
      "Epoch 176/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.9794e-06 - accuracy: 1.0000\n",
      "Epoch 00176: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.9794e-06 - accuracy: 1.0000 - val_loss: 1.1528 - val_accuracy: 0.8462\n",
      "Epoch 177/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.7098e-06 - accuracy: 1.0000\n",
      "Epoch 00177: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.7098e-06 - accuracy: 1.0000 - val_loss: 1.1895 - val_accuracy: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.5283e-06 - accuracy: 1.0000\n",
      "Epoch 00178: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.5283e-06 - accuracy: 1.0000 - val_loss: 1.0865 - val_accuracy: 0.8654\n",
      "Epoch 179/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.4594e-06 - accuracy: 1.0000\n",
      "Epoch 00179: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.4594e-06 - accuracy: 1.0000 - val_loss: 1.3469 - val_accuracy: 0.8269\n",
      "Epoch 180/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.3200e-06 - accuracy: 1.0000\n",
      "Epoch 00180: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.3200e-06 - accuracy: 1.0000 - val_loss: 0.9178 - val_accuracy: 0.8654\n",
      "Epoch 181/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.3381e-06 - accuracy: 1.0000\n",
      "Epoch 00181: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 4.3381e-06 - accuracy: 1.0000 - val_loss: 1.2000 - val_accuracy: 0.8846\n",
      "Epoch 182/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.5976e-06 - accuracy: 1.0000\n",
      "Epoch 00182: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.5976e-06 - accuracy: 1.0000 - val_loss: 0.7028 - val_accuracy: 0.9038\n",
      "Epoch 183/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9721e-06 - accuracy: 1.0000\n",
      "Epoch 00183: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.9721e-06 - accuracy: 1.0000 - val_loss: 0.7436 - val_accuracy: 0.8654\n",
      "Epoch 184/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.2066e-06 - accuracy: 1.0000\n",
      "Epoch 00184: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.2066e-06 - accuracy: 1.0000 - val_loss: 0.9374 - val_accuracy: 0.8846\n",
      "Epoch 185/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8888e-06 - accuracy: 1.0000\n",
      "Epoch 00185: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.8888e-06 - accuracy: 1.0000 - val_loss: 1.2821 - val_accuracy: 0.8462\n",
      "Epoch 186/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6174e-06 - accuracy: 1.0000\n",
      "Epoch 00186: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6174e-06 - accuracy: 1.0000 - val_loss: 1.1516 - val_accuracy: 0.8654\n",
      "Epoch 187/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9252e-06 - accuracy: 1.0000\n",
      "Epoch 00187: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.9252e-06 - accuracy: 1.0000 - val_loss: 0.9904 - val_accuracy: 0.8654\n",
      "Epoch 188/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.2991e-06 - accuracy: 1.0000\n",
      "Epoch 00188: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.2991e-06 - accuracy: 1.0000 - val_loss: 1.0255 - val_accuracy: 0.8462\n",
      "Epoch 189/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4537e-06 - accuracy: 1.0000\n",
      "Epoch 00189: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.4537e-06 - accuracy: 1.0000 - val_loss: 0.7725 - val_accuracy: 0.8654\n",
      "Epoch 190/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0979e-06 - accuracy: 1.0000\n",
      "Epoch 00190: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.0979e-06 - accuracy: 1.0000 - val_loss: 1.2965 - val_accuracy: 0.8462\n",
      "Epoch 191/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.3907e-06 - accuracy: 1.0000\n",
      "Epoch 00191: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.3907e-06 - accuracy: 1.0000 - val_loss: 0.8781 - val_accuracy: 0.9038\n",
      "Epoch 192/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.4917e-06 - accuracy: 1.0000\n",
      "Epoch 00192: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.4917e-06 - accuracy: 1.0000 - val_loss: 0.8691 - val_accuracy: 0.8846\n",
      "Epoch 193/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.6168e-06 - accuracy: 1.0000\n",
      "Epoch 00193: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.6168e-06 - accuracy: 1.0000 - val_loss: 0.9640 - val_accuracy: 0.8462\n",
      "Epoch 194/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.1083e-06 - accuracy: 1.0000\n",
      "Epoch 00194: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.1083e-06 - accuracy: 1.0000 - val_loss: 1.0861 - val_accuracy: 0.8846\n",
      "Epoch 195/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.0988e-06 - accuracy: 1.0000\n",
      "Epoch 00195: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.0988e-06 - accuracy: 1.0000 - val_loss: 0.9700 - val_accuracy: 0.8846\n",
      "Epoch 196/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.6425e-06 - accuracy: 1.0000\n",
      "Epoch 00196: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.6425e-06 - accuracy: 1.0000 - val_loss: 1.1610 - val_accuracy: 0.8654\n",
      "Epoch 197/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.1439e-06 - accuracy: 1.0000\n",
      "Epoch 00197: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.1439e-06 - accuracy: 1.0000 - val_loss: 1.2150 - val_accuracy: 0.8654\n",
      "Epoch 198/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0759e-06 - accuracy: 1.0000\n",
      "Epoch 00198: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.0759e-06 - accuracy: 1.0000 - val_loss: 1.1491 - val_accuracy: 0.8654\n",
      "Epoch 199/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.6472e-06 - accuracy: 1.0000\n",
      "Epoch 00199: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.6472e-06 - accuracy: 1.0000 - val_loss: 1.3281 - val_accuracy: 0.8462\n",
      "Epoch 200/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.2706e-06 - accuracy: 1.0000\n",
      "Epoch 00200: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.2706e-06 - accuracy: 1.0000 - val_loss: 1.4129 - val_accuracy: 0.8654\n",
      "Epoch 201/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9612e-06 - accuracy: 1.0000\n",
      "Epoch 00201: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00201: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.9612e-06 - accuracy: 1.0000 - val_loss: 1.3838 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4681e-06 - accuracy: 1.0000\n",
      "Epoch 00202: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00202: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4681e-06 - accuracy: 1.0000 - val_loss: 1.2029 - val_accuracy: 0.8269\n",
      "Epoch 203/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.3802e-06 - accuracy: 1.0000\n",
      "Epoch 00203: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00203: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.3802e-06 - accuracy: 1.0000 - val_loss: 1.3226 - val_accuracy: 0.8462\n",
      "Epoch 204/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7166e-06 - accuracy: 1.0000\n",
      "Epoch 00204: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00204: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.7166e-06 - accuracy: 1.0000 - val_loss: 1.0053 - val_accuracy: 0.8846\n",
      "Epoch 205/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.7754e-06 - accuracy: 1.0000\n",
      "Epoch 00205: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00205: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.7754e-06 - accuracy: 1.0000 - val_loss: 1.0264 - val_accuracy: 0.8654\n",
      "Epoch 206/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6795e-06 - accuracy: 1.0000\n",
      "Epoch 00206: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00206: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.6795e-06 - accuracy: 1.0000 - val_loss: 0.7793 - val_accuracy: 0.8846\n",
      "Epoch 207/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4749e-06 - accuracy: 1.0000\n",
      "Epoch 00207: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00207: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4749e-06 - accuracy: 1.0000 - val_loss: 1.0429 - val_accuracy: 0.9038\n",
      "Epoch 208/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5445e-06 - accuracy: 1.0000\n",
      "Epoch 00208: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00208: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.5445e-06 - accuracy: 1.0000 - val_loss: 1.2032 - val_accuracy: 0.8462\n",
      "Epoch 209/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4696e-06 - accuracy: 1.0000\n",
      "Epoch 00209: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00209: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4696e-06 - accuracy: 1.0000 - val_loss: 0.9092 - val_accuracy: 0.8654\n",
      "Epoch 210/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.5334e-06 - accuracy: 1.0000\n",
      "Epoch 00210: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00210: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.5334e-06 - accuracy: 1.0000 - val_loss: 1.2231 - val_accuracy: 0.8462\n",
      "Epoch 211/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7227e-06 - accuracy: 1.0000\n",
      "Epoch 00211: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00211: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7227e-06 - accuracy: 1.0000 - val_loss: 1.4406 - val_accuracy: 0.8269\n",
      "Epoch 212/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2827e-06 - accuracy: 1.0000\n",
      "Epoch 00212: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00212: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2827e-06 - accuracy: 1.0000 - val_loss: 0.9629 - val_accuracy: 0.8654\n",
      "Epoch 213/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6310e-06 - accuracy: 1.0000\n",
      "Epoch 00213: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00213: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6310e-06 - accuracy: 1.0000 - val_loss: 1.0658 - val_accuracy: 0.8462\n",
      "Epoch 214/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.0898e-07 - accuracy: 1.0000\n",
      "Epoch 00214: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00214: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.0898e-07 - accuracy: 1.0000 - val_loss: 1.2428 - val_accuracy: 0.8654\n",
      "Epoch 215/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4700e-06 - accuracy: 1.0000\n",
      "Epoch 00215: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00215: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 1.4700e-06 - accuracy: 1.0000 - val_loss: 1.0725 - val_accuracy: 0.8654\n",
      "Epoch 216/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3115e-06 - accuracy: 1.0000\n",
      "Epoch 00216: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00216: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3115e-06 - accuracy: 1.0000 - val_loss: 0.9531 - val_accuracy: 0.8462\n",
      "Epoch 217/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2730e-06 - accuracy: 1.0000\n",
      "Epoch 00217: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00217: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2730e-06 - accuracy: 1.0000 - val_loss: 1.1695 - val_accuracy: 0.8462\n",
      "Epoch 218/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.1071e-07 - accuracy: 1.0000\n",
      "Epoch 00218: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00218: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.1071e-07 - accuracy: 1.0000 - val_loss: 1.3422 - val_accuracy: 0.8654\n",
      "Epoch 219/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2196e-06 - accuracy: 1.0000\n",
      "Epoch 00219: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00219: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2196e-06 - accuracy: 1.0000 - val_loss: 1.1033 - val_accuracy: 0.8462\n",
      "Epoch 220/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3856e-06 - accuracy: 1.0000\n",
      "Epoch 00220: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00220: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.3856e-06 - accuracy: 1.0000 - val_loss: 1.3938 - val_accuracy: 0.8654\n",
      "Epoch 221/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4501e-06 - accuracy: 1.0000\n",
      "Epoch 00221: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00221: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4501e-06 - accuracy: 1.0000 - val_loss: 1.1034 - val_accuracy: 0.8462\n",
      "Epoch 222/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4892e-06 - accuracy: 1.0000\n",
      "Epoch 00222: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00222: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4892e-06 - accuracy: 1.0000 - val_loss: 0.9364 - val_accuracy: 0.8846\n",
      "Epoch 223/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4567e-06 - accuracy: 1.0000\n",
      "Epoch 00223: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00223: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.4567e-06 - accuracy: 1.0000 - val_loss: 1.0337 - val_accuracy: 0.8077\n",
      "Epoch 224/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0662e-06 - accuracy: 1.0000\n",
      "Epoch 00224: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00224: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.0662e-06 - accuracy: 1.0000 - val_loss: 1.4250 - val_accuracy: 0.8077\n",
      "Epoch 225/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.6053e-06 - accuracy: 1.0000\n",
      "Epoch 00225: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00225: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.6053e-06 - accuracy: 1.0000 - val_loss: 1.0669 - val_accuracy: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7416e-06 - accuracy: 1.0000\n",
      "Epoch 00226: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00226: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.7416e-06 - accuracy: 1.0000 - val_loss: 1.1504 - val_accuracy: 0.8462\n",
      "Epoch 227/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.8901e-06 - accuracy: 1.0000\n",
      "Epoch 00227: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00227: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 2.8901e-06 - accuracy: 1.0000 - val_loss: 0.9965 - val_accuracy: 0.8846\n",
      "Epoch 228/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.8696e-07 - accuracy: 1.0000\n",
      "Epoch 00228: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00228: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 9.8696e-07 - accuracy: 1.0000 - val_loss: 1.2696 - val_accuracy: 0.8462\n",
      "Epoch 229/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.2733e-07 - accuracy: 1.0000\n",
      "Epoch 00229: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00229: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.2733e-07 - accuracy: 1.0000 - val_loss: 0.8436 - val_accuracy: 0.8654\n",
      "Epoch 230/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1049e-06 - accuracy: 1.0000\n",
      "Epoch 00230: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00230: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 1.1049e-06 - accuracy: 1.0000 - val_loss: 1.0927 - val_accuracy: 0.8846\n",
      "Epoch 231/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2670e-06 - accuracy: 1.0000\n",
      "Epoch 00231: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00231: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2670e-06 - accuracy: 1.0000 - val_loss: 1.3554 - val_accuracy: 0.8462\n",
      "Epoch 232/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6788e-06 - accuracy: 1.0000\n",
      "Epoch 00232: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00232: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.6788e-06 - accuracy: 1.0000 - val_loss: 1.4673 - val_accuracy: 0.8269\n",
      "Epoch 233/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 9.3310e-07 - accuracy: 1.0000\n",
      "Epoch 00233: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00233: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 9.3310e-07 - accuracy: 1.0000 - val_loss: 1.0308 - val_accuracy: 0.8846\n",
      "Epoch 234/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.7854e-07 - accuracy: 1.0000\n",
      "Epoch 00234: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00234: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.7854e-07 - accuracy: 1.0000 - val_loss: 0.9200 - val_accuracy: 0.8846\n",
      "Epoch 235/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1353e-06 - accuracy: 1.0000\n",
      "Epoch 00235: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00235: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1353e-06 - accuracy: 1.0000 - val_loss: 0.9720 - val_accuracy: 0.8269\n",
      "Epoch 236/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.7541e-07 - accuracy: 1.0000\n",
      "Epoch 00236: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00236: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.7541e-07 - accuracy: 1.0000 - val_loss: 0.7375 - val_accuracy: 0.8846\n",
      "Epoch 237/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.0610e-07 - accuracy: 1.0000\n",
      "Epoch 00237: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00237: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.0610e-07 - accuracy: 1.0000 - val_loss: 1.0506 - val_accuracy: 0.8654\n",
      "Epoch 238/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.1211e-07 - accuracy: 1.0000\n",
      "Epoch 00238: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00238: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 8.1211e-07 - accuracy: 1.0000 - val_loss: 1.1116 - val_accuracy: 0.8269\n",
      "Epoch 239/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2774e-06 - accuracy: 1.0000\n",
      "Epoch 00239: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00239: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.2774e-06 - accuracy: 1.0000 - val_loss: 1.3086 - val_accuracy: 0.8462\n",
      "Epoch 240/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.5569e-07 - accuracy: 1.0000\n",
      "Epoch 00240: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00240: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 6.5569e-07 - accuracy: 1.0000 - val_loss: 1.2046 - val_accuracy: 0.8462\n",
      "Epoch 241/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.7775e-07 - accuracy: 1.0000\n",
      "Epoch 00241: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00241: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.7775e-07 - accuracy: 1.0000 - val_loss: 1.5599 - val_accuracy: 0.8462\n",
      "Epoch 242/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.2580e-07 - accuracy: 1.0000\n",
      "Epoch 00242: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00242: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.2580e-07 - accuracy: 1.0000 - val_loss: 1.0145 - val_accuracy: 0.8654\n",
      "Epoch 243/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1837e-06 - accuracy: 1.0000\n",
      "Epoch 00243: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00243: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1837e-06 - accuracy: 1.0000 - val_loss: 1.1279 - val_accuracy: 0.8269\n",
      "Epoch 244/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.2819e-07 - accuracy: 1.0000\n",
      "Epoch 00244: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00244: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.2819e-07 - accuracy: 1.0000 - val_loss: 1.3072 - val_accuracy: 0.8462\n",
      "Epoch 245/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.9813e-07 - accuracy: 1.0000\n",
      "Epoch 00245: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00245: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.9813e-07 - accuracy: 1.0000 - val_loss: 1.0236 - val_accuracy: 0.8462\n",
      "Epoch 246/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1018e-06 - accuracy: 1.0000\n",
      "Epoch 00246: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00246: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 1.1018e-06 - accuracy: 1.0000 - val_loss: 1.0528 - val_accuracy: 0.8462\n",
      "Epoch 247/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 8.9373e-07 - accuracy: 1.0000\n",
      "Epoch 00247: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00247: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 8.9373e-07 - accuracy: 1.0000 - val_loss: 1.0609 - val_accuracy: 0.8654\n",
      "Epoch 248/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 6.5939e-07 - accuracy: 1.0000\n",
      "Epoch 00248: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00248: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 6.5939e-07 - accuracy: 1.0000 - val_loss: 0.8351 - val_accuracy: 0.8654\n",
      "Epoch 249/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.8919e-07 - accuracy: 1.0000\n",
      "Epoch 00249: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00249: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.8919e-07 - accuracy: 1.0000 - val_loss: 1.3051 - val_accuracy: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.9179e-07 - accuracy: 1.0000\n",
      "Epoch 00250: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00250: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 5.9179e-07 - accuracy: 1.0000 - val_loss: 0.7581 - val_accuracy: 0.8462\n",
      "Epoch 251/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 7.4294e-07 - accuracy: 1.0000\n",
      "Epoch 00251: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00251: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 7.4294e-07 - accuracy: 1.0000 - val_loss: 1.2780 - val_accuracy: 0.8462\n",
      "Epoch 252/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 5.1941e-07 - accuracy: 1.0000\n",
      "Epoch 00252: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00252: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 5.1941e-07 - accuracy: 1.0000 - val_loss: 1.2993 - val_accuracy: 0.8462\n",
      "Epoch 253/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 4.0546e-07 - accuracy: 1.0000\n",
      "Epoch 00253: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00253: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 4.0546e-07 - accuracy: 1.0000 - val_loss: 1.2753 - val_accuracy: 0.8462\n",
      "Epoch 254/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.9023e-07 - accuracy: 1.0000\n",
      "Epoch 00254: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00254: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 3.9023e-07 - accuracy: 1.0000 - val_loss: 1.0523 - val_accuracy: 0.8654\n",
      "Epoch 255/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8932 - accuracy: 0.8737\n",
      "Epoch 00255: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00255: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.8932 - accuracy: 0.8737 - val_loss: 83263.4141 - val_accuracy: 0.0962\n",
      "Epoch 256/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.8647\n",
      "Epoch 00256: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00256: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.5078 - accuracy: 0.8647 - val_loss: 205.9726 - val_accuracy: 0.0962\n",
      "Epoch 257/2000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9499\n",
      "Epoch 00257: val_loss did not improve from 0.39689\n",
      "\n",
      "Epoch 00257: val_accuracy did not improve from 0.90385\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.1586 - accuracy: 0.9499 - val_loss: 0.9353 - val_accuracy: 0.8654\n",
      "2  번째 학습을 완료했습니다.\n",
      "Epoch 1/2000\n",
      " 2/63 [..............................] - ETA: 22s - loss: 4.6443 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1568s vs `on_train_batch_end` time: 0.5839s). Check your callbacks.\n",
      " 5/63 [=>............................] - ETA: 30s - loss: 11.3559 - accuracy: 0.1125"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,1024,7,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/functional_7/conv2d_103/Conv2D/Conv2DBackpropInput (defined at <ipython-input-6-d10037c174cd>:23) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_195428]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d10037c174cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreLR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcp2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \"\"\"\n\u001b[0;32m   1814\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit_generator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1816\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,1024,7,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/functional_7/conv2d_103/Conv2D/Conv2DBackpropInput (defined at <ipython-input-6-d10037c174cd>:23) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_195428]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "index=1\n",
    "result = 0\n",
    "for train_index,val_index in kfold.split(x,y):\n",
    "    modelpath = './AI_models/02_04_AI_val_loss_index_{}.h5'.format(index)\n",
    "    modelpath2 = './AI_models/02_04_AI_val_accuracy_index_{}.h5'.format(index)\n",
    "    cp = ModelCheckpoint(monitor = 'val_loss',filepath=modelpath,save_best_only=True,verbose=1)\n",
    "    cp2 = ModelCheckpoint(monitor = 'val_accuracy',filepath=modelpath2,save_best_only=True,verbose=1)\n",
    "\n",
    "\n",
    "    x_train = x[train_index]\n",
    "    x_val = x[val_index]\n",
    "    y_train = y[train_index]\n",
    "    y_val = y[val_index]\n",
    "\n",
    "    onehot = OneHotEncoder()\n",
    "    y_train = onehot.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "    y_val = onehot.fit_transform(y_val.reshape(-1,1)).toarray().astype('float32')\n",
    "\n",
    "    train_generator = datagen.flow(x_train,y_train,batch_size=32)\n",
    "    val_generator = datagen.flow(x_val,y_val)\n",
    "    model = modeling()\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n",
    "    model.fit_generator(train_generator,validation_data = val_generator,epochs=epochs,callbacks=[cp,es,reLR,cp2])\n",
    "\n",
    "    model = load_model(modelpath)\n",
    "    model2 = load_model(modelpath2)\n",
    "    df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "    x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred2 = model2.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred,axis=-1)\n",
    "    y_pred2 = np.argmax(y_pred2,axis=-1)\n",
    "    df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "    df_sub['digit'] = y_pred\n",
    "    df_sub.to_csv('./AI_models/loss_kfold_{}.csv'.format(index))\n",
    "    df_sub['digit'] = y_pred2\n",
    "    df_sub.to_csv('./AI_models/accuracy_kfold_{}.csv'.format(index))\n",
    "\n",
    "    print(index, \" 번째 학습을 완료했습니다.\")\n",
    "    index+=1\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "0.905\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onOYBeVy-Mkb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KswTfNSi-Mkc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "0.87\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx1Z4nhC-Mkc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = _x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(128,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(256,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "_x = Conv2D(512,3,padding='same')(x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "_x = Conv2D(128,3,padding='same')(_x)\n",
    "_x = BatchNormalization()(_x)\n",
    "_x = Activation('relu')(_x)\n",
    "x = x+_x\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dense(10,activation='softmax')(x)\n",
    "outputs=x\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "44FNCmNG-Mkd"
   },
   "outputs": [],
   "source": [
    "model = load_model('./models/02_03_imger_best_index_1.h5')\n",
    "df = pd.read_csv(\"test.csv\",index_col=[0])\n",
    "x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzM0rw6U-Mke",
    "outputId": "0312f408-37af-44e5-b673-febb0961614c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "binary_model = []\n",
    "for i in range(0,10):\n",
    "    print(i)\n",
    "    model = load_model('./binary_models/{}_binary.h5'.format(i))\n",
    "    binary_model.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QwofhPdW-Mke"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "def ordering(array):\n",
    "    temp = array.copy()\n",
    "    result = []\n",
    "    for i in range(len(temp)):\n",
    "        sol = np.argmax(temp)\n",
    "        result.append(sol)\n",
    "        temp[sol]=0\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fI2mNTi-Mkf",
    "outputId": "f253edd1-103b-41f5-8832-f5ac667f5a38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6569414e-08, 2.2260668e-08, 4.6740688e-06, ..., 2.5446711e-10,\n",
       "        5.9703314e-07, 5.4656005e-11],\n",
       "       [2.9817595e-16, 2.0647546e-12, 4.4424861e-16, ..., 2.6480063e-10,\n",
       "        8.9294266e-11, 1.0000000e+00],\n",
       "       [2.4707546e-05, 1.3310514e-01, 8.9886552e-03, ..., 9.1972132e-04,\n",
       "        1.2284001e-02, 2.9377347e-06],\n",
       "       ...,\n",
       "       [8.6063210e-09, 7.5784501e-10, 2.6377617e-10, ..., 3.1178827e-13,\n",
       "        1.2005766e-07, 6.1626551e-15],\n",
       "       [1.0054513e-03, 2.9064235e-01, 1.6038346e-05, ..., 1.7447629e-06,\n",
       "        8.1512779e-03, 4.5916289e-03],\n",
       "       [9.9825722e-01, 6.9838888e-16, 2.4161539e-10, ..., 1.8471900e-15,\n",
       "        6.6174334e-13, 1.4705076e-15]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy1domP0-Mkf",
    "outputId": "db3b1dcb-6d43-4fc7-9938-34990934eb64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 3, ..., 6, 5, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_notyet = np.argmax(y_pred,axis=-1)\n",
    "y_notyet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1xQykxG-Mkf",
    "outputId": "946d6597-cef3-4297-f226-1a1349f5005d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.88120088e-36],\n",
       "       [1.13777095e-36]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_model[3].predict(x_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULE3KyHS-Mkg",
    "outputId": "41ead2b9-7d2e-44a9-a6ba-e8c4645752cd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cc4ab4411252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Jhbsfx-Mkg"
   },
   "outputs": [],
   "source": [
    "k=729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPmflg5c-Mkg",
    "outputId": "05282ba4-d754-4522-aa33-73d47b49b612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래모델 :  9 \n",
      "원래모델확률 :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] \n",
      "바이너리 :  1.6463632e-24\n",
      "0 0.0\n",
      "1 5.6854813e-21\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 3.455557e-05\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 1.2415284e-22\n",
      "9 1.6463632e-24\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "for i in [k]:\n",
    "    temp_result = y_notyet[i]\n",
    "    binary_result = binary_model[temp_result].predict(np.array([x_test[i].reshape(28,28,1)]))\n",
    "    a=np.round(y_pred[i],3)\n",
    "    print(\"원래모델 : \",temp_result,'\\n원래모델확률 : ',a,\"\\n바이너리 : \",binary_result[0][0])\n",
    "    print(\"0\",binary_model[0].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"1\",binary_model[1].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"2\",binary_model[2].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"3\",binary_model[3].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"4\",binary_model[4].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"5\",binary_model[5].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"6\",binary_model[6].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"7\",binary_model[7].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"8\",binary_model[8].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n",
    "    print(\"9\",binary_model[9].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEgcogTL-Mkg",
    "outputId": "b1e9adb4-4593-4bda-ae06-bb49fd8bf6ac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV90lEQVR4nO3da4xc5XkH8P8zu+tdvL7g9Q1jr2xjjGRuMXTFJaQURJIClQJplSZum1IJ4bQKEqmiNoS0Cv0QCTUlaT60JKZQTEOJUBMKalEDsaJSokK9uAbbMQZjbDBedn3BlzX27szO0w87VBvY838mc+amvP+fZO16nnnPeefMPHNm9jnv+5q7Q0R+9RVa3QERaQ4lu0gilOwiiVCyiyRCyS6SiM5m7myGdXuP9Wbf4Ve1MGDG41FFJGjOt52jbTXyPrZ21crHFe2bPKmn/STGfWzaDeRKdjO7HsB3AHQA+Ad3v4fdv8d6cUXnb2bGfWIi2CH5IOJl3jbCth1tP2hrHR1806VirvZ82yV+h0Kw7TJ/TqxrRrB/8tiihIn6lkcjHxdQxRt4dkJbZ1ew7ezX4vOlH2fGav4Yb2YdAP4OwA0AzgewzszOr3V7ItJYeb6zXwZgt7vvcfdxAD8AcFN9uiUi9ZYn2ZcCeGvK//dXbvsFZrbezAbNbLDoYzl2JyJ55En26b50fOiLirtvcPcBdx/osu4cuxORPPIk+34A/VP+vwzAgXzdEZFGyZPsmwGsNrOVZjYDwOcAPFmfbolIvdVcenP3kpndDuDHmCy9PejuO+rWs2lYgdQfjZcrwlJJvPN87dmmo9JajrKgdfOvTl4MSnNhvTlHybPBZT/WN3f+uKIycFgeC3hxvPZ9szwgctXZ3f0pAE/l2YaINIculxVJhJJdJBFKdpFEKNlFEqFkF0mEkl0kEU0dzw4HvEyG/gXDAtlwzajmGtWyC3Pn8PYzZ9I41cHfU0cvOovGD13En6au0ezY7Ld4zbZQ5Md81uA+Gp949yiNG7lGILz2IajDs1p1ZeekMX/c1pHv+oJ4uDYZ4hrU0emwZfKwdGYXSYSSXSQRSnaRRCjZRRKhZBdJhJJdJBHNLb3lZJ3Z3Y3KMNFQz5NXnkvjI5eSQxW8ZY4t5GWYP7/232j8E727aPxr+z+VGdu8+TzatnCad352/zk0vuCl92i8643hzFj52HHaFmVe3iqfDspbTDB0NxxmGpRyGzkjMMsDkKY6s4skQskukgglu0gilOwiiVCyiyRCyS6SCCW7SCKaX2dn0wOHS9USwXDIjoULaDwaRjreR6YlDrp98YV7aXzdnN00Prcwi8Znd2Yvq+UFPpSz3EPDOL6Kx0eX86G/Xcey6/Rz9vE6es8RXus+uJZP57zs3kEaZ6LrNnKtOAxehw9XkGV90xBXEVGyiyRCyS6SCCW7SCKU7CKJULKLJELJLpKI5tbZLRiTzqaZjkRLBwfTOXsw/JjWq4O3zFWzDtJ4j/Gn4b0yr/mWPLsDVg7GbQd1eO/i7ctdvP1ET3b89CLaFJ1nZ18/AADFQw08VwXXbUTj1aM6PJ1GO6rRszp8Mfv5ypXsZrYXwAkAEwBK7j6QZ3si0jj1OLNf6+6H6rAdEWkgfWcXSUTeZHcAT5vZi2a2fro7mNl6Mxs0s8Gi8+9gItI4eT/GX+XuB8xsEYBnzOwVd3926h3cfQOADQAwp9CX4y9wIpJHrjO7ux+o/BwB8DiAy+rRKRGpv5qT3cx6zWz2+78D+CSA7fXqmIjUV56P8YsBPG6TY9A7Afyzu/9Hrt5Ey+CWc3wQKUXjj4P2ZNB6oe80bfp7fc/TeGfwNBws87nZB4f6s4PB5QcWDcaPvnjleEpsEf8bzt8PPELjD7xzNY2/8M1LM2Or/2wLbUvnXQCAYFnlaNllILtOH80bjxrnpK852d19D4CP1NpeRJpLpTeRRCjZRRKhZBdJhJJdJBFKdpFENHeIq1cxBS/BhxXyckSpn08lPdEdDPXszq5h3XbRz2jbNXzGYxwv89Ldb225jcaX/WV23wrvvknblpbOp/ET5/TS+LGV/HxxenF2335j1Wu07eXdJ2n8YyuepvHPlrKHgo6dz+fI9m2v8ni0rHI0HTQd4ppjOWnPfh3rzC6SCCW7SCKU7CKJULKLJELJLpIIJbtIIpTsIolo8lTSBusMis4EW6q2Y9482vadS3i9uDSL19mtJ7u2ecPsbbTtzAJfF/mvhvmcH4vu5e3LP385OxYN1TwwRMNzd/Almc9cehaN7/qThZmxO876CW17hvFadUcw5fKJYndmrPskv7YhHOIaiJZ8jurwebadRWd2kUQo2UUSoWQXSYSSXSQRSnaRRCjZRRKhZBdJRHPr7AE6xjdgvbwePHZmOFd0sIPseJEsmVyNncd5rfr4Sl5nn/c8mZa4ph5NaT8ePCcz+HUT5TOyx7P3Gh8TXgbf9qvjfIrtN/Zn1/jXlPgy2tGSzWEdPhqTzmrl0b5rpDO7SCKU7CKJULKLJELJLpIIJbtIIpTsIolQsoskoq3q7NFYdzZfdvnQYdp27h6yrDGA04tpmNpb5HPSXziD921OFx9bvf3XeLX8zIdrr9nyufiBfV8doPGx+XxN6D++alNmbEUnvzbicPkUjd++ex2Nz9iXPZ4dxWDe9+C4eLC8eJ7XcrR0OX1OyWbDM7uZPWhmI2a2fcptfWb2jJm9VvnJZ44QkZar5mP8QwCu/8BtdwLY5O6rAWyq/F9E2liY7O7+LIAjH7j5JgAbK79vBHBzfbslIvVW63f2xe4+BADuPmRmi7LuaGbrAawHgB7w72gi0jgN/2u8u29w9wF3H+gyPqBDRBqn1mQfNrMlAFD5OVK/LolII9Sa7E8CuKXy+y0AnqhPd0SkUcLv7Gb2KIBrACwws/0Avg7gHgCPmdmtAN4E8Jmq9uaeq77Iap8+wdvmHHIOHM2e5/srm3+HNt198XM0PkrmNwcAdPA6++6/vSIzVu7hx6XrTF7jv7T/FRq/cX72nPUA8NnZ2fPSl4NzzStFPtf/njcz/1QEAJjJHlo5eL2w12k1otdyIXu8u5f5cWFtQXYbJru7Z125cF3UVkTahy6XFUmEkl0kEUp2kUQo2UUSoWQXSUTzh7jmWAqXlUM6l/IxqieWB+9rUamExMqH+fK73/3ZtXzfhWDC504eX3RO9hDagYVv0bafmreFxi/vOU7jcwtn0DjIdND/enIWbfmNXTfSuJ3gL99OMtO0B6U3Wt4C4CX+nHg5xyTeQY44yBBXslud2UUSoWQXSYSSXSQRSnaRRCjZRRKhZBdJhJJdJBHNr7OTaXCj6XtpLTxYIrec95GS+qWVg+WgyUzPAGAT/D3XgyGuw8NzM2M/GeV18Gvn7qTxmcaHwEb2l0YzY998/bdp28Nv5Ju0eDz7sAB9LAj4QT79dySq01MF/mKtdfitzuwiiVCyiyRCyS6SCCW7SCKU7CKJULKLJELJLpKItlqyOawfsjp7NDVw9EijsijbfDB02TzYeBQu8TvYsewx4+PBtld0HaLxLuNj9UfLvA7/3SNXZsaGXl9I20aPOzpu433ZT9q+m/m+V/wjH8dfGubrongpWBK6ixzXaMnmGunMLpIIJbtIIpTsIolQsoskQskukgglu0gilOwiiWiveeODMelwNqg8qEXzsifCUjiJR+PNbSLH2OYqeFf2/lf3D9O2yzuLND4WHJj7jl5A4/9+/69n7/t1vu+hj/KX5/jc2udmL87hbYurltC4jfDrE/iFGYCXyGNnr3MA1kmOS555483sQTMbMbPtU26728zeNrOtlX98Nn8RablqPsY/BOD6aW7/truvrfx7qr7dEpF6C5Pd3Z8FcKQJfRGRBsrzB7rbzezlysf8zMnCzGy9mQ2a2WARYzl2JyJ51Jrs9wFYBWAtgCEA92bd0d03uPuAuw90obvG3YlIXjUlu7sPu/uEu5cB3A/gsvp2S0TqraZkN7OpdYlPA9iedV8RaQ9hnd3MHgVwDYAFZrYfwNcBXGNmazFZ1dsL4AtV7c0s3zhey35v8lOnaNPu4E+Mp/nwZj5mPSrSR+XgqAwfvCV3L8peiPwPl/03bfsvJ86j8SeHP0Lj7zy2nMaXPLojM+bjvM4+r+9iGh++IqhHk/n8J3p428MX8vn2F7/aR+MTBw/SOKuVR2PhWR6wF1OY7O6+bpqbH4jaiUh70eWyIolQsoskQskukgglu0gilOwiiWjyEFen5TUvB6UUsqJz+Rif+rd3hE9TfeKcYMnnGaRvuaeK5vFZ+/h78qkTszJjfzF6M9/4ON/2Gfuzp6kGgP7vvUDjE+T5LnTzKyq7j/LnzMrBEt+kkuud/LV2YiV/0uafezaNF47z16OPZV86ToewAvAiWQOcDI/VmV0kEUp2kUQo2UUSoWQXSYSSXSQRSnaRRCjZRRLR3Dq7V7EsM2vOZqHu4g9l5hCfEqswzoc0Oivp5pwpevYb/D130YujNP7Wx7Pr7OfdupXvPBhWbB28lu1savBA+TRf7nnma4dp3K5bROPODmtwbUQ5qMOPLuevl7kvBcetkB2Prjfh85pnh3RmF0mEkl0kEUp2kUQo2UUSoWQXSYSSXSQRSnaRRDR/yWYmWKqWFdp9jNd7u97mc0l3jC2j8YmZbDw7bRrGCyV+h0MX99J4/zfIdNHR8r9sam8EY6eBeJlt1jSo4ePQuzTccWoxjRfnZF9DYCXeb3pdBYCTS/h58szZ2dc+AABO174UmnWSOQaK2Y9LZ3aRRCjZRRKhZBdJhJJdJBFKdpFEKNlFEqFkF0lEe9XZyRhfAECOsdMo5WgL8DnIg26jwGvd7338JI2b8fa711yeGTv3T5+nbcP5BXI+J9Ec6HTTq5bSeIld+wBeS4/mjWfLPQNA7xCfB6B85CjfPrvGIJhjwEtkqes888abWb+Z/dTMdprZDjO7o3J7n5k9Y2avVX7Oi7YlIq1Tzcf4EoAvu/saAFcA+KKZnQ/gTgCb3H01gE2V/4tImwqT3d2H3H1L5fcTAHYCWArgJgAbK3fbCODmBvVRROrgl/oDnZmtAHAJgBcALHb3IWDyDQHAtBOCmdl6Mxs0s8Eiar8eWETyqTrZzWwWgB8C+JK781XrpnD3De4+4O4DXeAL+YlI41SV7GbWhclEf8Tdf1S5edjMllTiSwCMNKaLIlIPYV3EzAzAAwB2uvu3poSeBHALgHsqP58I92ZGh+fRkgLylXEiHTm+YRivlGD+uXx47fcvfIjGj5b5MNTfH78tM7b/qx+lbVd8/00aj4bIlvtm07hNZLd/b/kc2vbIGr5cdHTgvYOUochQUACYvYefB+f9114aZ0tVR8KppKOh4BmqyZ6rAHwewDYz21q57S5MJvljZnYrgDcBfKamHohIU4TJ7u7PIXsZhOvq2x0RaRRdLiuSCCW7SCKU7CKJULKLJELJLpKIJg9xdTp8L1wemAzHjNqWj/Bpiefu6afxU2fRMHXumYdofGVnD40XgjWh/+CC/8mMPf78NbTtqTX8gY1cwmv8xVm1DxWd6Alq+DOCOnpwqmL77hzljRdu4cOOJ4bzXUOW57VcawlfZ3aRRCjZRRKhZBdJhJJdJBFKdpFEKNlFEqFkF0lEc+vsDniplBmOlg8GyJLNwZTIfuoUjc968z0a77ogewne8T6+7+6O7McMAGU2TzWA6D15JhmMf3w179vJs/mY8ajWHWLTaEebDpe65vGuY9nHbemzp/m2t7zCN275zpMWTT/eADqziyRCyS6SCCW7SCKU7CKJULKLJELJLpIIJbtIIpq/ZLPxsdm8bePem+x/d9F4f+eazNiBq2fStoNDfKz8fy7g7XeNnU3jD+26IjNWGOfHu9wdjUen4bAWTuPBssiFYDXp2Xt4fP627DHphcGdvHE0aDx6LUbLLucYz05ziBxvndlFEqFkF0mEkl0kEUp2kUQo2UUSoWQXSYSSXSQR5sFaz2bWD+BhAGdhcgTyBnf/jpndDeA2AAcrd73L3Z9i25pjfX65ZS/8Go1nj9ZvbyS2rnzHogW0rfeeQeOHrlxE4x1F/hx1nsqOR3OrB1PShwql2tYKBwAPrrmYOcTnIOjY/TaNl48ey953MP8Be76raR/V2dn2w22Xs+Mv+CYc9yPTHthqLqopAfiyu28xs9kAXjSzZyqxb7v731SxDRFpsWrWZx8CMFT5/YSZ7QSwtNEdE5H6+qW+s5vZCgCXAHihctPtZvaymT1oZvMy2qw3s0EzGywie/okEWmsqpPdzGYB+CGAL7n7cQD3AVgFYC0mz/z3TtfO3Te4+4C7D3ShO3+PRaQmVSW7mXVhMtEfcfcfAYC7D7v7hLuXAdwP4LLGdVNE8gqT3cwMwAMAdrr7t6bcvmTK3T4NYHv9uyci9VLNX+OvAvB5ANvMbGvltrsArDOztZgcVLcXwBeq2mMhxxy6ZFhhOCyw1nVuqzAxwpdkjkqGfXv28fbloDxaICWs6LiEJSj+EimP88dWmMFLWExUFi4Hx4Uet2DbYeksfL3xsiJ7TYRlP9Y38rCq+Wv8c5i+Gktr6iLSXnQFnUgilOwiiVCyiyRCyS6SCCW7SCKU7CKJaP5U0nmQ+qIHowLZsEAgric3sk6fq44etS8H6xpHUx6TJbYBhFODl8fIeIhgOubocYfTObPnPJrSPO9U0dFxI9ebeHGcNqVDwYvZj0tndpFEKNlFEqFkF0mEkl0kEUp2kUQo2UUSoWQXSUQ4lXRdd2Z2EMDUwdsLAPDB4K3Trn1r134B6lut6tm35e6+cLpAU5P9Qzs3G3T3gZZ1gGjXvrVrvwD1rVbN6ps+xoskQskukohWJ/uGFu+fade+tWu/APWtVk3pW0u/s4tI87T6zC4iTaJkF0lES5LdzK43s11mttvM7mxFH7KY2V4z22ZmW81ssMV9edDMRsxs+5Tb+szsGTN7rfJz2jX2WtS3u83s7cqx22pmN7aob/1m9lMz22lmO8zsjsrtLT12pF9NOW5N/85uZh0AXgXwCQD7AWwGsM7df97UjmQws70ABty95RdgmNnVAEYBPOzuF1Zu+2sAR9z9nsob5Tx3/0qb9O1uAKOtXsa7slrRkqnLjAO4GcAfoYXHjvTrd9GE49aKM/tlAHa7+x53HwfwAwA3taAfbc/dnwVw5AM33wRgY+X3jZh8sTRdRt/agrsPufuWyu8nALy/zHhLjx3pV1O0ItmXAnhryv/3o73We3cAT5vZi2a2vtWdmcZidx8CJl88ABa1uD8fFC7j3UwfWGa8bY5dLcuf59WKZJ9ukqx2qv9d5e6XArgBwBcrH1elOlUt490s0ywz3hZqXf48r1Yk+34A/VP+vwzAgRb0Y1rufqDycwTA42i/paiH319Bt/JzpMX9+X/ttIz3dMuMow2OXSuXP29Fsm8GsNrMVprZDACfA/BkC/rxIWbWW/nDCcysF8An0X5LUT8J4JbK77cAeKKFffkF7bKMd9Yy42jxsWv58ufu3vR/AG7E5F/kXwfwtVb0IaNf5wB4qfJvR6v7BuBRTH6sK2LyE9GtAOYD2ATgtcrPvjbq2z8B2AbgZUwm1pIW9e1jmPxq+DKArZV/N7b62JF+NeW46XJZkUToCjqRRCjZRRKhZBdJhJJdJBFKdpFEKNlFEqFkF0nE/wETAyVuGpsoiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-LIIBisa-Mkh",
    "outputId": "aa584c54-1643-48af-eb35-d853835f2d2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8, 8, ..., 6, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred,axis=-1)\n",
    "df_sub = pd.read_csv('submission.csv',index_col=0)\n",
    "df_sub['digit'] = y_pred\n",
    "df_sub.to_csv('test_4.csv')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT0ZAdKJ-Mkh",
    "outputId": "4f04bdd5-32cc-4226-9a6a-19a6aa77a588"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 8, ..., 6, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TjhvZQW-Mkh",
    "outputId": "4b83ad9f-6588-4c0e-c152-b4310a2c4f32"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUzElEQVR4nO3da4yc1XkH8P9/9mp2fbfXbGxzM04TenOSjQuCtlSokSFSDKpSxVUjIqE4UkMVqvSCaKXwoapo2gTlQ0RlihWnSklQA4JIpMWlSCiKcL12Ddh1AQPG+IKNsc36gr2Xefphh2oD+z7PMGdm3qHn/5NWuztnznnPvDvPzuw+73MOzQwi8v9fpewJiEh7KNhFMqFgF8mEgl0kEwp2kUx0t/Ngvey3fg449wgyA14zG5nRBxkgJWuROnbQ38uohOclYey6xvfGTugLAEyce1ljh+M3HgfncRbjdmHWwZOCneQ6AN8B0AXgH83sHu/+/RzA1T3riu9gVfd4NjlZPJfuxN9bDN7kBHNr6dhBf5ucKO7a1ZU29tRU0L3xaLdqWsBEj80mxosbK37fpLFTx4/iwDlv26aeKJ6SO6qDZBeA7wK4EcBVADaQvKrR8USktVL+Zl8LYJ+ZvWJm4wB+CGB9c6YlIs2WEuzLAbw+4/uDtdt+AcmNJEdJjk7Y+YTDiUiKlGCf7Y+19/0xYWabzGzEzEZ62J9wOBFJkRLsBwGsnPH9CgCH06YjIq2SEuzbAawmeTnJXgBfAPBYc6YlIs3WcL7KzCZJ3g7g3zCdettsZntSJuOl1lrNS18B8FNUVT89FeVs2d3jtqfMLUyddbf4uipnbgyygmEKKnpsPb3FfaPUWZiyTLuwI5q7e2hvbtXieSUlp83scQCPp4whIu2hy2VFMqFgF8mEgl0kEwp2kUwo2EUyoWAXyURb69lh5uY3U8pUoxy9l3OtDeD37y7ObYaFmkEePsr5xnN38tEMcviJpZphmao55bfR9QXB2GH5bkJZcnRtQ2rpcMqxzXs6OXX2emUXyYSCXSQTCnaRTCjYRTKhYBfJhIJdJBPtTb0BbionLscsTtVEabswvZXQP0wZVoKxg7RhWA7ppJiiMtLwcUcpzeixO6m/sHQ3YBONlxanPu6U1Nr0AZyfWZCSdDklrnplF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTLQ3z066pYFu6V4Tju0Jc9lO/5TrAwDEO36GO6U65zQ1Tx4Iy1Ar3jUAQflswnLL0wMUzy112fLUawTcLZ8Td5gtHLahXiLyoaNgF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTba5nt6D2uvG8a5iL9vKaQJjbdCUuFR1eA1Bt/HdytAz1vr/5VMNjA8CVd+1w2718dHT9QbSddHheWyh1KemUPL17/YHzPE8KdpL7AZwGMAVg0sxGUsYTkdZpxiv775jZ8SaMIyItpL/ZRTKRGuwG4AmSO0hunO0OJDeSHCU5OmEXEg8nIo1KfRt/rZkdJjkEYCvJ/zGzp2fewcw2AdgEAPMqi8Jt0USkNZJe2c3scO3zMQCPAFjbjEmJSPM1HOwkB0jOffdrAJ8BsLtZExOR5kp5G78MwCOczhF3A/hnM/vXlMmkrI8e5aLj9c0T3uRE1wdEOdXg2NE1BPv+tjjj2XXe79sV/BulMun3r669ym3nz58tbAvPS3BtRHQNQVIePtqqOqqHj67bcH7m8foFDmeX6oaD3cxeAfDrjfYXkfZS6k0kEwp2kUwo2EUyoWAXyYSCXSQTbS5xZZBmipZzdvoGZaboSkvTJC25HKSQXv6mfy3S1CI/RdX9VnGqZuB1tysm5vppnkq0c/GUk+sB0L38I4VtduaM27d69h23PUrdeam5OO3nP66kkuhAUlrPW6G6semIyIeNgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTHy4lpL2cqNRSWLClsyAn2c/u+7X3L6vf9bPs/cv8PPNX/7YM277P2z/7cK2sS7/R2yVoIw0SDe/duOg2z73tYHCtqGfvur2tVNv+wdPEG6jHeTZw62wo/Jb7/kYPBfDawAK6JVdJBMKdpFMKNhFMqFgF8mEgl0kEwp2kUwo2EUy0d48u/n5ySg32bLldwEAfp6+smRxYVu1yz/2pZcec9s/tfiA237NwEtu+/391xa2WZBn95YeBoBqj5+Hn+r3288uLz43Y1df6vYd+EnafqEp20VbNW2L7/C6Dk/K0uKqZxcRBbtIJhTsIplQsItkQsEukgkFu0gmFOwimWhzPXuaqN7d7dvr51Ur8+a67UdvvKSwbewK/9gblvl58lvm7XTbV3T7tdM3XPlCYdu/H/U32u05Hfy+p59vvjDk55MHPzZW2HZkwQK376pHg1x3xFmvP3WfgOR69pRrALx1HVLy7CQ3kzxGcveM2xaR3ErypdrnhdE4IlKuet7Gfw/AuvfcdieAJ81sNYAna9+LSAcLg93MngZw4j03rwewpfb1FgA3N3daItJsjf6DbpmZHQGA2uehojuS3EhylOToBC40eDgRSdXy/8ab2SYzGzGzkR70tfpwIlKg0WA/SnIYAGqf/bIuESldo8H+GIBba1/fCuDR5kxHRFolzLOTfBDA9QCWkDwI4BsA7gHwEMnbABwA8PlmTCbcb9vJs0f1x11z/ezghV8q3kcc8OuyJy/yi8KfeuOjbvtQT3EuGgB+td/fZP21M4sK2/qO+7/P57/qz31ijl+rP36Zn2++fOFbhW27+ua7fVOF6yM4wv3bo/5BHt/fYz1YZCCody8SBruZbShouqGhI4pIKXS5rEgmFOwimVCwi2RCwS6SCQW7SCbaX+KakHLwygq7lhVesQsAeONzfh3qO8uCFNOC4rlVxv2+x58edtvvXfhZt706FKRxThanmAbP+10jE4PR9sF+87OvrShsW/RfwZLJCSXNABre2ni6b/DAoi2+w+3HnZRlxQ9Ldylp5yHrlV0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTCjYRTLR3jw7/RyhVaPfPcVJxOqKpW7PMI8+L8irOvnLntP+2MPP+MtxdZ/28+jH1wy67Us3by9si7YOjvLBx/5uxG2vHPNXH1q6s/i8zvvRf7p9w4Wkq8G2yF4uPMijpywFPT1A8Fz2rjcJ+rrls9qyWUQU7CKZULCLZELBLpIJBbtIJhTsIplQsItkos317HRziKwE9exOHv7cygG37/h8f+yqv0suek8VH3v+K/7YfTv2ue024S/HvGTUL0o3L9/s5XOBMKd75Z8843eP8tFenj+l3rweTi492pI5FOXCg+sb3GsEvHr1BHplF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTLQ3z27m1gFHtdXsKZ6uRbnJqDkqZ+8pvsOp1UHO9XNXue0956Jtk/3xT/5ycdvkHP+Brb5jm9se2XfPJ932VX/qjB/kqt310evg5rpTasYR5+mjLcS9/uEaBN6xveXo3VEBkNxM8hjJ3TNuu5vkIZK7ah83ReOISLnqeRv/PQDrZrn9XjNbU/t4vLnTEpFmC4PdzJ4GcKINcxGRFkr5B93tJJ+rvc1fWHQnkhtJjpIcnYC/FpuItE6jwX4fgFUA1gA4AuBbRXc0s01mNmJmIz3wFycUkdZpKNjN7KiZTZlZFcD9ANY2d1oi0mwNBTvJmXsQ3wJgd9F9RaQzhHl2kg8CuB7AEpIHAXwDwPUk12B6ler9AL7SjMm4e1YDqDj5xcp4kCi3tJzt1MrimvK//o2H3b5HJxa47bvPfsRtf/O8v278yUPF/Xv2XeT2PfTn17jtZy/3fyYDQ6fd9he/++nCtkt/4v/MBp4/7LaHnHy1jft5dFs+5I9d8V8nK/sPue1Tb485B/fPixsnTtcw2M1swyw3PxD1E5HOostlRTKhYBfJhIJdJBMKdpFMKNhFMtHmpaThlxaaX9pXHS8uj+076adSKpNz3PbJXr/MdHjp24Vtv9nvp1n65xxx208N7nHbX5mc77b/1dmbC9vGzvlLbFf8zBp6FvjLWP/eql1u+8sXF2+lvW3s427focEVbntUllyZKL5D3yl/y+WxS/rddgteJpeMByd27ExxW5Ql9pbg1pbNIqJgF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQT7c+zp2wv7PTtftHPdc97+Uq3/e0r/d97f3DJ9sK2oS6/jLQrWLZ4YfCw36z6ue43T8wrbOMCPxk9udDPB396hX9e/2zxDre9srj4sX/pOv+B7+hd7Y896Sek6Ty07nf8p37XO24z+t4OkvwM5uYtix6U37Lb2V98ovi4emUXyYSCXSQTCnaRTCjYRTKhYBfJhIJdJBMKdpFMtD/PHuXSPU7u0s6edbsOHvTzyac+6p+K1b1vFLYdmDzn9j1R7XXbt79zudu+ad91bnv1reLxgxWPUbnIPy9DfU7dNYA+OjlfAGeqxVt+jY37NeO9Y8GWzkHJOJ1LOvpP+HnywcP+2gpz3ggS8YePuc1uLj3aTtrZ9txbhlqv7CKZULCLZELBLpIJBbtIJhTsIplQsItkQsEukon25tlJsMvJs3vrYcNfVj6qAb7ogLNFLoAFLyxy2/9o2x8Wtk2N+bnmiw74p7nvpJ/zrfhLnGN4rPi8nV/o9z055NddP3fC305649T1bvurY4sL2848NOz2vWLr6247qkFNebX4vNg5P0/ubqkMwILn6lSw7TKd7cejPLu7/XjKuvEkV5J8iuRekntIfq12+yKSW0m+VPscPK1EpEz1vI2fBPB1M/s4gKsBfJXkVQDuBPCkma0G8GTtexHpUGGwm9kRM9tZ+/o0gL0AlgNYD2BL7W5bANzcojmKSBN8oL/ZSV4G4BMAtgFYZmZHgOlfCCSHCvpsBLARAPrhr9UmIq1T93/jSQ4C+DGAO8zM/+/FDGa2ycxGzGykh37hg4i0Tl3BTrIH04H+AzN7uHbzUZLDtfZhAH6Zj4iUKnwbT5IAHgCw18y+PaPpMQC3Arin9vnR+HDmptds0q9Z9NIVUd+pvfvc9sV7/JLGxQ8UpzuO/vE1bt/h/zjhtvN8cRkoANigv930uZWDhW3VHj+1Vjnul98eOrnMbT885bdf/PPiXNDSnz7r9p0MzkuUqnWXXI76VqJ9k/1S7ej56KXXbMp/Lrrpa2e/53r+Zr8WwBcBPE9yV+22uzAd5A+RvA3AAQCfr2MsESlJGOxm9jMU/7q4obnTEZFW0eWyIplQsItkQsEukgkFu0gmFOwimWhviasB5pUlBtvcen3dkkHUkfcMeLnNi+8bdftWvaV/AXf5XwAY23C12z7/X3YWts0J8slDUU7Xy1Unis5LeGyv1BP+kst+rhppyzkjfj56ef5wbg3SK7tIJhTsIplQsItkQsEukgkFu0gmFOwimVCwi2SizUtJB3XCwfa/Xm7TW2YaQLhVdFS/7OXp2ePXhKfmdOf9aLvb7mXpo8cV5bKj2mpUozy98xQLri8IlxaPlpJOEOXRw7mH4ydc9+Fdj6Itm0VEwS6SCQW7SCYU7CKZULCLZELBLpIJBbtIJtpfz+7lbS3IPQa58hThWt3emvUT/nbRUR4+FOSb/a7+73NGpzQljw7/vIY139HWxdXWnffwvARSrgEIz4t74OImvbKLZELBLpIJBbtIJhTsIplQsItkQsEukgkFu0gm6tmffSWA7wO4GEAVwCYz+w7JuwF8GcCbtbveZWaPp0wmJe8a5rqjfHCQjw7ruhNEcw+vL2jlGuTRWv7RefF+ZlEuOjWP7pyX+NjR40pbJwDOzyWspfeuP3AeVj3Z+0kAXzeznSTnAthBcmut7V4z+/s6xhCRktWzP/sRAEdqX58muRfA8lZPTESa6wP9zU7yMgCfALCtdtPtJJ8juZnkwoI+G0mOkhydwIW02YpIw+oOdpKDAH4M4A4zGwNwH4BVANZg+pX/W7P1M7NNZjZiZiM96EufsYg0pK5gJ9mD6UD/gZk9DABmdtTMpsysCuB+AGtbN00RSRUGO0kCeADAXjP79ozbh2fc7RYAu5s/PRFplnr+G38tgC8CeJ7krtptdwHYQHINpv/Zvx/AV+o6YkIqxis7TCm1nB4gKgVtXXltKCpxjUpBvaGTt00Olnv2znt0zqOfaZSybHDJZSBO68VLTQfnxVtKOkrrecuDO4et57/xPwMw2+hJOXURaS9dQSeSCQW7SCYU7CKZULCLZELBLpIJBbtIJtq7lDSQtiyym7Jt8dbE7uBBeWyYy068RsArxwyuD4iuH0jdstkrz41+Zsl5+IRttiPJz6eWnZfivnplF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTNCCut6mHox8E8BrM25aAuB42ybwwXTq3Dp1XoDm1qhmzu1SM1s6W0Nbg/19BydHzWyktAk4OnVunTovQHNrVLvmprfxIplQsItkouxg31Ty8T2dOrdOnReguTWqLXMr9W92EWmfsl/ZRaRNFOwimSgl2EmuI/kCyX0k7yxjDkVI7if5PMldJEdLnstmksdI7p5x2yKSW0m+VPs86x57Jc3tbpKHauduF8mbSprbSpJPkdxLcg/Jr9VuL/XcOfNqy3lr+9/sJLsAvAjgdwEcBLAdwAYz+++2TqQAyf0ARsys9AswSP4WgDMAvm9mv1K77ZsATpjZPbVflAvN7C86ZG53AzhT9jbetd2KhmduMw7gZgBfQonnzpnX76MN562MV/a1APaZ2StmNg7ghwDWlzCPjmdmTwM48Z6b1wPYUvt6C6afLG1XMLeOYGZHzGxn7evTAN7dZrzUc+fMqy3KCPblAF6f8f1BdNZ+7wbgCZI7SG4sezKzWGZmR4DpJw+AoZLn817hNt7t9J5txjvm3DWy/XmqMoJ9tkWyOin/d62ZfRLAjQC+Wnu7KvWpaxvvdpllm/GO0Oj256nKCPaDAFbO+H4FgMMlzGNWZna49vkYgEfQeVtRH313B93a52Mlz+f/dNI23rNtM44OOHdlbn9eRrBvB7Ca5OUkewF8AcBjJczjfUgO1P5xApIDAD6DztuK+jEAt9a+vhXAoyXO5Rd0yjbeRduMo+RzV/r252bW9g8AN2H6P/IvA/jLMuZQMK8rADxb+9hT9twAPIjpt3UTmH5HdBuAxQCeBPBS7fOiDprbPwF4HsBzmA6s4ZLmdh2m/zR8DsCu2sdNZZ87Z15tOW+6XFYkE7qCTiQTCnaRTCjYRTKhYBfJhIJdJBMKdpFMKNhFMvG/kr0K5rmdSfAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=5\n",
    "df = pd.read_csv(\"preprocessing_150.csv\",index_col=[0])\n",
    "df2 = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "plt.imshow(df.values[k,2:].reshape(28,28).astype('float32'))\n",
    "plt.show()\n",
    "df2 = pd.read_csv(\"train.csv\",index_col=[0])\n",
    "plt.imshow(df2.values[k,2:].reshape(28,28).astype('float32'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHKAVNiD-Mkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7nEeGDn-Mkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc_-Flz2-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wImlSOL--Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj2wn75b-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7YkDa_J-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6a0jyE7-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5jYY0jU-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4NiWL-c-Mki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzhsBOPW-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDPP4Lp2-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wsmnbLS-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4vmV_gu-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxBZ_-66-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBoKPs_E-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwaS-Ang-Mkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5_Acvtq-Mkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7pmh6Ej-Mkk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
