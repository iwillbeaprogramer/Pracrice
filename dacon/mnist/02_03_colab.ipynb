{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"02_03_colab.ipynb","provenance":[{"file_id":"1Q_J3eve2qjICe0sZy7NOfwys5zAlmLMm","timestamp":1612323545073}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HCoSxipm-x0V","executionInfo":{"status":"ok","timestamp":1612426259594,"user_tz":-540,"elapsed":15303,"user":{"displayName":"‍김수현[학생](응용과학대학 응용수학과)","photoUrl":"","userId":"14006156848407236102"}},"outputId":"85e9cd1d-4c2f-4786-812a-a6d83a63583b"},"source":["import os\r\n","from google.colab import drive\r\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ooCpvRmT-MkR"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dropout,Input,Activation,Dense\n","from tensorflow.keras.models import Sequential,Model,load_model\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import OneHotEncoder\n","from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import StratifiedKFold"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GysUTZTp-MkX"},"source":["# 모델링"]},{"cell_type":"code","metadata":{"id":"WekXPQ2HF8pY"},"source":["from keras import models, layers\r\n","from keras import Input\r\n","from keras.models import Model, load_model\r\n","from keras.preprocessing.image import ImageDataGenerator\r\n","from keras import optimizers, initializers, regularizers, metrics\r\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\r\n","from keras.layers import BatchNormalization, Conv2D, Activation, Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add\r\n"," \r\n","import os\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import math\r\n"," \r\n"," \r\n"," \r\n"," \r\n"," \r\n","# train_datagen = ImageDataGenerator(rescale=1./255)\r\n","# val_datagen = ImageDataGenerator(rescale=1./255)\r\n"," \r\n"," \r\n","# train_dir = os.path.join('./dataset/1/images/train')\r\n","# val_dir = os.path.join('./dataset/1/images/val')\r\n"," \r\n"," \r\n"," \r\n","# train_generator = train_datagen.flow_from_directory(train_dir, batch_size=16, target_size=(224, 224), color_mode='rgb')\r\n","# val_generator = val_datagen.flow_from_directory(val_dir, batch_size=16, target_size=(224, 224), color_mode='rgb')\r\n"," \r\n"," \r\n","# number of classes\r\n","K = 10\r\n"," \r\n"," \r\n","input_tensor = Input(shape=(28, 28, 1))\r\n"," \r\n"," \r\n","def conv1_layer(x):    \r\n","    x = ZeroPadding2D(padding=(3, 3))(x)\r\n","    x = Conv2D(64, (7, 7), strides=(2, 2))(x)\r\n","    x = BatchNormalization()(x)\r\n","    x = Activation('relu')(x)\r\n","    x = ZeroPadding2D(padding=(1,1))(x)\r\n"," \r\n","    return x   \r\n"," \r\n","    \r\n"," \r\n","def conv2_layer(x):         \r\n","    x = MaxPooling2D((3, 3), 2)(x)     \r\n"," \r\n","    shortcut = x\r\n"," \r\n","    for i in range(3):\r\n","        if (i == 0):\r\n","            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n","            \r\n","            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n"," \r\n","            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            shortcut = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(shortcut)            \r\n","            x = BatchNormalization()(x)\r\n","            shortcut = BatchNormalization()(shortcut)\r\n"," \r\n","            x = Add()([x, shortcut])\r\n","            x = Activation('relu')(x)\r\n","            \r\n","            shortcut = x\r\n"," \r\n","        else:\r\n","            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n","            \r\n","            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n"," \r\n","            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)            \r\n"," \r\n","            x = Add()([x, shortcut])   \r\n","            x = Activation('relu')(x)  \r\n"," \r\n","            shortcut = x        \r\n","    \r\n","    return x\r\n"," \r\n"," \r\n"," \r\n","def conv3_layer(x):        \r\n","    shortcut = x    \r\n","    \r\n","    for i in range(4):     \r\n","        if(i == 0):            \r\n","            x = Conv2D(128, (1, 1), strides=(2, 2), padding='valid')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)        \r\n","            \r\n","            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)  \r\n"," \r\n","            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            shortcut = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(shortcut)\r\n","            x = BatchNormalization()(x)\r\n","            shortcut = BatchNormalization()(shortcut)            \r\n"," \r\n","            x = Add()([x, shortcut])    \r\n","            x = Activation('relu')(x)    \r\n"," \r\n","            shortcut = x              \r\n","        \r\n","        else:\r\n","            x = Conv2D(128, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n","            \r\n","            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n"," \r\n","            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)            \r\n"," \r\n","            x = Add()([x, shortcut])     \r\n","            x = Activation('relu')(x)\r\n"," \r\n","            shortcut = x      \r\n","            \r\n","    return x\r\n"," \r\n"," \r\n"," \r\n","def conv4_layer(x):\r\n","    shortcut = x        \r\n","  \r\n","    for i in range(6):     \r\n","        if(i == 0):            \r\n","            x = Conv2D(256, (1, 1), strides=(2, 2), padding='valid')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)        \r\n","            \r\n","            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)  \r\n"," \r\n","            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            shortcut = Conv2D(1024, (1, 1), strides=(2, 2), padding='valid')(shortcut)\r\n","            x = BatchNormalization()(x)\r\n","            shortcut = BatchNormalization()(shortcut)\r\n"," \r\n","            x = Add()([x, shortcut]) \r\n","            x = Activation('relu')(x)\r\n"," \r\n","            shortcut = x               \r\n","        \r\n","        else:\r\n","            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n","            \r\n","            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n"," \r\n","            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)            \r\n"," \r\n","            x = Add()([x, shortcut])    \r\n","            x = Activation('relu')(x)\r\n"," \r\n","            shortcut = x      \r\n"," \r\n","    return x\r\n"," \r\n"," \r\n"," \r\n","def conv5_layer(x):\r\n","    shortcut = x    \r\n","  \r\n","    for i in range(3):     \r\n","        if(i == 0):            \r\n","            x = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)        \r\n","            \r\n","            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)  \r\n"," \r\n","            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            shortcut = Conv2D(2048, (1, 1), strides=(2, 2), padding='valid')(shortcut)\r\n","            x = BatchNormalization()(x)\r\n","            shortcut = BatchNormalization()(shortcut)            \r\n"," \r\n","            x = Add()([x, shortcut])  \r\n","            x = Activation('relu')(x)      \r\n"," \r\n","            shortcut = x               \r\n","        \r\n","        else:\r\n","            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n","            \r\n","            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\r\n","            x = BatchNormalization()(x)\r\n","            x = Activation('relu')(x)\r\n"," \r\n","            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\r\n","            x = BatchNormalization()(x)           \r\n","            \r\n","            x = Add()([x, shortcut]) \r\n","            x = Activation('relu')(x)       \r\n"," \r\n","            shortcut = x                  \r\n"," \r\n","    return x\r\n"," \r\n"," \r\n","x = conv1_layer(input_tensor)\r\n","x = conv2_layer(x)\r\n","x = conv3_layer(x)\r\n","x = conv4_layer(x)\r\n","x = conv5_layer(x)\r\n"," \r\n","x = GlobalAveragePooling2D()(x)\r\n","output_tensor = Dense(10, activation='softmax')(x)\r\n"," \r\n","model = Model(input_tensor, output_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4kX312e-MkY"},"source":["def modeling():\n","    inputs = Input(shape=(28,28,1))\n","    x = inputs\n","    _x = Conv2D(128,3,padding='same')(x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(256,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(512,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(1024,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(1024,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(128,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    x = _x\n","    _x = Conv2D(128,3,padding='same')(x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(256,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(512,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(1024,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(1024,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(128,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    x = x+_x\n","    x = MaxPooling2D(2)(x)\n","    _x = Conv2D(128,3,padding='same')(x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(256,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(512,3,padding='same')(x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(1024,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(1024,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(128,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    x = x+_x\n","    x = MaxPooling2D(2)(x)\n","    _x = Conv2D(128,3,padding='same')(x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(256,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(512,3,padding='same')(x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(1024,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(1024,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(128,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    x = x+_x\n","    x = MaxPooling2D(2)(x)\n","    _x = Conv2D(512,3,padding='same')(x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(128,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    _x = Conv2D(128,3,padding='same')(_x)\n","    _x = BatchNormalization()(_x)\n","    _x = Activation('relu')(_x)\n","    x = x+_x\n","    x = MaxPooling2D(2)(x)\n","    x = Flatten()(x)\n","    x = Dense(2048)(x)\n","    x = Dense(10,activation='softmax')(x)\n","    outputs=x\n","    model = Model(inputs=inputs,outputs=outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cccyJfWO-MkY"},"source":["# 하이퍼 파라미터"]},{"cell_type":"code","metadata":{"id":"W73nGvZM-MkZ"},"source":["epochs = 2000\n","es = EarlyStopping(monitor='val_loss',patience=160)\n","reLR = ReduceLROnPlateau(patience=100,verbose=1,factor=0.5)\n","kfold = StratifiedKFold(n_splits=4,random_state=42,shuffle=True)\n","\n","datagen = ImageDataGenerator(height_shift_range=(-1,1),width_shift_range=(-1,1))\n","datagen2 = ImageDataGenerator()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWqLv-B1-MkZ"},"source":["# 데이터 불러오기 및 전처리"]},{"cell_type":"code","metadata":{"id":"j6UbQKZk-MkZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612427365984,"user_tz":-540,"elapsed":833,"user":{"displayName":"‍김수현[학생](응용과학대학 응용수학과)","photoUrl":"","userId":"14006156848407236102"}},"outputId":"c883dad9-2c4f-46f5-8385-b50a0a538bc8"},"source":["df = pd.read_csv(\"/content/drive/My Drive/Study/dacon/mnist/train.csv\",index_col=[0])\n","y = df.values[:,0].astype('int32')\n","x = df.values[:,2:].astype('float32')/255.0\n","# print(x.shape,y.shape)               # (2048, 28, 28) (2048,)\n","#onehot = OneHotEncoder()\n","#y = onehot.fit_transform(y.reshape(-1,1)).toarray().astype('float32')\n","x = x.reshape(-1,28,28,1)\n","# x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.15)\n","# x_train = x_train.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n","# x_val = x_val.reshape(-1,28,28,1)#[:,2:26,2:26,:]\n","# print(x_train.shape,x_val.shape,y_train.shape,y_val.shape)\n","print(x.shape,y.shape) \n","y"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(2048, 28, 28, 1) (2048,)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([5, 0, 4, ..., 9, 0, 5], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"LmMESLTT-Mkb","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1612428572841,"user_tz":-540,"elapsed":1206331,"user":{"displayName":"‍김수현[학생](응용과학대학 응용수학과)","photoUrl":"","userId":"14006156848407236102"}},"outputId":"d5cb030f-3799-42aa-ba1f-3957ec122aa8"},"source":["index=1\n","result = 0\n","for train_index,val_index in kfold.split(x,y):\n","    modelpath = '/content/drive/My Drive/Study/dacon/mnist/models/02_03_imger_best_index_{}_colab.h5'.format(index)\n","    cp = ModelCheckpoint(monitor = 'val_accuracy',filepath=modelpath,save_best_only=True)\n","    \n","    \n","    x_train = x[train_index]\n","    x_val = x[val_index]\n","    y_train = y[train_index]\n","    y_val = y[val_index]\n","    \n","    onehot = OneHotEncoder()\n","    y_train = onehot.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n","    y_val = onehot.fit_transform(y_val.reshape(-1,1)).toarray().astype('float32')\n","    \n","    train_generator = datagen.flow(x_train,y_train,batch_size=32)\n","    val_generator = datagen.flow(x_val,y_val)\n","    model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n","    model.fit_generator(train_generator,validation_data = val_generator,epochs=epochs,callbacks=[cp,es,reLR])\n","    \n","    model = load_model(modelpath)\n","    df = pd.read_csv(\"/content/drive/My Drive/Study/dacon/mnist/test.csv\",index_col=[0])\n","    x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0\n","    y_pred = model.predict(x_test)\n","    y_pred = np.argmax(y_pred,axis=-1)\n","    df_sub = pd.read_csv(\"/content/drive/My Drive/Study/dacon/mnist/submission.csv\",index_col=0)\n","    df_sub['digit']=y_pred\n","    df_sub.to_csv(\"kfold_colab_{}.csv\".format(index))\n","    print(index, \" 번째 학습을 완료했습니다.\")\n","    index+=1\n","    \n","\n","    \n","\"\"\"\n","0.905\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/2000\n","48/48 [==============================] - 15s 64ms/step - loss: 4.3091 - accuracy: 0.1231 - val_loss: 2.3430 - val_accuracy: 0.0977\n","Epoch 2/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 2.4230 - accuracy: 0.2106 - val_loss: 2.5851 - val_accuracy: 0.0938\n","Epoch 3/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 2.2740 - accuracy: 0.2821 - val_loss: 2.7658 - val_accuracy: 0.0938\n","Epoch 4/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 2.3005 - accuracy: 0.3693 - val_loss: 2.7063 - val_accuracy: 0.0938\n","Epoch 5/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 2.0377 - accuracy: 0.3921 - val_loss: 3.0597 - val_accuracy: 0.0938\n","Epoch 6/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 1.9994 - accuracy: 0.4438 - val_loss: 3.1798 - val_accuracy: 0.0938\n","Epoch 7/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 1.7478 - accuracy: 0.5042 - val_loss: 4.1358 - val_accuracy: 0.0938\n","Epoch 8/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 1.7627 - accuracy: 0.5561 - val_loss: 4.3318 - val_accuracy: 0.0957\n","Epoch 9/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.8578 - accuracy: 0.4424 - val_loss: 6.2513 - val_accuracy: 0.0977\n","Epoch 10/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.4797 - accuracy: 0.5307 - val_loss: 4.8498 - val_accuracy: 0.1211\n","Epoch 11/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 1.4775 - accuracy: 0.6076 - val_loss: 3.8214 - val_accuracy: 0.1602\n","Epoch 12/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.4152 - accuracy: 0.6106 - val_loss: 3.2243 - val_accuracy: 0.1836\n","Epoch 13/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.4246 - accuracy: 0.6474 - val_loss: 2.8145 - val_accuracy: 0.2559\n","Epoch 14/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.3492 - accuracy: 0.6394 - val_loss: 2.6594 - val_accuracy: 0.2949\n","Epoch 15/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.3096 - accuracy: 0.6302 - val_loss: 3.1444 - val_accuracy: 0.2871\n","Epoch 16/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.4009 - accuracy: 0.5999 - val_loss: 3.6301 - val_accuracy: 0.2148\n","Epoch 17/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 1.6089 - accuracy: 0.5529 - val_loss: 2.6872 - val_accuracy: 0.2949\n","Epoch 18/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.5434 - accuracy: 0.5329 - val_loss: 4.4328 - val_accuracy: 0.1660\n","Epoch 19/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 1.6797 - accuracy: 0.4835 - val_loss: 2.2078 - val_accuracy: 0.3457\n","Epoch 20/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.4502 - accuracy: 0.6140 - val_loss: 47.0422 - val_accuracy: 0.1797\n","Epoch 21/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.5788 - accuracy: 0.5251 - val_loss: 2.2623 - val_accuracy: 0.3066\n","Epoch 22/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.4866 - accuracy: 0.5600 - val_loss: 2.0366 - val_accuracy: 0.3613\n","Epoch 23/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.5678 - accuracy: 0.5725 - val_loss: 7.0301 - val_accuracy: 0.3457\n","Epoch 24/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.6143 - accuracy: 0.5874 - val_loss: 2.0750 - val_accuracy: 0.4375\n","Epoch 25/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.3474 - accuracy: 0.6456 - val_loss: 2.1473 - val_accuracy: 0.3516\n","Epoch 26/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.3150 - accuracy: 0.5844 - val_loss: 67.4475 - val_accuracy: 0.1816\n","Epoch 27/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.5959 - accuracy: 0.5098 - val_loss: 22.8030 - val_accuracy: 0.1895\n","Epoch 28/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 1.7431 - accuracy: 0.5026 - val_loss: 527.4458 - val_accuracy: 0.1191\n","Epoch 29/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 1.7639 - accuracy: 0.5307 - val_loss: 408.8030 - val_accuracy: 0.1621\n","Epoch 30/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.8821 - accuracy: 0.4998 - val_loss: 30.4960 - val_accuracy: 0.2930\n","Epoch 31/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.6518 - accuracy: 0.5467 - val_loss: 263.6819 - val_accuracy: 0.1895\n","Epoch 32/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 1.7953 - accuracy: 0.5129 - val_loss: 213.9283 - val_accuracy: 0.1836\n","Epoch 33/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 1.4329 - accuracy: 0.5879 - val_loss: 213.2380 - val_accuracy: 0.2441\n","Epoch 34/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.4889 - accuracy: 0.6124 - val_loss: 4.6865 - val_accuracy: 0.4570\n","Epoch 35/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.1869 - accuracy: 0.6659 - val_loss: 2.5988 - val_accuracy: 0.4082\n","Epoch 36/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.9234 - accuracy: 0.7191 - val_loss: 3.7233 - val_accuracy: 0.4785\n","Epoch 37/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.1625 - accuracy: 0.6971 - val_loss: 3.4468 - val_accuracy: 0.4043\n","Epoch 38/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.0001 - accuracy: 0.7254 - val_loss: 1.9799 - val_accuracy: 0.4707\n","Epoch 39/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.8536 - accuracy: 0.7559 - val_loss: 1.8300 - val_accuracy: 0.4668\n","Epoch 40/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.7631 - accuracy: 0.7626 - val_loss: 1.8257 - val_accuracy: 0.4883\n","Epoch 41/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.7766 - accuracy: 0.7576 - val_loss: 2.5759 - val_accuracy: 0.3984\n","Epoch 42/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 0.8844 - accuracy: 0.7426 - val_loss: 2.3921 - val_accuracy: 0.4414\n","Epoch 43/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.0455 - accuracy: 0.7113 - val_loss: 3.1017 - val_accuracy: 0.3457\n","Epoch 44/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 1.4123 - accuracy: 0.6103 - val_loss: 2.9392 - val_accuracy: 0.3008\n","Epoch 45/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 1.2881 - accuracy: 0.6333 - val_loss: 2.2286 - val_accuracy: 0.4043\n","Epoch 46/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 1.3021 - accuracy: 0.6387 - val_loss: 3.8900 - val_accuracy: 0.2930\n","Epoch 47/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.3173 - accuracy: 0.6372 - val_loss: 1.6142 - val_accuracy: 0.5176\n","Epoch 48/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.0050 - accuracy: 0.7081 - val_loss: 4.4340 - val_accuracy: 0.3633\n","Epoch 49/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.2934 - accuracy: 0.6671 - val_loss: 4.0686 - val_accuracy: 0.2129\n","Epoch 50/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.4713 - accuracy: 0.5848 - val_loss: 3.5072 - val_accuracy: 0.3438\n","Epoch 51/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 1.2152 - accuracy: 0.6559 - val_loss: 2.8554 - val_accuracy: 0.3867\n","Epoch 52/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.1160 - accuracy: 0.6732 - val_loss: 3.1438 - val_accuracy: 0.4082\n","Epoch 53/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.9241 - accuracy: 0.7576 - val_loss: 2.2170 - val_accuracy: 0.4590\n","Epoch 54/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.9541 - accuracy: 0.7180 - val_loss: 2.3849 - val_accuracy: 0.4180\n","Epoch 55/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.9144 - accuracy: 0.7052 - val_loss: 1.7403 - val_accuracy: 0.5117\n","Epoch 56/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 0.9244 - accuracy: 0.7453 - val_loss: 1.9988 - val_accuracy: 0.4102\n","Epoch 57/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.2189 - accuracy: 0.6543 - val_loss: 4.1325 - val_accuracy: 0.3535\n","Epoch 58/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.2492 - accuracy: 0.6631 - val_loss: 2.2280 - val_accuracy: 0.4258\n","Epoch 59/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.0112 - accuracy: 0.6889 - val_loss: 1.6220 - val_accuracy: 0.5020\n","Epoch 60/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.9915 - accuracy: 0.7377 - val_loss: 3.1683 - val_accuracy: 0.3984\n","Epoch 61/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.1738 - accuracy: 0.6913 - val_loss: 4.5926 - val_accuracy: 0.4629\n","Epoch 62/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.1971 - accuracy: 0.6370 - val_loss: 3.3314 - val_accuracy: 0.2070\n","Epoch 63/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 2.0145 - accuracy: 0.4242 - val_loss: 2.7307 - val_accuracy: 0.2500\n","Epoch 64/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.6335 - accuracy: 0.5150 - val_loss: 2.1400 - val_accuracy: 0.3945\n","Epoch 65/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.3443 - accuracy: 0.5991 - val_loss: 2.4254 - val_accuracy: 0.3652\n","Epoch 66/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.0761 - accuracy: 0.6480 - val_loss: 1.8305 - val_accuracy: 0.4980\n","Epoch 67/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.9853 - accuracy: 0.7189 - val_loss: 1.6909 - val_accuracy: 0.5098\n","Epoch 68/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.7796 - accuracy: 0.7449 - val_loss: 6.0605 - val_accuracy: 0.4082\n","Epoch 69/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.4818 - accuracy: 0.5302 - val_loss: 2.0745 - val_accuracy: 0.3906\n","Epoch 70/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.4414 - accuracy: 0.5677 - val_loss: 2.9403 - val_accuracy: 0.4258\n","Epoch 71/2000\n","48/48 [==============================] - 2s 43ms/step - loss: 1.2307 - accuracy: 0.6365 - val_loss: 1.8906 - val_accuracy: 0.4336\n","Epoch 72/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.9279 - accuracy: 0.7019 - val_loss: 1.7331 - val_accuracy: 0.4922\n","Epoch 73/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.8275 - accuracy: 0.7258 - val_loss: 2.4854 - val_accuracy: 0.4277\n","Epoch 74/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.0579 - accuracy: 0.6723 - val_loss: 21.4590 - val_accuracy: 0.2715\n","Epoch 75/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.7007 - accuracy: 0.5020 - val_loss: 13.9363 - val_accuracy: 0.3379\n","Epoch 76/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.1888 - accuracy: 0.6099 - val_loss: 6.9279 - val_accuracy: 0.3887\n","Epoch 77/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.0731 - accuracy: 0.6433 - val_loss: 1.7772 - val_accuracy: 0.4961\n","Epoch 78/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.7176 - accuracy: 0.7606 - val_loss: 1.6597 - val_accuracy: 0.4629\n","Epoch 79/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.8361 - accuracy: 0.7313 - val_loss: 1.6948 - val_accuracy: 0.4844\n","Epoch 80/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.7070 - accuracy: 0.7715 - val_loss: 1.4626 - val_accuracy: 0.5312\n","Epoch 81/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.6066 - accuracy: 0.8022 - val_loss: 1.6613 - val_accuracy: 0.4980\n","Epoch 82/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.8294 - accuracy: 0.7319 - val_loss: 1.7338 - val_accuracy: 0.5371\n","Epoch 83/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.6225 - accuracy: 0.8062 - val_loss: 2.3176 - val_accuracy: 0.4355\n","Epoch 84/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5610 - accuracy: 0.8428 - val_loss: 1.5198 - val_accuracy: 0.5508\n","Epoch 85/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.4442 - accuracy: 0.8660 - val_loss: 2.6943 - val_accuracy: 0.5254\n","Epoch 86/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5792 - accuracy: 0.8395 - val_loss: 1.6152 - val_accuracy: 0.5781\n","Epoch 87/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3751 - accuracy: 0.8801 - val_loss: 1.5426 - val_accuracy: 0.5840\n","Epoch 88/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.3156 - accuracy: 0.8897 - val_loss: 1.6244 - val_accuracy: 0.5859\n","Epoch 89/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.4796 - accuracy: 0.8621 - val_loss: 3.4138 - val_accuracy: 0.3047\n","Epoch 90/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.9200 - accuracy: 0.7485 - val_loss: 2.1078 - val_accuracy: 0.5117\n","Epoch 91/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.5653 - accuracy: 0.8323 - val_loss: 1.6474 - val_accuracy: 0.5391\n","Epoch 92/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.7869 - accuracy: 0.7549 - val_loss: 3.4857 - val_accuracy: 0.3809\n","Epoch 93/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.9420 - accuracy: 0.7183 - val_loss: 3.5032 - val_accuracy: 0.4570\n","Epoch 94/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.1768 - accuracy: 0.6278 - val_loss: 10.9418 - val_accuracy: 0.3457\n","Epoch 95/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.1055 - accuracy: 0.6771 - val_loss: 3.8505 - val_accuracy: 0.4785\n","Epoch 96/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.6262 - accuracy: 0.7855 - val_loss: 2.0233 - val_accuracy: 0.5508\n","Epoch 97/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.4598 - accuracy: 0.8476 - val_loss: 1.7613 - val_accuracy: 0.5449\n","Epoch 98/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.4229 - accuracy: 0.8523 - val_loss: 1.4402 - val_accuracy: 0.5859\n","Epoch 99/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3351 - accuracy: 0.9092 - val_loss: 1.8821 - val_accuracy: 0.5605\n","Epoch 100/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.3709 - accuracy: 0.8869 - val_loss: 1.6093 - val_accuracy: 0.5645\n","Epoch 101/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3312 - accuracy: 0.9028 - val_loss: 1.5802 - val_accuracy: 0.6055\n","Epoch 102/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1634 - accuracy: 0.9424 - val_loss: 1.4887 - val_accuracy: 0.5977\n","Epoch 103/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1706 - accuracy: 0.9498 - val_loss: 1.8728 - val_accuracy: 0.5840\n","Epoch 104/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2144 - accuracy: 0.9307 - val_loss: 1.8023 - val_accuracy: 0.5859\n","Epoch 105/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1939 - accuracy: 0.9324 - val_loss: 1.8578 - val_accuracy: 0.5957\n","Epoch 106/2000\n","48/48 [==============================] - 2s 50ms/step - loss: 0.2052 - accuracy: 0.9347 - val_loss: 1.6422 - val_accuracy: 0.6152\n","Epoch 107/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1977 - accuracy: 0.9424 - val_loss: 1.8414 - val_accuracy: 0.5938\n","Epoch 108/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1304 - accuracy: 0.9458 - val_loss: 1.6912 - val_accuracy: 0.6094\n","Epoch 109/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1095 - accuracy: 0.9646 - val_loss: 1.7914 - val_accuracy: 0.6113\n","Epoch 110/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1505 - accuracy: 0.9478 - val_loss: 2.0573 - val_accuracy: 0.5625\n","Epoch 111/2000\n","48/48 [==============================] - 2s 51ms/step - loss: 0.1581 - accuracy: 0.9515 - val_loss: 3.1986 - val_accuracy: 0.4980\n","Epoch 112/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4342 - accuracy: 0.8590 - val_loss: 1.8639 - val_accuracy: 0.5898\n","Epoch 113/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2349 - accuracy: 0.9253 - val_loss: 1.9125 - val_accuracy: 0.5801\n","Epoch 114/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1428 - accuracy: 0.9540 - val_loss: 2.0156 - val_accuracy: 0.5684\n","Epoch 115/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1154 - accuracy: 0.9571 - val_loss: 1.9234 - val_accuracy: 0.6152\n","Epoch 116/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.2806 - accuracy: 0.9308 - val_loss: 26.6476 - val_accuracy: 0.1953\n","Epoch 117/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.5959 - accuracy: 0.5738 - val_loss: 9.4144 - val_accuracy: 0.3203\n","Epoch 118/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.3443 - accuracy: 0.6732 - val_loss: 12.0844 - val_accuracy: 0.3750\n","Epoch 119/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.9743 - accuracy: 0.7414 - val_loss: 2.7311 - val_accuracy: 0.5293\n","Epoch 120/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.6281 - accuracy: 0.8289 - val_loss: 1.4422 - val_accuracy: 0.5898\n","Epoch 121/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5007 - accuracy: 0.8470 - val_loss: 1.7026 - val_accuracy: 0.5508\n","Epoch 122/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4393 - accuracy: 0.8761 - val_loss: 2.4172 - val_accuracy: 0.4688\n","Epoch 123/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5715 - accuracy: 0.8132 - val_loss: 1.5123 - val_accuracy: 0.6152\n","Epoch 124/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4787 - accuracy: 0.8528 - val_loss: 4.6138 - val_accuracy: 0.4805\n","Epoch 125/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.6168 - accuracy: 0.8553 - val_loss: 2.1560 - val_accuracy: 0.5391\n","Epoch 126/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3991 - accuracy: 0.8961 - val_loss: 1.8852 - val_accuracy: 0.5410\n","Epoch 127/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.3652 - accuracy: 0.8937 - val_loss: 1.7739 - val_accuracy: 0.5391\n","Epoch 128/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3138 - accuracy: 0.8905 - val_loss: 2.0989 - val_accuracy: 0.5605\n","Epoch 129/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4564 - accuracy: 0.8886 - val_loss: 2.5927 - val_accuracy: 0.4922\n","Epoch 130/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.4480 - accuracy: 0.8525 - val_loss: 7.5734 - val_accuracy: 0.3320\n","Epoch 131/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.8354 - accuracy: 0.7993 - val_loss: 3.7253 - val_accuracy: 0.3613\n","Epoch 132/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4856 - accuracy: 0.8599 - val_loss: 2.3009 - val_accuracy: 0.5020\n","Epoch 133/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.3856 - accuracy: 0.8768 - val_loss: 5.6527 - val_accuracy: 0.3965\n","Epoch 134/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.6190 - accuracy: 0.8063 - val_loss: 1.6242 - val_accuracy: 0.5840\n","Epoch 135/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5412 - accuracy: 0.8575 - val_loss: 3.0302 - val_accuracy: 0.5840\n","Epoch 136/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.3796 - accuracy: 0.9102 - val_loss: 1.5256 - val_accuracy: 0.6328\n","Epoch 137/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.0930 - accuracy: 0.7431 - val_loss: 271.3953 - val_accuracy: 0.1172\n","Epoch 138/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.1561 - accuracy: 0.6868 - val_loss: 2.9831 - val_accuracy: 0.4883\n","Epoch 139/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.6991 - accuracy: 0.8084 - val_loss: 4.7796 - val_accuracy: 0.4746\n","Epoch 140/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.7044 - accuracy: 0.8040 - val_loss: 2.0276 - val_accuracy: 0.4746\n","Epoch 141/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.6086 - accuracy: 0.8592 - val_loss: 1.7878 - val_accuracy: 0.5098\n","Epoch 142/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.9964 - accuracy: 0.7166 - val_loss: 45.8266 - val_accuracy: 0.2363\n","Epoch 143/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.2922 - accuracy: 0.6311 - val_loss: 130.5744 - val_accuracy: 0.2168\n","Epoch 144/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.1452 - accuracy: 0.7283 - val_loss: 43.0359 - val_accuracy: 0.3047\n","Epoch 145/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.8163 - accuracy: 0.8010 - val_loss: 2.1599 - val_accuracy: 0.4805\n","Epoch 146/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.5120 - accuracy: 0.8601 - val_loss: 1.5066 - val_accuracy: 0.5723\n","Epoch 147/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3954 - accuracy: 0.8996 - val_loss: 1.4892 - val_accuracy: 0.5469\n","Epoch 148/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2907 - accuracy: 0.9128 - val_loss: 2.0972 - val_accuracy: 0.4863\n","Epoch 149/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.4512 - accuracy: 0.8758 - val_loss: 1.8325 - val_accuracy: 0.5352\n","Epoch 150/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3099 - accuracy: 0.9083 - val_loss: 1.6875 - val_accuracy: 0.5723\n","Epoch 151/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3636 - accuracy: 0.8980 - val_loss: 1.9537 - val_accuracy: 0.5332\n","Epoch 152/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.6522 - accuracy: 0.8406 - val_loss: 130.5836 - val_accuracy: 0.1445\n","Epoch 153/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.0844 - accuracy: 0.6615 - val_loss: 3.5574 - val_accuracy: 0.4141\n","Epoch 154/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.7310 - accuracy: 0.7886 - val_loss: 1.6425 - val_accuracy: 0.5469\n","Epoch 155/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4625 - accuracy: 0.8656 - val_loss: 1.6215 - val_accuracy: 0.5566\n","Epoch 156/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.6489 - accuracy: 0.8241 - val_loss: 1.8050 - val_accuracy: 0.4961\n","Epoch 157/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.6116 - accuracy: 0.8350 - val_loss: 2.7973 - val_accuracy: 0.5078\n","Epoch 158/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3836 - accuracy: 0.8937 - val_loss: 1.5733 - val_accuracy: 0.5742\n","Epoch 159/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4217 - accuracy: 0.8837 - val_loss: 5.7619 - val_accuracy: 0.4160\n","Epoch 160/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.6945 - accuracy: 0.8149 - val_loss: 2.9925 - val_accuracy: 0.4707\n","Epoch 161/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4494 - accuracy: 0.8633 - val_loss: 1.5370 - val_accuracy: 0.6328\n","Epoch 162/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2688 - accuracy: 0.9206 - val_loss: 2.4580 - val_accuracy: 0.5293\n","Epoch 163/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3331 - accuracy: 0.8999 - val_loss: 1.7873 - val_accuracy: 0.6055\n","Epoch 164/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.2934 - accuracy: 0.9207 - val_loss: 1.6638 - val_accuracy: 0.6055\n","Epoch 165/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.3042 - accuracy: 0.9049 - val_loss: 1.7694 - val_accuracy: 0.6016\n","Epoch 166/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1876 - accuracy: 0.9330 - val_loss: 1.9740 - val_accuracy: 0.5508\n","Epoch 167/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3264 - accuracy: 0.9088 - val_loss: 2.2733 - val_accuracy: 0.4746\n","Epoch 168/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 2.4183 - accuracy: 0.3655 - val_loss: 67.1647 - val_accuracy: 0.1094\n","Epoch 169/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 2.0493 - accuracy: 0.3860 - val_loss: 12.6586 - val_accuracy: 0.1777\n","Epoch 170/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.8560 - accuracy: 0.4478 - val_loss: 3.9674 - val_accuracy: 0.2266\n","Epoch 171/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 1.6219 - accuracy: 0.5540 - val_loss: 2.0417 - val_accuracy: 0.3711\n","Epoch 172/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.2853 - accuracy: 0.5940 - val_loss: 1.6990 - val_accuracy: 0.4883\n","Epoch 173/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 1.1977 - accuracy: 0.6158 - val_loss: 1.5149 - val_accuracy: 0.4766\n","Epoch 174/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.0024 - accuracy: 0.6786 - val_loss: 1.3935 - val_accuracy: 0.5254\n","Epoch 175/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.4988 - accuracy: 0.5774 - val_loss: 16.7118 - val_accuracy: 0.2207\n","Epoch 176/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.3541 - accuracy: 0.5742 - val_loss: 2.3130 - val_accuracy: 0.4082\n","Epoch 177/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.2292 - accuracy: 0.6224 - val_loss: 1.6003 - val_accuracy: 0.5352\n","Epoch 178/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 1.0000 - accuracy: 0.6840 - val_loss: 1.6150 - val_accuracy: 0.4824\n","Epoch 179/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.8118 - accuracy: 0.7492 - val_loss: 1.4640 - val_accuracy: 0.5312\n","Epoch 180/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.8060 - accuracy: 0.7552 - val_loss: 1.5595 - val_accuracy: 0.5527\n","Epoch 181/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.6948 - accuracy: 0.7925 - val_loss: 1.9773 - val_accuracy: 0.5020\n","Epoch 182/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.7364 - accuracy: 0.7882 - val_loss: 1.4682 - val_accuracy: 0.5566\n","Epoch 183/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.6198 - accuracy: 0.8274 - val_loss: 1.5080 - val_accuracy: 0.5820\n","Epoch 184/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.4592 - accuracy: 0.8725 - val_loss: 1.6401 - val_accuracy: 0.5762\n","Epoch 185/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.6606 - accuracy: 0.8038 - val_loss: 1.6883 - val_accuracy: 0.5254\n","Epoch 186/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.5254 - accuracy: 0.8447 - val_loss: 1.6363 - val_accuracy: 0.5762\n","Epoch 187/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.3607 - accuracy: 0.8856 - val_loss: 1.3313 - val_accuracy: 0.5859\n","Epoch 188/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3074 - accuracy: 0.9103 - val_loss: 1.5143 - val_accuracy: 0.5957\n","Epoch 189/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.7369 - accuracy: 0.7889 - val_loss: 3.3726 - val_accuracy: 0.5273\n","Epoch 190/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5916 - accuracy: 0.7984 - val_loss: 1.7041 - val_accuracy: 0.5820\n","Epoch 191/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.4077 - accuracy: 0.8733 - val_loss: 1.7263 - val_accuracy: 0.5508\n","Epoch 192/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4196 - accuracy: 0.8610 - val_loss: 1.5243 - val_accuracy: 0.5781\n","Epoch 193/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.2514 - accuracy: 0.9263 - val_loss: 1.4730 - val_accuracy: 0.6406\n","Epoch 194/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.3120 - accuracy: 0.9150 - val_loss: 1.8292 - val_accuracy: 0.5938\n","Epoch 195/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2200 - accuracy: 0.9397 - val_loss: 1.8401 - val_accuracy: 0.5820\n","Epoch 196/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2009 - accuracy: 0.9364 - val_loss: 1.9514 - val_accuracy: 0.5996\n","Epoch 197/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2121 - accuracy: 0.9517 - val_loss: 2.0939 - val_accuracy: 0.5664\n","Epoch 198/2000\n","48/48 [==============================] - 2s 51ms/step - loss: 0.2284 - accuracy: 0.9462 - val_loss: 1.6073 - val_accuracy: 0.6680\n","Epoch 199/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.1780 - accuracy: 0.9400 - val_loss: 1.5540 - val_accuracy: 0.6504\n","Epoch 200/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1193 - accuracy: 0.9638 - val_loss: 1.6934 - val_accuracy: 0.6406\n","Epoch 201/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1512 - accuracy: 0.9534 - val_loss: 1.9847 - val_accuracy: 0.6055\n","Epoch 202/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.1899 - accuracy: 0.9533 - val_loss: 2.1006 - val_accuracy: 0.6348\n","Epoch 203/2000\n","48/48 [==============================] - 2s 50ms/step - loss: 0.1442 - accuracy: 0.9573 - val_loss: 2.4437 - val_accuracy: 0.5820\n","Epoch 204/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1214 - accuracy: 0.9617 - val_loss: 3.5087 - val_accuracy: 0.5312\n","Epoch 205/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3498 - accuracy: 0.9124 - val_loss: 2.7794 - val_accuracy: 0.5742\n","Epoch 206/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1413 - accuracy: 0.9520 - val_loss: 1.7774 - val_accuracy: 0.6328\n","Epoch 207/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.4298 - accuracy: 0.8941 - val_loss: 2.0619 - val_accuracy: 0.5762\n","Epoch 208/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.2772 - accuracy: 0.9177 - val_loss: 1.8020 - val_accuracy: 0.6328\n","Epoch 209/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1425 - accuracy: 0.9563 - val_loss: 1.7732 - val_accuracy: 0.6270\n","Epoch 210/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1280 - accuracy: 0.9664 - val_loss: 1.6642 - val_accuracy: 0.6367\n","Epoch 211/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1043 - accuracy: 0.9665 - val_loss: 1.6361 - val_accuracy: 0.6504\n","Epoch 212/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0737 - accuracy: 0.9713 - val_loss: 1.6286 - val_accuracy: 0.6328\n","Epoch 213/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1697 - accuracy: 0.9559 - val_loss: 1.8817 - val_accuracy: 0.6074\n","Epoch 214/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1188 - accuracy: 0.9559 - val_loss: 1.8482 - val_accuracy: 0.6172\n","Epoch 215/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0645 - accuracy: 0.9801 - val_loss: 1.6734 - val_accuracy: 0.6641\n","Epoch 216/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0519 - accuracy: 0.9837 - val_loss: 1.7255 - val_accuracy: 0.6367\n","Epoch 217/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0826 - accuracy: 0.9702 - val_loss: 1.8091 - val_accuracy: 0.6230\n","Epoch 218/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0584 - accuracy: 0.9802 - val_loss: 1.8108 - val_accuracy: 0.6270\n","Epoch 219/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0851 - accuracy: 0.9791 - val_loss: 2.0194 - val_accuracy: 0.6152\n","Epoch 220/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0676 - accuracy: 0.9762 - val_loss: 2.0643 - val_accuracy: 0.6191\n","Epoch 221/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0851 - accuracy: 0.9737 - val_loss: 2.0475 - val_accuracy: 0.6094\n","Epoch 222/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0604 - accuracy: 0.9818 - val_loss: 1.9024 - val_accuracy: 0.6230\n","Epoch 223/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0478 - accuracy: 0.9849 - val_loss: 1.8854 - val_accuracy: 0.6406\n","Epoch 224/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2347 - accuracy: 0.9571 - val_loss: 4.8486 - val_accuracy: 0.4883\n","Epoch 225/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.7845 - accuracy: 0.7871 - val_loss: 6.1959 - val_accuracy: 0.3125\n","Epoch 226/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.7038 - accuracy: 0.7859 - val_loss: 2.3316 - val_accuracy: 0.5703\n","Epoch 227/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.3714 - accuracy: 0.8828 - val_loss: 2.4090 - val_accuracy: 0.5293\n","Epoch 228/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3544 - accuracy: 0.8878 - val_loss: 1.7495 - val_accuracy: 0.6270\n","Epoch 229/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.3647 - accuracy: 0.8993 - val_loss: 1.8357 - val_accuracy: 0.5938\n","Epoch 230/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2455 - accuracy: 0.9313 - val_loss: 1.4737 - val_accuracy: 0.6387\n","Epoch 231/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2019 - accuracy: 0.9492 - val_loss: 1.6192 - val_accuracy: 0.6562\n","Epoch 232/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2201 - accuracy: 0.9415 - val_loss: 5.0522 - val_accuracy: 0.3418\n","Epoch 233/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.0453 - accuracy: 0.7231 - val_loss: 1.7078 - val_accuracy: 0.6152\n","Epoch 234/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.3851 - accuracy: 0.8909 - val_loss: 1.5014 - val_accuracy: 0.6133\n","Epoch 235/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2969 - accuracy: 0.9384 - val_loss: 2.2009 - val_accuracy: 0.4590\n","Epoch 236/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.5015 - accuracy: 0.6238 - val_loss: 9.8811 - val_accuracy: 0.3301\n","Epoch 237/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.9105 - accuracy: 0.7688 - val_loss: 4.5887 - val_accuracy: 0.4785\n","Epoch 238/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5947 - accuracy: 0.8396 - val_loss: 2.4809 - val_accuracy: 0.5605\n","Epoch 239/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.5155 - accuracy: 0.8651 - val_loss: 1.8018 - val_accuracy: 0.5879\n","Epoch 240/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3981 - accuracy: 0.9006 - val_loss: 2.3510 - val_accuracy: 0.5879\n","Epoch 241/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2973 - accuracy: 0.8977 - val_loss: 1.6788 - val_accuracy: 0.6484\n","Epoch 242/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1876 - accuracy: 0.9517 - val_loss: 1.3433 - val_accuracy: 0.6582\n","Epoch 243/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1726 - accuracy: 0.9429 - val_loss: 1.5299 - val_accuracy: 0.6211\n","Epoch 244/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1087 - accuracy: 0.9704 - val_loss: 1.5464 - val_accuracy: 0.6445\n","Epoch 245/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.1517 - accuracy: 0.9554 - val_loss: 1.6368 - val_accuracy: 0.6641\n","Epoch 246/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1049 - accuracy: 0.9711 - val_loss: 1.7640 - val_accuracy: 0.6367\n","Epoch 247/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0896 - accuracy: 0.9774 - val_loss: 1.7964 - val_accuracy: 0.6406\n","Epoch 248/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0607 - accuracy: 0.9792 - val_loss: 1.7807 - val_accuracy: 0.6465\n","Epoch 249/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0787 - accuracy: 0.9734 - val_loss: 1.7395 - val_accuracy: 0.6367\n","Epoch 250/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0826 - accuracy: 0.9741 - val_loss: 1.6311 - val_accuracy: 0.6602\n","Epoch 251/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0510 - accuracy: 0.9866 - val_loss: 1.7541 - val_accuracy: 0.6602\n","Epoch 252/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0399 - accuracy: 0.9856 - val_loss: 1.7651 - val_accuracy: 0.6777\n","Epoch 253/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0243 - accuracy: 0.9943 - val_loss: 1.9976 - val_accuracy: 0.6289\n","Epoch 254/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0369 - accuracy: 0.9921 - val_loss: 1.8822 - val_accuracy: 0.6582\n","Epoch 255/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0241 - accuracy: 0.9942 - val_loss: 2.0292 - val_accuracy: 0.6328\n","Epoch 256/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.0704 - accuracy: 0.9907 - val_loss: 1.9505 - val_accuracy: 0.6504\n","Epoch 257/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 2.0868 - val_accuracy: 0.6348\n","Epoch 258/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0944 - accuracy: 0.9738 - val_loss: 2.1803 - val_accuracy: 0.6309\n","Epoch 259/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0824 - accuracy: 0.9809 - val_loss: 1.9701 - val_accuracy: 0.6406\n","Epoch 260/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1280 - accuracy: 0.9676 - val_loss: 2.7141 - val_accuracy: 0.5488\n","Epoch 261/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.3673 - accuracy: 0.9014 - val_loss: 2.9468 - val_accuracy: 0.5566\n","Epoch 262/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5207 - accuracy: 0.8815 - val_loss: 2.8908 - val_accuracy: 0.5684\n","Epoch 263/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.4420 - accuracy: 0.8726 - val_loss: 2.9663 - val_accuracy: 0.5352\n","Epoch 264/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.4301 - accuracy: 0.8649 - val_loss: 3.2070 - val_accuracy: 0.5918\n","Epoch 265/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.1958 - accuracy: 0.9337 - val_loss: 1.7552 - val_accuracy: 0.6270\n","Epoch 266/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1659 - accuracy: 0.9568 - val_loss: 1.7250 - val_accuracy: 0.6387\n","Epoch 267/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.4577 - accuracy: 0.9002 - val_loss: 3.6735 - val_accuracy: 0.4902\n","Epoch 268/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5658 - accuracy: 0.8391 - val_loss: 1.6006 - val_accuracy: 0.6270\n","Epoch 269/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1992 - accuracy: 0.9531 - val_loss: 1.6350 - val_accuracy: 0.6504\n","Epoch 270/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.1063 - accuracy: 0.9643 - val_loss: 1.5541 - val_accuracy: 0.6680\n","Epoch 271/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0651 - accuracy: 0.9792 - val_loss: 1.4843 - val_accuracy: 0.6602\n","Epoch 272/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0944 - accuracy: 0.9656 - val_loss: 1.7085 - val_accuracy: 0.6504\n","Epoch 273/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0726 - accuracy: 0.9807 - val_loss: 1.9192 - val_accuracy: 0.6309\n","Epoch 274/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0871 - accuracy: 0.9689 - val_loss: 1.9433 - val_accuracy: 0.6719\n","Epoch 275/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.1136 - accuracy: 0.9786 - val_loss: 1.7331 - val_accuracy: 0.6875\n","Epoch 276/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0260 - accuracy: 0.9941 - val_loss: 1.8769 - val_accuracy: 0.6562\n","Epoch 277/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0469 - accuracy: 0.9836 - val_loss: 1.7679 - val_accuracy: 0.6777\n","Epoch 278/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0451 - accuracy: 0.9858 - val_loss: 2.0141 - val_accuracy: 0.6777\n","Epoch 279/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0573 - accuracy: 0.9864 - val_loss: 1.7839 - val_accuracy: 0.6680\n","Epoch 280/2000\n","48/48 [==============================] - 2s 51ms/step - loss: 0.0205 - accuracy: 0.9958 - val_loss: 1.9523 - val_accuracy: 0.6660\n","Epoch 281/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0216 - accuracy: 0.9942 - val_loss: 1.9730 - val_accuracy: 0.6660\n","Epoch 282/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0361 - accuracy: 0.9843 - val_loss: 2.0253 - val_accuracy: 0.6582\n","Epoch 283/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0433 - accuracy: 0.9853 - val_loss: 2.1423 - val_accuracy: 0.6406\n","Epoch 284/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0558 - accuracy: 0.9931 - val_loss: 1.8998 - val_accuracy: 0.6797\n","Epoch 285/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0376 - accuracy: 0.9880 - val_loss: 2.1684 - val_accuracy: 0.6406\n","Epoch 286/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0398 - accuracy: 0.9889 - val_loss: 1.8499 - val_accuracy: 0.6738\n","Epoch 287/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0275 - accuracy: 0.9923 - val_loss: 1.9459 - val_accuracy: 0.6680\n","\n","Epoch 00287: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 288/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0087 - accuracy: 0.9967 - val_loss: 1.8388 - val_accuracy: 0.6660\n","Epoch 289/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0170 - accuracy: 0.9920 - val_loss: 1.9482 - val_accuracy: 0.6660\n","Epoch 290/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0093 - accuracy: 0.9977 - val_loss: 1.8928 - val_accuracy: 0.6836\n","Epoch 291/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0060 - accuracy: 0.9996 - val_loss: 1.9851 - val_accuracy: 0.6562\n","Epoch 292/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 1.8646 - val_accuracy: 0.6738\n","Epoch 293/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 1.9613 - val_accuracy: 0.6562\n","Epoch 294/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.8288 - val_accuracy: 0.6895\n","Epoch 295/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0086 - accuracy: 0.9986 - val_loss: 1.9806 - val_accuracy: 0.6816\n","Epoch 296/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0071 - accuracy: 0.9996 - val_loss: 1.9175 - val_accuracy: 0.6816\n","Epoch 297/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 1.9574 - val_accuracy: 0.6777\n","Epoch 298/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0122 - accuracy: 0.9953 - val_loss: 1.8995 - val_accuracy: 0.6914\n","Epoch 299/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 1.9398 - val_accuracy: 0.6758\n","Epoch 300/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.7695 - val_accuracy: 0.6855\n","Epoch 301/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.8840 - val_accuracy: 0.6836\n","Epoch 302/2000\n","48/48 [==============================] - 2s 51ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 1.8167 - val_accuracy: 0.6797\n","Epoch 303/2000\n","48/48 [==============================] - 2s 51ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.8830 - val_accuracy: 0.7012\n","Epoch 304/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 2.0145 - val_accuracy: 0.6816\n","Epoch 305/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 2.4919 - val_accuracy: 0.6621\n","Epoch 306/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0897 - accuracy: 0.9801 - val_loss: 2.2369 - val_accuracy: 0.6816\n","Epoch 307/2000\n","48/48 [==============================] - 2s 49ms/step - loss: 0.0276 - accuracy: 0.9942 - val_loss: 2.3562 - val_accuracy: 0.6484\n","Epoch 308/2000\n","48/48 [==============================] - 2s 51ms/step - loss: 0.1168 - accuracy: 0.9657 - val_loss: 2.1572 - val_accuracy: 0.6523\n","Epoch 309/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0545 - accuracy: 0.9778 - val_loss: 2.1304 - val_accuracy: 0.6816\n","Epoch 310/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0232 - accuracy: 0.9920 - val_loss: 2.0645 - val_accuracy: 0.6738\n","Epoch 311/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0146 - accuracy: 0.9970 - val_loss: 2.0570 - val_accuracy: 0.6855\n","Epoch 312/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0234 - accuracy: 0.9928 - val_loss: 2.0573 - val_accuracy: 0.6543\n","Epoch 313/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0454 - accuracy: 0.9859 - val_loss: 1.8329 - val_accuracy: 0.6777\n","Epoch 314/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 1.9626 - val_accuracy: 0.6816\n","Epoch 315/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0422 - accuracy: 0.9862 - val_loss: 2.1262 - val_accuracy: 0.6582\n","Epoch 316/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0263 - accuracy: 0.9909 - val_loss: 1.9194 - val_accuracy: 0.6445\n","Epoch 317/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 2.0323 - val_accuracy: 0.6641\n","Epoch 318/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0137 - accuracy: 0.9966 - val_loss: 1.8677 - val_accuracy: 0.7031\n","Epoch 319/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0082 - accuracy: 0.9981 - val_loss: 1.8833 - val_accuracy: 0.6855\n","Epoch 320/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0119 - accuracy: 0.9958 - val_loss: 1.9272 - val_accuracy: 0.6758\n","Epoch 321/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0038 - accuracy: 0.9983 - val_loss: 1.8267 - val_accuracy: 0.6836\n","Epoch 322/2000\n","48/48 [==============================] - 2s 50ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 1.9769 - val_accuracy: 0.6758\n","Epoch 323/2000\n","48/48 [==============================] - 2s 51ms/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 1.9172 - val_accuracy: 0.6602\n","Epoch 324/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0042 - accuracy: 0.9993 - val_loss: 2.0042 - val_accuracy: 0.6816\n","Epoch 325/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.8395 - val_accuracy: 0.7148\n","Epoch 326/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 0.6797\n","Epoch 327/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0086 - accuracy: 0.9969 - val_loss: 2.0155 - val_accuracy: 0.6836\n","Epoch 328/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 2.0086 - val_accuracy: 0.6914\n","Epoch 329/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.0336 - val_accuracy: 0.7012\n","Epoch 330/2000\n","48/48 [==============================] - 3s 53ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 2.1319 - val_accuracy: 0.6699\n","Epoch 331/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 2.0530 - val_accuracy: 0.6914\n","Epoch 332/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 2.3024 - val_accuracy: 0.6699\n","Epoch 333/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.0813 - val_accuracy: 0.6719\n","Epoch 334/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 2.1770 - val_accuracy: 0.6855\n","Epoch 335/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0107 - accuracy: 0.9970 - val_loss: 2.3387 - val_accuracy: 0.6465\n","Epoch 336/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0150 - accuracy: 0.9937 - val_loss: 2.4408 - val_accuracy: 0.6504\n","Epoch 337/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0372 - accuracy: 0.9864 - val_loss: 2.4807 - val_accuracy: 0.6270\n","Epoch 338/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0651 - accuracy: 0.9751 - val_loss: 2.6859 - val_accuracy: 0.6484\n","Epoch 339/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0736 - accuracy: 0.9841 - val_loss: 2.6292 - val_accuracy: 0.6387\n","Epoch 340/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0961 - accuracy: 0.9692 - val_loss: 2.7843 - val_accuracy: 0.6602\n","Epoch 341/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0715 - accuracy: 0.9853 - val_loss: 2.1736 - val_accuracy: 0.6484\n","Epoch 342/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0438 - accuracy: 0.9885 - val_loss: 2.1224 - val_accuracy: 0.6660\n","Epoch 343/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0441 - accuracy: 0.9821 - val_loss: 2.1682 - val_accuracy: 0.6602\n","Epoch 344/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 2.0811 - val_accuracy: 0.6738\n","Epoch 345/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0415 - accuracy: 0.9860 - val_loss: 2.4191 - val_accuracy: 0.6191\n","Epoch 346/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2771 - accuracy: 0.9349 - val_loss: 2.4515 - val_accuracy: 0.5762\n","Epoch 347/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1988 - accuracy: 0.9324 - val_loss: 2.5689 - val_accuracy: 0.6367\n","1  번째 학습을 완료했습니다.\n","Epoch 1/2000\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["48/48 [==============================] - 8s 64ms/step - loss: 0.9427 - accuracy: 0.8395 - val_loss: 1.4588 - val_accuracy: 0.8477\n","Epoch 2/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.4057 - accuracy: 0.8975 - val_loss: 0.4201 - val_accuracy: 0.8887\n","Epoch 3/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2822 - accuracy: 0.9059 - val_loss: 0.1174 - val_accuracy: 0.9590\n","Epoch 4/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.2237 - accuracy: 0.9332 - val_loss: 0.1604 - val_accuracy: 0.9453\n","Epoch 5/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3317 - accuracy: 0.9401 - val_loss: 0.1983 - val_accuracy: 0.9238\n","Epoch 6/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.2158 - accuracy: 0.9381 - val_loss: 0.1883 - val_accuracy: 0.9414\n","Epoch 7/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.1638 - accuracy: 0.9449 - val_loss: 0.2256 - val_accuracy: 0.9375\n","Epoch 8/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1691 - accuracy: 0.9486 - val_loss: 0.1711 - val_accuracy: 0.9355\n","Epoch 9/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1768 - accuracy: 0.9509 - val_loss: 0.5133 - val_accuracy: 0.9297\n","Epoch 10/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2835 - accuracy: 0.9346 - val_loss: 3.4033 - val_accuracy: 0.5410\n","Epoch 11/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 1.1394 - accuracy: 0.7218 - val_loss: 2.1975 - val_accuracy: 0.5957\n","Epoch 12/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.6405 - accuracy: 0.8183 - val_loss: 0.7868 - val_accuracy: 0.7559\n","Epoch 13/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.3833 - accuracy: 0.8978 - val_loss: 0.7937 - val_accuracy: 0.8496\n","Epoch 14/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2766 - accuracy: 0.9350 - val_loss: 0.6976 - val_accuracy: 0.7910\n","Epoch 15/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.3096 - accuracy: 0.9117 - val_loss: 0.4386 - val_accuracy: 0.8828\n","Epoch 16/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1770 - accuracy: 0.9443 - val_loss: 0.4261 - val_accuracy: 0.8633\n","Epoch 17/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.1248 - accuracy: 0.9523 - val_loss: 0.3415 - val_accuracy: 0.8984\n","Epoch 18/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0890 - accuracy: 0.9655 - val_loss: 0.2513 - val_accuracy: 0.9023\n","Epoch 19/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0950 - accuracy: 0.9762 - val_loss: 0.4247 - val_accuracy: 0.8906\n","Epoch 20/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.1104 - accuracy: 0.9733 - val_loss: 0.7389 - val_accuracy: 0.8262\n","Epoch 21/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.1491 - accuracy: 0.9536 - val_loss: 0.4014 - val_accuracy: 0.8730\n","Epoch 22/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1135 - accuracy: 0.9753 - val_loss: 0.3238 - val_accuracy: 0.9043\n","Epoch 23/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0594 - accuracy: 0.9845 - val_loss: 0.3773 - val_accuracy: 0.8984\n","Epoch 24/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0623 - accuracy: 0.9840 - val_loss: 0.3452 - val_accuracy: 0.9160\n","Epoch 25/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0622 - accuracy: 0.9838 - val_loss: 0.4932 - val_accuracy: 0.8457\n","Epoch 26/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2243 - accuracy: 0.9352 - val_loss: 2.6138 - val_accuracy: 0.6602\n","Epoch 27/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.8188 - accuracy: 0.8109 - val_loss: 1.2685 - val_accuracy: 0.7285\n","Epoch 28/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.2216 - accuracy: 0.9298 - val_loss: 1.0029 - val_accuracy: 0.7461\n","Epoch 29/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1931 - accuracy: 0.9506 - val_loss: 0.5531 - val_accuracy: 0.8281\n","Epoch 30/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2271 - accuracy: 0.9327 - val_loss: 1.1171 - val_accuracy: 0.8184\n","Epoch 31/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.5073 - accuracy: 0.9446 - val_loss: 0.4923 - val_accuracy: 0.8398\n","Epoch 32/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0854 - accuracy: 0.9704 - val_loss: 0.4015 - val_accuracy: 0.8867\n","Epoch 33/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0856 - accuracy: 0.9683 - val_loss: 0.5517 - val_accuracy: 0.8418\n","Epoch 34/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0542 - accuracy: 0.9866 - val_loss: 0.4890 - val_accuracy: 0.8691\n","Epoch 35/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0910 - accuracy: 0.9753 - val_loss: 0.5040 - val_accuracy: 0.8809\n","Epoch 36/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0709 - accuracy: 0.9806 - val_loss: 0.6573 - val_accuracy: 0.8438\n","Epoch 37/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0874 - accuracy: 0.9737 - val_loss: 0.5429 - val_accuracy: 0.8613\n","Epoch 38/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0650 - accuracy: 0.9822 - val_loss: 0.4080 - val_accuracy: 0.8828\n","Epoch 39/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0524 - accuracy: 0.9913 - val_loss: 0.4982 - val_accuracy: 0.8809\n","Epoch 40/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0427 - accuracy: 0.9883 - val_loss: 0.4193 - val_accuracy: 0.8887\n","Epoch 41/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0138 - accuracy: 0.9972 - val_loss: 0.4364 - val_accuracy: 0.8848\n","Epoch 42/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0087 - accuracy: 0.9989 - val_loss: 0.4616 - val_accuracy: 0.8945\n","Epoch 43/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0211 - accuracy: 0.9945 - val_loss: 0.6601 - val_accuracy: 0.8418\n","Epoch 44/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.2030 - accuracy: 0.9490 - val_loss: 0.6116 - val_accuracy: 0.8340\n","Epoch 45/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1067 - accuracy: 0.9739 - val_loss: 0.5416 - val_accuracy: 0.8457\n","Epoch 46/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0780 - accuracy: 0.9747 - val_loss: 0.5227 - val_accuracy: 0.8672\n","Epoch 47/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0451 - accuracy: 0.9837 - val_loss: 0.4429 - val_accuracy: 0.8770\n","Epoch 48/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0615 - accuracy: 0.9814 - val_loss: 0.5628 - val_accuracy: 0.8672\n","Epoch 49/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0355 - accuracy: 0.9871 - val_loss: 0.4919 - val_accuracy: 0.8613\n","Epoch 50/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0180 - accuracy: 0.9937 - val_loss: 0.4071 - val_accuracy: 0.8887\n","Epoch 51/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0088 - accuracy: 0.9978 - val_loss: 0.4193 - val_accuracy: 0.8906\n","Epoch 52/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0089 - accuracy: 0.9967 - val_loss: 0.3192 - val_accuracy: 0.9121\n","Epoch 53/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0294 - accuracy: 0.9883 - val_loss: 0.4365 - val_accuracy: 0.8789\n","Epoch 54/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 0.4840 - val_accuracy: 0.8574\n","Epoch 55/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0211 - accuracy: 0.9927 - val_loss: 0.4106 - val_accuracy: 0.8945\n","Epoch 56/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0214 - accuracy: 0.9936 - val_loss: 0.5711 - val_accuracy: 0.8574\n","Epoch 57/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0427 - accuracy: 0.9918 - val_loss: 0.6224 - val_accuracy: 0.8418\n","Epoch 58/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0265 - accuracy: 0.9930 - val_loss: 0.4018 - val_accuracy: 0.8984\n","Epoch 59/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0508 - accuracy: 0.9853 - val_loss: 0.5049 - val_accuracy: 0.8477\n","Epoch 60/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0440 - accuracy: 0.9878 - val_loss: 0.4548 - val_accuracy: 0.8828\n","Epoch 61/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1403 - accuracy: 0.9666 - val_loss: 1.7182 - val_accuracy: 0.6914\n","Epoch 62/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.2989 - accuracy: 0.9134 - val_loss: 1.9608 - val_accuracy: 0.7930\n","Epoch 63/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1681 - accuracy: 0.9483 - val_loss: 0.7033 - val_accuracy: 0.8262\n","Epoch 64/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.1142 - accuracy: 0.9594 - val_loss: 0.4786 - val_accuracy: 0.8633\n","Epoch 65/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0448 - accuracy: 0.9856 - val_loss: 0.5544 - val_accuracy: 0.8555\n","Epoch 66/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0358 - accuracy: 0.9932 - val_loss: 0.7305 - val_accuracy: 0.8086\n","Epoch 67/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0978 - accuracy: 0.9723 - val_loss: 0.7562 - val_accuracy: 0.7969\n","Epoch 68/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1589 - accuracy: 0.9595 - val_loss: 7.4974 - val_accuracy: 0.6641\n","Epoch 69/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.5915 - accuracy: 0.8879 - val_loss: 2.9236 - val_accuracy: 0.4668\n","Epoch 70/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.6114 - accuracy: 0.8400 - val_loss: 2.1705 - val_accuracy: 0.6914\n","Epoch 71/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.4664 - accuracy: 0.8840 - val_loss: 1.2625 - val_accuracy: 0.7402\n","Epoch 72/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.3672 - accuracy: 0.9211 - val_loss: 2.3873 - val_accuracy: 0.5391\n","Epoch 73/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.7635 - accuracy: 0.8167 - val_loss: 4.3197 - val_accuracy: 0.6582\n","Epoch 74/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.3032 - accuracy: 0.9232 - val_loss: 0.9744 - val_accuracy: 0.7812\n","Epoch 75/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.1168 - accuracy: 0.9647 - val_loss: 0.7197 - val_accuracy: 0.7715\n","Epoch 76/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.2220 - accuracy: 0.9425 - val_loss: 0.7803 - val_accuracy: 0.7734\n","Epoch 77/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.1206 - accuracy: 0.9586 - val_loss: 0.7265 - val_accuracy: 0.8047\n","Epoch 78/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0697 - accuracy: 0.9752 - val_loss: 0.6792 - val_accuracy: 0.8008\n","Epoch 79/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0580 - accuracy: 0.9838 - val_loss: 0.6964 - val_accuracy: 0.7910\n","Epoch 80/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0624 - accuracy: 0.9838 - val_loss: 0.7894 - val_accuracy: 0.8164\n","Epoch 81/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0221 - accuracy: 0.9962 - val_loss: 0.6605 - val_accuracy: 0.8027\n","Epoch 82/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.7678 - val_accuracy: 0.8066\n","Epoch 83/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0147 - accuracy: 0.9916 - val_loss: 0.6763 - val_accuracy: 0.8262\n","Epoch 84/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0303 - accuracy: 0.9893 - val_loss: 0.7925 - val_accuracy: 0.8008\n","Epoch 85/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0288 - accuracy: 0.9919 - val_loss: 0.6671 - val_accuracy: 0.8398\n","Epoch 86/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.7407 - val_accuracy: 0.8145\n","Epoch 87/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0358 - accuracy: 0.9932 - val_loss: 0.8450 - val_accuracy: 0.8301\n","Epoch 88/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0157 - accuracy: 0.9966 - val_loss: 0.7203 - val_accuracy: 0.8145\n","Epoch 89/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0301 - accuracy: 0.9933 - val_loss: 0.6996 - val_accuracy: 0.8340\n","Epoch 90/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0164 - accuracy: 0.9951 - val_loss: 0.7227 - val_accuracy: 0.8223\n","Epoch 91/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0284 - accuracy: 0.9919 - val_loss: 0.9229 - val_accuracy: 0.8145\n","Epoch 92/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0306 - accuracy: 0.9938 - val_loss: 0.8781 - val_accuracy: 0.7969\n","Epoch 93/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0229 - accuracy: 0.9946 - val_loss: 0.8470 - val_accuracy: 0.8145\n","Epoch 94/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0231 - accuracy: 0.9931 - val_loss: 0.7666 - val_accuracy: 0.8203\n","Epoch 95/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0199 - accuracy: 0.9930 - val_loss: 0.9462 - val_accuracy: 0.8027\n","Epoch 96/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0299 - accuracy: 0.9854 - val_loss: 0.9033 - val_accuracy: 0.8184\n","Epoch 97/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0151 - accuracy: 0.9955 - val_loss: 0.9559 - val_accuracy: 0.8262\n","Epoch 98/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0122 - accuracy: 0.9974 - val_loss: 0.8275 - val_accuracy: 0.8203\n","Epoch 99/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0116 - accuracy: 0.9961 - val_loss: 0.7850 - val_accuracy: 0.8262\n","Epoch 100/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.9479 - val_accuracy: 0.7969\n","Epoch 101/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0280 - accuracy: 0.9896 - val_loss: 1.0865 - val_accuracy: 0.8047\n","Epoch 102/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0459 - accuracy: 0.9871 - val_loss: 0.9375 - val_accuracy: 0.8066\n","Epoch 103/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0380 - accuracy: 0.9864 - val_loss: 1.0574 - val_accuracy: 0.7949\n","\n","Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 104/2000\n","48/48 [==============================] - 2s 48ms/step - loss: 0.0264 - accuracy: 0.9927 - val_loss: 0.7851 - val_accuracy: 0.8301\n","Epoch 105/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0036 - accuracy: 0.9998 - val_loss: 0.7559 - val_accuracy: 0.8418\n","Epoch 106/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.6975 - val_accuracy: 0.8301\n","Epoch 107/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.6025 - val_accuracy: 0.8457\n","Epoch 108/2000\n","48/48 [==============================] - 2s 44ms/step - loss: 0.0154 - accuracy: 0.9956 - val_loss: 0.9364 - val_accuracy: 0.8027\n","Epoch 109/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0458 - accuracy: 0.9864 - val_loss: 0.8885 - val_accuracy: 0.8086\n","Epoch 110/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.7344 - val_accuracy: 0.8320\n","Epoch 111/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0113 - accuracy: 0.9952 - val_loss: 0.7865 - val_accuracy: 0.8164\n","Epoch 112/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0131 - accuracy: 0.9968 - val_loss: 0.7872 - val_accuracy: 0.8340\n","Epoch 113/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 0.7124 - val_accuracy: 0.8340\n","Epoch 114/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0142 - accuracy: 0.9935 - val_loss: 0.6682 - val_accuracy: 0.8398\n","Epoch 115/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.8706 - val_accuracy: 0.8203\n","Epoch 116/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.7358 - val_accuracy: 0.8418\n","Epoch 117/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.7516 - val_accuracy: 0.8320\n","Epoch 118/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.7786 - val_accuracy: 0.8242\n","Epoch 119/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 0.8496\n","Epoch 120/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0020 - accuracy: 0.9989 - val_loss: 0.7607 - val_accuracy: 0.8301\n","Epoch 121/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7808 - val_accuracy: 0.8398\n","Epoch 122/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6898 - val_accuracy: 0.8359\n","Epoch 123/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.6668 - val_accuracy: 0.8457\n","Epoch 124/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.7811 - val_accuracy: 0.8379\n","Epoch 125/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.7194 - val_accuracy: 0.8477\n","Epoch 126/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6624 - val_accuracy: 0.8613\n","Epoch 127/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7526 - val_accuracy: 0.8340\n","Epoch 128/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7969 - val_accuracy: 0.8496\n","Epoch 129/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6761 - val_accuracy: 0.8477\n","Epoch 130/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6621 - val_accuracy: 0.8555\n","Epoch 131/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0021 - accuracy: 0.9985 - val_loss: 0.7257 - val_accuracy: 0.8516\n","Epoch 132/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.7400 - val_accuracy: 0.8438\n","Epoch 133/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 5.4760e-04 - accuracy: 1.0000 - val_loss: 0.7412 - val_accuracy: 0.8555\n","Epoch 134/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 3.5402e-04 - accuracy: 1.0000 - val_loss: 0.7732 - val_accuracy: 0.8477\n","Epoch 135/2000\n","48/48 [==============================] - 2s 47ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6547 - val_accuracy: 0.8535\n","Epoch 136/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0024 - accuracy: 0.9988 - val_loss: 0.6833 - val_accuracy: 0.8477\n","Epoch 137/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.6952 - val_accuracy: 0.8574\n","Epoch 138/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0022 - accuracy: 0.9986 - val_loss: 0.7699 - val_accuracy: 0.8418\n","Epoch 139/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0104 - accuracy: 0.9984 - val_loss: 0.9327 - val_accuracy: 0.8184\n","Epoch 140/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.8377 - val_accuracy: 0.8359\n","Epoch 141/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0053 - accuracy: 0.9978 - val_loss: 0.8515 - val_accuracy: 0.8301\n","Epoch 142/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.7457 - val_accuracy: 0.8457\n","Epoch 143/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.8431 - val_accuracy: 0.8359\n","Epoch 144/2000\n","48/48 [==============================] - 2s 46ms/step - loss: 0.0070 - accuracy: 0.9968 - val_loss: 1.0681 - val_accuracy: 0.7988\n","Epoch 145/2000\n","48/48 [==============================] - 2s 45ms/step - loss: 0.0040 - accuracy: 0.9999 - val_loss: 0.8819 - val_accuracy: 0.8457\n","Epoch 146/2000\n","27/48 [===============>..............] - ETA: 0s - loss: 0.0041 - accuracy: 0.9997"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-55ab248bd134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mval_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreLR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"onOYBeVy-Mkb"},"source":["'''\n","inputs = Input(shape=(28,28,1))\n","x = inputs\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(512,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = _x\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(512,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(512,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(512,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(512,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","x = Flatten()(x)\n","x = Dense(2048)(x)\n","x = Dense(10,activation='softmax')(x)\n","outputs=x\n","model = Model(inputs=inputs,outputs=outputs)\n","\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KswTfNSi-Mkc"},"source":["'''\n","0.87\n","inputs = Input(shape=(28,28,1))\n","x = inputs\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = _x\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(512,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","x = Flatten()(x)\n","x = Dense(2048)(x)\n","x = Dense(10,activation='softmax')(x)\n","outputs=x\n","model = Model(inputs=inputs,outputs=outputs)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fx1Z4nhC-Mkc"},"source":["'''\n","inputs = Input(shape=(28,28,1))\n","x = inputs\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = _x\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(128,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(256,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","_x = Conv2D(512,3,padding='same')(x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","_x = Conv2D(128,3,padding='same')(_x)\n","_x = BatchNormalization()(_x)\n","_x = Activation('relu')(_x)\n","x = x+_x\n","x = MaxPooling2D(2)(x)\n","x = Flatten()(x)\n","x = Dense(2048)(x)\n","x = Dense(10,activation='softmax')(x)\n","outputs=x\n","model = Model(inputs=inputs,outputs=outputs)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"44FNCmNG-Mkd"},"source":["model = load_model('./models/02_02_88.h5')\n","df = pd.read_csv(\"test.csv\",index_col=[0])\n","x_test = df.values[:,1:].reshape(-1,28,28).astype('float32')/255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzM0rw6U-Mke","outputId":"0312f408-37af-44e5-b673-febb0961614c"},"source":["binary_model = []\n","for i in range(0,10):\n","    print(i)\n","    model = load_model('./binary_models/{}_binary.h5'.format(i))\n","    binary_model.append(model)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QwofhPdW-Mke"},"source":["y_pred = model.predict(x_test)\n","\n","def ordering(array):\n","    temp = array.copy()\n","    result = []\n","    for i in range(len(temp)):\n","        sol = np.argmax(temp)\n","        result.append(sol)\n","        temp[sol]=0\n","    return np.array(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fI2mNTi-Mkf","outputId":"f253edd1-103b-41f5-8832-f5ac667f5a38"},"source":["y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[7.6569414e-08, 2.2260668e-08, 4.6740688e-06, ..., 2.5446711e-10,\n","        5.9703314e-07, 5.4656005e-11],\n","       [2.9817595e-16, 2.0647546e-12, 4.4424861e-16, ..., 2.6480063e-10,\n","        8.9294266e-11, 1.0000000e+00],\n","       [2.4707546e-05, 1.3310514e-01, 8.9886552e-03, ..., 9.1972132e-04,\n","        1.2284001e-02, 2.9377347e-06],\n","       ...,\n","       [8.6063210e-09, 7.5784501e-10, 2.6377617e-10, ..., 3.1178827e-13,\n","        1.2005766e-07, 6.1626551e-15],\n","       [1.0054513e-03, 2.9064235e-01, 1.6038346e-05, ..., 1.7447629e-06,\n","        8.1512779e-03, 4.5916289e-03],\n","       [9.9825722e-01, 6.9838888e-16, 2.4161539e-10, ..., 1.8471900e-15,\n","        6.6174334e-13, 1.4705076e-15]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"Zy1domP0-Mkf","outputId":"db3b1dcb-6d43-4fc7-9938-34990934eb64"},"source":["y_notyet = np.argmax(y_pred,axis=-1)\n","y_notyet\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([6, 9, 3, ..., 6, 5, 0], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"g1xQykxG-Mkf","outputId":"946d6597-cef3-4297-f226-1a1349f5005d"},"source":["binary_model[3].predict(x_test[0:2])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[3.88120088e-36],\n","       [1.13777095e-36]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"ULE3KyHS-Mkg","outputId":"41ead2b9-7d2e-44a9-a6ba-e8c4645752cd"},"source":["y_pred = model.predict(x_train)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'x_train' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-18-cc4ab4411252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"l4Jhbsfx-Mkg"},"source":["k=729"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hPmflg5c-Mkg","outputId":"05282ba4-d754-4522-aa33-73d47b49b612"},"source":["k+=1\n","for i in [k]:\n","    temp_result = y_notyet[i]\n","    binary_result = binary_model[temp_result].predict(np.array([x_test[i].reshape(28,28,1)]))\n","    a=np.round(y_pred[i],3)\n","    print(\"원래모델 : \",temp_result,'\\n원래모델확률 : ',a,\"\\n바이너리 : \",binary_result[0][0])\n","    print(\"0\",binary_model[0].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"1\",binary_model[1].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"2\",binary_model[2].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"3\",binary_model[3].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"4\",binary_model[4].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"5\",binary_model[5].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"6\",binary_model[6].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"7\",binary_model[7].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"8\",binary_model[8].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n","    print(\"9\",binary_model[9].predict(np.array([x_test[i].reshape(28,28,1)]))[0][0])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["원래모델 :  9 \n","원래모델확률 :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] \n","바이너리 :  1.6463632e-24\n","0 0.0\n","1 5.6854813e-21\n","2 0.0\n","3 0.0\n","4 3.455557e-05\n","5 0.0\n","6 0.0\n","7 0.0\n","8 1.2415284e-22\n","9 1.6463632e-24\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UEgcogTL-Mkg","outputId":"b1e9adb4-4593-4bda-ae06-bb49fd8bf6ac"},"source":["plt.imshow(x_test[k])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"-LIIBisa-Mkh","outputId":"aa584c54-1643-48af-eb35-d853835f2d2b"},"source":["y_pred = model.predict(x_test)\n","y_pred = np.argmax(y_pred,axis=-1)\n","df_sub = pd.read_csv('submission.csv',index_col=0)\n","df_sub['digit'] = y_pred\n","df_sub.to_csv('test_4.csv')\n","y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([6, 9, 8, ..., 6, 8, 0], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"DT0ZAdKJ-Mkh","outputId":"4f04bdd5-32cc-4226-9a6a-19a6aa77a588"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([6, 9, 8, ..., 6, 8, 0], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"8TjhvZQW-Mkh","outputId":"4b83ad9f-6588-4c0e-c152-b4310a2c4f32"},"source":["k=5\n","df = pd.read_csv(\"preprocessing_150.csv\",index_col=[0])\n","df2 = pd.read_csv(\"train.csv\",index_col=[0])\n","plt.imshow(df.values[k,2:].reshape(28,28).astype('float32'))\n","plt.show()\n","df2 = pd.read_csv(\"train.csv\",index_col=[0])\n","plt.imshow(df2.values[k,2:].reshape(28,28).astype('float32'))\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"cHKAVNiD-Mkh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7nEeGDn-Mkh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gc_-Flz2-Mki"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wImlSOL--Mki"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bj2wn75b-Mki"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r7YkDa_J-Mki"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6a0jyE7-Mki"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5jYY0jU-Mki"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4NiWL-c-Mki"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzhsBOPW-Mkj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pDPP4Lp2-Mkj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wsmnbLS-Mkj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4vmV_gu-Mkj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zxBZ_-66-Mkj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBoKPs_E-Mkj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwaS-Ang-Mkj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y5_Acvtq-Mkk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k7pmh6Ej-Mkk"},"source":[""],"execution_count":null,"outputs":[]}]}